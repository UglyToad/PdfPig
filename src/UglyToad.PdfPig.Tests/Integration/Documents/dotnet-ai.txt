Tell us about your PDF experience.AI apps for .NET developersLearn to build AI apps with .NET. Browse sample code, tutorials, quickstarts, conceptual articles,and more.Get startedｅOVERVIEWDevelop .NET apps with AI featuresMicrosoft.Extensions.AI librariesｂGET STARTEDConnect to and prompt an AI modelBuild an Azure AI chat appGenerate images using Azure AIEssential conceptsｐCONCEPTHow generative AI and LLMs workUnderstand tokensPreserve semantic meaning with embeddingsSemantic search with vector databasesPrompt engineeringEvaluation librariesCommon tasksｃHOW-TO GUIDEAuthenticate App Service to Azure OpenAIAuthenticate App Service to a vector databaseUse Redis with the Semantic Kernel SDKUse custom and local AI models with the Semantic Kernel SDKWork with content filteringTutorialsｇTUTORIALScale Azure OpenAI with Azure Container Apps.NET enterprise chat sample using RAGImplement RAG using vector searchEvaluate a model's responseTrainingｇTUTORIALFundamentals of Azure OpenAI ServiceGenerate conversations Azure OpenAI completions.NET enterprise chat sample using RAGDevelop AI agents using Azure OpenAIAPI referenceｉREFERENCEChatClientBuilderIChatClientIEmbeddingGeneratorDevelop .NET apps with AI featuresArticle•05/02/2025With .NET, you can use artificial intelligence (AI) to automate and accomplish complex tasks inyour applications using the tools, platforms, and services that are familiar to you.Millions of developers use .NET to create applications that run on the web, on mobile anddesktop devices, or in the cloud. By using .NET to integrate AI into your applications, you cantake advantage of all that .NET has to offer:A unified story for building web UIs, APIs, and applications.Supported on Windows, macOS, and Linux.Is open-source and community-focused.Runs on top of the most popular web servers and cloud platforms.Provides powerful tooling to edit, debug, test, and deploy.The opportunities with AI are near endless. Here are a few examples of solutions you can buildusing AI in your .NET applications:Language processing: Create virtual agents or chatbots to talk with your data andgenerate content and images.Computer vision: Identify objects in an object or video.Audio generation: Use synthesized voices to interact with customers.Classification: Label the severity of a customer-reported issue.Task automation: Automatically perform the next step in a workflow as tasks arecompleted.We recommend the following sequence of tutorials and articles for an introduction todeveloping applications with AI and .NET:ScenarioTutorialCreate a chat applicationBuild an Azure AI chat app with .NETWhy choose .NET to build AI apps?What can you build with AI and .NET?Recommended learning pathﾉExpand tableScenarioTutorialSummarize textSummarize text using Azure AI chat app with .NETChat with your dataGet insight about your data from an .NET Azure AI chat appCall .NET functions with AIExtend Azure AI using tools and execute a local function with .NETGenerate imagesGenerate images using Azure AI with .NETTrain your own modelML.NET tutorialBrowse the table of contents to learn more about the core concepts, starting with Howgenerative AI and LLMs work.Quickstart: Build an Azure AI chat app with .NETVideo series: Machine Learning and AI with .NETNext stepsConnect to and prompt an AI modelArticle•05/18/2025In this quickstart, you learn how to create a .NET console chat app to connect to and promptan OpenAI or Azure OpenAI model. The app uses the Microsoft.Extensions.AI library so you canwrite code using AI abstractions rather than a specific SDK. AI abstractions enable you tochange the underlying AI model with minimal code changes..NET 8.0 SDK or higher - Install the .NET 8.0 SDK.An API key from OpenAI so you can run this sample.Complete the following steps to create a .NET console app to connect to an AI model.1. In an empty directory on your computer, use the dotnet new command to create a newconsole app:.NET CLI2. Change directory into the app folder:.NET CLI3. Install the required packages:BashPrerequisites７ NoteYou can also use Semantic Kernel to accomplish the tasks in this article. Semantic Kernel isa lightweight, open-source SDK that lets you build AI agents and integrate the latest AImodels into your .NET apps.Create the appdotnet new console -o ExtensionsAIcd ExtensionsAI4. Open the app in Visual Studio Code or your editor of choice.1. Navigate to the root of your .NET project from a terminal or command prompt.2. Run the following commands to configure your OpenAI API key as a secret for the sampleapp:BashThe app uses the Microsoft.Extensions.AI package to send and receive requests to the AImodel.1. Copy the benefits.md file to your project directory. Configure the project to copy thisfile to the output directory. If you're using Visual Studio, right-click on the file in SolutionExplorer, select Properties, and then set Copy to Output Directory to Copy if newer.2. In the Program.cs file, add the following code to connect and authenticate to the AImodel.C#dotnet add package OpenAIdotnet add package Microsoft.Extensions.AI.OpenAI --prereleasedotnet add package Microsoft.Extensions.Configurationdotnet add package Microsoft.Extensions.Configuration.UserSecretsConfigure the appdotnet user-secrets initdotnet user-secrets set OpenAIKey <your-OpenAI-key>dotnet user-secrets set ModelName <your-OpenAI-model-name>Add the app codeusing Microsoft.Extensions.AI;using Microsoft.Extensions.Configuration;using OpenAI;IConfigurationRoot config = new ConfigurationBuilder()    .AddUserSecrets<Program>()    .Build();string? model = config["ModelName"];string? key = config["OpenAIKey"];3. Add code to read the benefits.md file content and then create a prompt for the model.The prompt instructs the model to summarize the file's text content in 20 words or less.C#4. Call the GetResponseAsync method to send the prompt to the model to generate aresponse.C#5. Run the app:.NET CLIThe app prints out the completion response from the AI model. Customize the textcontent of the benefits.md file or the length of the summary to see the differences in theresponses.Quickstart - Build an AI chat app with .NETGenerate text and conversations with .NET and Azure OpenAI CompletionsIChatClient client =    new OpenAIClient(key).GetChatClient(model).AsIChatClient();string text = File.ReadAllText("benefits.md");string prompt = $"""    Summarize the the following text in 20 words or less:    {text}    """;// Submit the prompt and print out the response.ChatResponse response = await client.GetResponseAsync(    prompt,    new ChatOptions { MaxOutputTokens = 400 });Console.WriteLine(response);dotnet runNext steps.NET + AI ecosystem tools and SDKsArticle•05/29/2025The .NET ecosystem provides many powerful tools, libraries, and services to develop AIapplications. .NET supports both cloud and local AI model connections, many different SDKs forvarious AI and vector database services, and other tools to help you build intelligent apps ofvarying scope and complexity.Microsoft.Extensions.AI is a set of core .NET libraries that provide a unified layer of C#abstractions for interacting with AI services, such as small and large language models (SLMs andLLMs), embeddings, and middleware. These APIs were created in collaboration with developersacross the .NET ecosystem, including Semantic Kernel. The low-level APIs, such as IChatClientand IEmbeddingGenerator<TInput,TEmbedding>, were extracted from Semantic Kernel andmoved into the Microsoft.Extensions.AI namespace.Microsoft.Extensions.AI provides abstractions that can be implemented by various services, alladhering to the same core concepts. This library is not intended to provide APIs tailored to anyspecific provider's services. The goal of Microsoft.Extensions.AI is to act as a unifying layerwithin the .NET ecosystem, enabling developers to choose their preferred frameworks andlibraries while ensuring seamless integration and collaboration across the ecosystem.If you just want to use the low-level services, such as IChatClient andIEmbeddingGenerator<TInput,TEmbedding>, you can reference theMicrosoft.Extensions.AI.Abstractions package directly from your app. However, if you want touse higher-level, more opinionated approaches to AI, then you should use Semantic Kernel.Semantic Kernel, which has a dependency on the Microsoft.Extensions.AI.Abstractionspackage, is an open-source library that enables AI integration and orchestration capabilities inyour .NET apps. Its connectors provides concrete implementations of IChatClient and） ImportantNot all of the SDKs and services presented in this article are maintained by Microsoft.When considering an SDK, make sure to evaluate its quality, licensing, support, andcompatibility to ensure they meet your requirements.Microsoft.Extensions.AI librariesSemantic Kernel for .NETIEmbeddingGenerator<TInput,TEmbedding> for different services, including OpenAI, AmazonBedrock, and Google Gemini.The Semantic Kernel SDK is generally the recommended AI orchestration tool for .NET apps thatuse one or more AI services in combination with other APIs or web services, data stores, andcustom code. Semantic Kernel benefits enterprise developers in the following ways:Streamlines integration of AI capabilities into existing applications to enable a cohesivesolution for enterprise products.Minimizes the learning curve of working with different AI models or services by providingabstractions that reduce complexity.Improves reliability by reducing the unpredictable behavior of prompts and responsesfrom AI models. You can fine-tune prompts and plan tasks to create a controlled andpredictable user experience.For more information, see the Semantic Kernel documentation.Many different SDKs are available to build .NET apps with AI capabilities depending on thetarget platform or AI model. OpenAI models offer powerful generative AI capabilities, whileother Azure AI Services provide intelligent solutions for a variety of specific scenarios.NuGet packageSupported modelsMaintainer orvendorDocumentationMicrosoft.SemanticKernelOpenAI modelsAzure OpenAIsupported modelsSemantic Kernel(Microsoft)Semantic KerneldocumentationAzure OpenAI SDKAzure OpenAIsupported modelsAzure SDK for .NET(Microsoft)Azure OpenAI servicesdocumentationOpenAI SDKOpenAI supportedmodelsOpenAI SDK for.NET (OpenAI)OpenAI servicesdocumentationAzure offers many other AI services to build specific application capabilities and workflows.Most of these services provide a .NET SDK to integrate their functionality into custom apps..NET SDKs for building AI apps.NET SDKs for OpenAI modelsﾉExpand table.NET SDKs for Azure AI ServicesSome of the most commonly used services are shown in the following table. For a complete listof available services and learning resources, see the Azure AI Services documentation.ServiceDescriptionAzure AI SearchBring AI-powered cloud search to your mobile and web apps.Azure AI Content SafetyDetect unwanted or offensive content.Azure AI DocumentIntelligenceTurn documents into intelligent data-driven solutions.Azure AI LanguageBuild apps with industry-leading natural language understandingcapabilities.Azure AI SpeechSpeech to text, text to speech, translation, and speaker recognition.Azure AI TranslatorAI-powered translation technology with support for more than 100languages and dialects.Azure AI VisionAnalyze content in images and videos..NET apps can also connect to local AI models for many different development scenarios.Semantic Kernel is the recommended tool to connect to local models using .NET. SemanticKernel can connect to many different models hosted across a variety of platforms and abstractsaway lower-level implementation details.For example, you can use Ollama to connect to local AI models with .NET, including severalsmall language models (SLMs) developed by Microsoft:ModelDescriptionphi3 modelsA family of powerful SLMs with groundbreaking performance at low cost and low latency.orca modelsResearch models in tasks such as reasoning over user-provided data, readingcomprehension, math problem solving, and text summarization.ﾉExpand tableDevelop with local AI modelsﾉExpand table７ NoteThe preceding SLMs can also be hosted on other services, such as Azure.AI applications often use data vector databases and services to improve relevancy and providecustomized functionality. Many of these services provide a native SDK for .NET, while othersoffer a REST service you can connect to through custom code. Semantic Kernel provides anextensible component model that enables you to use different vector stores without needing tolearn each SDK.Semantic Kernel provides connectors for the following vector databases and services:VectorserviceSemantic Kernel connector.NET SDKAzure AISearchMicrosoft.SemanticKernel.Connectors.AzureAISearchAzure.Search.DocumentsAzureCosmos DBfor NoSQLMicrosoft.SemanticKernel.Connectors.AzureCosmosDBNoSQLMicrosoft.Azure.CosmosAzureCosmos DBforMongoDBMicrosoft.SemanticKernel.Connectors.AzureCosmosDBMongoDBMongoDb.DriverAzurePostgreSQLServerMicrosoft.SemanticKernel.Connectors.PostgresNpgsqlAzure SQLDatabaseMicrosoft.SemanticKernel.Connectors.SqlServerMicrosoft.Data.SqlClientChromaMicrosoft.SemanticKernel.Connectors.ChromaNADuckDBMicrosoft.SemanticKernel.Connectors.DuckDBDuckDB.NET.Data.FullMilvusMicrosoft.SemanticKernel.Connectors.MilvusMilvus.ClientMongoDBAtlasVectorSearchMicrosoft.SemanticKernel.Connectors.MongoDBMongoDb.DriverPineconeMicrosoft.SemanticKernel.Connectors.PineconeREST APIPostgresMicrosoft.SemanticKernel.Connectors.PostgresNpgsqlQdrantMicrosoft.SemanticKernel.Connectors.QdrantQdrant.ClientConnect to vector databases and servicesﾉExpand tableVectorserviceSemantic Kernel connector.NET SDKRedisMicrosoft.SemanticKernel.Connectors.RedisStackExchange.RedisWeaviateMicrosoft.SemanticKernel.Connectors.WeaviateREST APITo discover .NET SDK and API support, visit the documentation for each respective service.What is Semantic Kernel?Quickstart - Summarize text using Azure AI chat app with .NETNext stepsMicrosoft.Extensions.AI libraries06/23/2025.NET developers need to integrate and interact with a growing variety of artificial intelligence(AI) services in their apps. The Microsoft.Extensions.AI libraries provide a unified approach forrepresenting generative AI components, and enable seamless integration and interoperabilitywith various AI services. This article introduces the libraries and provides in-depth usageexamples to help you get started.The 📦 Microsoft.Extensions.AI.Abstractions package provides the core exchange types,including IChatClient and IEmbeddingGenerator<TInput,TEmbedding>. Any .NET library thatprovides an LLM client can implement the IChatClient interface to enable seamlessintegration with consuming code.The 📦 Microsoft.Extensions.AI package has an implicit dependency on theMicrosoft.Extensions.AI.Abstractions package. This package enables you to easily integratecomponents such as automatic function tool invocation, telemetry, and caching into yourapplications using familiar dependency injection and middleware patterns. For example, itprovides the UseOpenTelemetry(ChatClientBuilder, ILoggerFactory, String,Action<OpenTelemetryChatClient>) extension method, which adds OpenTelemetry support tothe chat client pipeline.Libraries that provide implementations of the abstractions typically reference onlyMicrosoft.Extensions.AI.Abstractions.To also have access to higher-level utilities for working with generative AI components,reference the Microsoft.Extensions.AI package instead (which itself referencesMicrosoft.Extensions.AI.Abstractions). Most consuming applications and services shouldreference the Microsoft.Extensions.AI package along with one or more libraries that provideconcrete implementations of the abstractions.For information about how to install NuGet packages, see dotnet package add or Managepackage dependencies in .NET applications.The packagesWhich package to referenceInstall the packagesThe following subsections show specific IChatClient usage examples:Request a chat responseRequest a streaming chat responseTool callingCache responsesUse telemetryProvide optionsPipelines of functionalityCustom IChatClient middlewareDependency injectionStateless vs. stateful clientsThe following sections show specific IEmbeddingGenerator usage examples:Create embeddingsPipelines of functionalityThe IChatClient interface defines a client abstraction responsible for interacting with AI servicesthat provide chat capabilities. It includes methods for sending and receiving messages withmulti-modal content (such as text, images, and audio), either as a complete set or streamedincrementally. Additionally, it allows for retrieving strongly typed services provided by the clientor its underlying services..NET libraries that provide clients for language models and services can provide animplementation of the IChatClient interface. Any consumers of the interface are then able tointeroperate seamlessly with these models and services via the abstractions. You can see asimple implementation at Sample implementations of IChatClient and IEmbeddingGenerator.With an instance of IChatClient, you can call the IChatClient.GetResponseAsync method to senda request and get a response. The request is composed of one or more messages, each ofwhich is composed of one or more pieces of content. Accelerator methods exist to simplifycommon cases, such as constructing a request for a single piece of text content.C#API usage examplesThe IChatClient interfaceRequest a chat responseThe core IChatClient.GetResponseAsync method accepts a list of messages. This list representsthe history of all messages that are part of the conversation.C#The ChatResponse that's returned from GetResponseAsync exposes a list of ChatMessageinstances that represent one or more messages generated as part of the operation. In commoncases, there is only one response message, but in some situations, there can be multiplemessages. The message list is ordered, such that the last message in the list represents the finalmessage to the request. To provide all of those response messages back to the service in asubsequent request, you can add the messages from the response back into the messages list.C#The inputs to IChatClient.GetStreamingResponseAsync are identical to those ofGetResponseAsync. However, rather than returning the complete response as part of ausing Microsoft.Extensions.AI;using OllamaSharp;IChatClient client = new OllamaApiClient(    new Uri("http://localhost:11434/"), "phi3:mini");Console.WriteLine(await client.GetResponseAsync("What is AI?"));Console.WriteLine(await client.GetResponseAsync([    new(ChatRole.System, "You are a helpful AI assistant"),    new(ChatRole.User, "What is AI?"),]));List<ChatMessage> history = [];while (true){    Console.Write("Q: ");    history.Add(new(ChatRole.User, Console.ReadLine()));    ChatResponse response = await client.GetResponseAsync(history);    Console.WriteLine(response);    history.AddMessages(response);}Request a streaming chat responseChatResponse object, the method returns an IAsyncEnumerable<T> where T isChatResponseUpdate, providing a stream of updates that collectively form the single response.C#As with GetResponseAsync, you can add the updates fromIChatClient.GetStreamingResponseAsync back into the messages list. Because the updates areindividual pieces of a response, you can use helpers likeToChatResponse(IEnumerable<ChatResponseUpdate>) to compose one or more updates backinto a single ChatResponse instance.Helpers like AddMessages compose a ChatResponse and then extract the composed messagesfrom the response and add them to a list.C#await foreach (ChatResponseUpdate update in client.GetStreamingResponseAsync("What is AI?")){    Console.Write(update);} TipStreaming APIs are nearly synonymous with AI user experiences. C# enables compellingscenarios with its IAsyncEnumerable<T> support, allowing for a natural and efficient way tostream data.List<ChatMessage> chatHistory = [];while (true){    Console.Write("Q: ");    chatHistory.Add(new(ChatRole.User, Console.ReadLine()));    List<ChatResponseUpdate> updates = [];    await foreach (ChatResponseUpdate update in        client.GetStreamingResponseAsync(history))    {        Console.Write(update);        updates.Add(update);    }    Console.WriteLine();    chatHistory.AddMessages(updates);}Some models and services support tool calling. To gather additional information, you canconfigure the ChatOptions with information about tools (usually .NET methods) that the modelcan request the client to invoke. Instead of sending a final response, the model requests afunction invocation with specific arguments. The client then invokes the function and sends theresults back to the model with the conversation history. TheMicrosoft.Extensions.AI.Abstractions library includes abstractions for various messagecontent types, including function call requests and results. While IChatClient consumers caninteract with this content directly, Microsoft.Extensions.AI provides helpers that can enableautomatically invoking the tools in response to corresponding requests. TheMicrosoft.Extensions.AI.Abstractions and Microsoft.Extensions.AI libraries provide thefollowing types:AIFunction: Represents a function that can be described to an AI model and invoked.AIFunctionFactory: Provides factory methods for creating AIFunction instances thatrepresent .NET methods.FunctionInvokingChatClient: Wraps an IChatClient as another IChatClient that addsautomatic function-invocation capabilities.The following example demonstrates a random function invocation (this example depends onthe 📦 OllamaSharp NuGet package):C#Tool callingusing Microsoft.Extensions.AI;using OllamaSharp;string GetCurrentWeather() => Random.Shared.NextDouble() > 0.5 ? "It's sunny" : "It's raining";IChatClient client = new OllamaApiClient(new Uri("http://localhost:11434"), "llama3.1");client = ChatClientBuilderChatClientExtensions    .AsBuilder(client)    .UseFunctionInvocation()    .Build();ChatOptions options = new() { Tools = [AIFunctionFactory.Create(GetCurrentWeather)] };var response = client.GetStreamingResponseAsync("Should I wear a rain coat?", options);await foreach (var update in response){The preceding code:Defines a function named GetCurrentWeather that returns a random weather forecast.Instantiates a ChatClientBuilder with an OllamaSharp.OllamaApiClient and configures it touse function invocation.Calls GetStreamingResponseAsync on the client, passing a prompt and a list of tools thatincludes a function created with Create.Iterates over the response, printing each update to the console.If you're familiar with Caching in .NET, it's good to know that Microsoft.Extensions.AI providesother such delegating IChatClient implementations. The DistributedCachingChatClient is anIChatClient that layers caching around another arbitrary IChatClient instance. When a novelchat history is submitted to the DistributedCachingChatClient, it forwards it to the underlyingclient and then caches the response before sending it back to the consumer. The next time thesame history is submitted, such that a cached response can be found in the cache, theDistributedCachingChatClient returns the cached response rather than forwarding the requestalong the pipeline.C#    Console.Write(update);}Cache responsesusing Microsoft.Extensions.AI;using Microsoft.Extensions.Caching.Distributed;using Microsoft.Extensions.Caching.Memory;using Microsoft.Extensions.Options;using OllamaSharp;var sampleChatClient = new OllamaApiClient(new Uri("http://localhost:11434"), "llama3.1");IChatClient client = new ChatClientBuilder(sampleChatClient)    .UseDistributedCache(new MemoryDistributedCache(        Options.Create(new MemoryDistributedCacheOptions())))    .Build();string[] prompts = ["What is AI?", "What is .NET?", "What is AI?"];foreach (var prompt in prompts){    await foreach (var update in client.GetStreamingResponseAsync(prompt))    {        Console.Write(update);    }This example depends on the 📦 Microsoft.Extensions.Caching.Memory NuGet package. Formore information, see Caching in .NET.Another example of a delegating chat client is the OpenTelemetryChatClient. Thisimplementation adheres to the OpenTelemetry Semantic Conventions for Generative AIsystems. Similar to other IChatClient delegators, it layers metrics and spans around otherarbitrary IChatClient implementations.C#(The preceding example depends on the 📦 OpenTelemetry.Exporter.Console NuGetpackage.)Alternatively, the LoggingChatClient and corresponding UseLogging(ChatClientBuilder,ILoggerFactory, Action<LoggingChatClient>) method provide a simple way to write log entriesto an ILogger for every request and response.    Console.WriteLine();}Use telemetryusing Microsoft.Extensions.AI;using OllamaSharp;using OpenTelemetry.Trace;// Configure OpenTelemetry exporter.string sourceName = Guid.NewGuid().ToString();TracerProvider tracerProvider = OpenTelemetry.Sdk.CreateTracerProviderBuilder()    .AddSource(sourceName)    .AddConsoleExporter()    .Build();IChatClient ollamaClient = new OllamaApiClient(    new Uri("http://localhost:11434/"), "phi3:mini");IChatClient client = new ChatClientBuilder(ollamaClient)    .UseOpenTelemetry(        sourceName: sourceName,        configure: c => c.EnableSensitiveData = true)    .Build();Console.WriteLine((await client.GetResponseAsync("What is AI?")).Text);Provide optionsEvery call to GetResponseAsync or GetStreamingResponseAsync can optionally supply aChatOptions instance containing additional parameters for the operation. The most commonparameters among AI models and services show up as strongly typed properties on the type,such as ChatOptions.Temperature. Other parameters can be supplied by name in a weaklytyped manner, via the ChatOptions.AdditionalProperties dictionary, or via an options instancethat the underlying provider understands, via the ChatOptions.RawRepresentationFactoryproperty.You can also specify options when building an IChatClient with the fluent ChatClientBuilderAPI by chaining a call to the ConfigureOptions(ChatClientBuilder, Action<ChatOptions>)extension method. This delegating client wraps another client and invokes the supplieddelegate to populate a ChatOptions instance for every call. For example, to ensure that theChatOptions.ModelId property defaults to a particular model name, you can use code like thefollowing:C#IChatClient instances can be layered to create a pipeline of components that each addadditional functionality. These components can come from Microsoft.Extensions.AI, otherNuGet packages, or custom implementations. This approach allows you to augment thebehavior of the IChatClient in various ways to meet your specific needs. Consider thefollowing code snippet that layers a distributed cache, function invocation, and OpenTelemetrytracing around a sample chat client:C#using Microsoft.Extensions.AI;using OllamaSharp;IChatClient client = new OllamaApiClient(new Uri("http://localhost:11434"));client = ChatClientBuilderChatClientExtensions.AsBuilder(client)    .ConfigureOptions(options => options.ModelId ??= "phi3")    .Build();// Will request "phi3".Console.WriteLine(await client.GetResponseAsync("What is AI?"));// Will request "llama3.1".Console.WriteLine(await client.GetResponseAsync("What is AI?", new() { ModelId = "llama3.1" }));Functionality pipelines// Explore changing the order of the intermediate "Use" calls.IChatClient client = new ChatClientBuilder(new OllamaApiClient(new To add additional functionality, you can implement IChatClient directly or use theDelegatingChatClient class. This class serves as a base for creating chat clients that delegateoperations to another IChatClient instance. It simplifies chaining multiple clients, allowingcalls to pass through to an underlying client.The DelegatingChatClient class provides default implementations for methods likeGetResponseAsync, GetStreamingResponseAsync, and Dispose, which forward calls to the innerclient. A derived class can then override only the methods it needs to augment the behavior,while delegating other calls to the base implementation. This approach is useful for creatingflexible and modular chat clients that are easy to extend and compose.The following is an example class derived from DelegatingChatClient that uses theSystem.Threading.RateLimiting library to provide rate-limiting functionality.C#Uri("http://localhost:11434"), "llama3.1"))    .UseDistributedCache(new MemoryDistributedCache(Options.Create(new MemoryDistributedCacheOptions())))    .UseFunctionInvocation()    .UseOpenTelemetry(sourceName: sourceName, configure: c => c.EnableSensitiveData = true)    .Build();Custom IChatClient middlewareusing Microsoft.Extensions.AI;using System.Runtime.CompilerServices;using System.Threading.RateLimiting;public sealed class RateLimitingChatClient(    IChatClient innerClient, RateLimiter rateLimiter)        : DelegatingChatClient(innerClient){    public override async Task<ChatResponse> GetResponseAsync(        IEnumerable<ChatMessage> messages,        ChatOptions? options = null,        CancellationToken cancellationToken = default)    {        using var lease = await rateLimiter.AcquireAsync(permitCount: 1, cancellationToken)            .ConfigureAwait(false);        if (!lease.IsAcquired)            throw new InvalidOperationException("Unable to acquire lease.");        return await base.GetResponseAsync(messages, options, cancellationToken)            .ConfigureAwait(false);    }As with other IChatClient implementations, the RateLimitingChatClient can be composed:C#To simplify the composition of such components with others, component authors should createa Use* extension method for registering the component into a pipeline. For example, considerthe following UseRatingLimiting extension method:C#    public override async IAsyncEnumerable<ChatResponseUpdate> GetStreamingResponseAsync(        IEnumerable<ChatMessage> messages,        ChatOptions? options = null,        [EnumeratorCancellation] CancellationToken cancellationToken = default)    {        using var lease = await rateLimiter.AcquireAsync(permitCount: 1, cancellationToken)            .ConfigureAwait(false);        if (!lease.IsAcquired)            throw new InvalidOperationException("Unable to acquire lease.");        await foreach (var update in base.GetStreamingResponseAsync(messages, options, cancellationToken)            .ConfigureAwait(false))        {            yield return update;        }    }    protected override void Dispose(bool disposing)    {        if (disposing)            rateLimiter.Dispose();        base.Dispose(disposing);    }}using Microsoft.Extensions.AI;using OllamaSharp;using System.Threading.RateLimiting;var client = new RateLimitingChatClient(    new OllamaApiClient(new Uri("http://localhost:11434"), "llama3.1"),    new ConcurrencyLimiter(new() { PermitLimit = 1, QueueLimit = int.MaxValue }));Console.WriteLine(await client.GetResponseAsync("What color is the sky?"));Such extensions can also query for relevant services from the DI container; the IServiceProviderused by the pipeline is passed in as an optional parameter:C#Now it's easy for the consumer to use this in their pipeline, for example:C#using Microsoft.Extensions.AI;using System.Threading.RateLimiting;public static class RateLimitingChatClientExtensions{    public static ChatClientBuilder UseRateLimiting(        this ChatClientBuilder builder,        RateLimiter rateLimiter) =>        builder.Use(innerClient =>            new RateLimitingChatClient(innerClient, rateLimiter)        );}using Microsoft.Extensions.AI;using Microsoft.Extensions.DependencyInjection;using System.Threading.RateLimiting;public static class RateLimitingChatClientExtensions{    public static ChatClientBuilder UseRateLimiting(        this ChatClientBuilder builder,        RateLimiter? rateLimiter = null) =>        builder.Use((innerClient, services) =>            new RateLimitingChatClient(                innerClient,                services.GetRequiredService<RateLimiter>())        );}HostApplicationBuilder builder = Host.CreateApplicationBuilder(args);IChatClient client = new OllamaApiClient(    new Uri("http://localhost:11434/"),    "phi3:mini");builder.Services.AddChatClient(services =>        client        .AsBuilder()        .UseDistributedCache()        .UseRateLimiting()The previous extension methods demonstrate using a Use method on ChatClientBuilder.ChatClientBuilder also provides Use overloads that make it easier to write such delegatinghandlers. For example, in the earlier RateLimitingChatClient example, the overrides ofGetResponseAsync and GetStreamingResponseAsync only need to do work before and afterdelegating to the next client in the pipeline. To achieve the same thing without writing acustom class, you can use an overload of Use that accepts a delegate that's used for bothGetResponseAsync and GetStreamingResponseAsync, reducing the boilerplate required:C#For scenarios where you need a different implementation for GetResponseAsync andGetStreamingResponseAsync in order to handle their unique return types, you can use theUse(Func<IEnumerable<ChatMessage>,ChatOptions,IChatClient,CancellationToken,Task<ChatResponse>>, Func<IEnumerable<ChatMessage>,ChatOptions,IChatClient,CancellationToken,IAsyncEnumerable<ChatResponseUpdate>>) overload thataccepts a delegate for each.        .UseOpenTelemetry()        .Build(services));using Microsoft.Extensions.AI;using OllamaSharp;using System.Threading.RateLimiting;RateLimiter rateLimiter = new ConcurrencyLimiter(new(){    PermitLimit = 1,    QueueLimit = int.MaxValue});IChatClient client = new OllamaApiClient(new Uri("http://localhost:11434"), "llama3.1");client = ChatClientBuilderChatClientExtensions    .AsBuilder(client)    .UseDistributedCache()    .Use(async (messages, options, nextAsync, cancellationToken) =>    {        using var lease = await rateLimiter.AcquireAsync(permitCount: 1, cancellationToken).ConfigureAwait(false);        if (!lease.IsAcquired)            throw new InvalidOperationException("Unable to acquire lease.");        await nextAsync(messages, options, cancellationToken);    })    .UseOpenTelemetry()    .Build();IChatClient implementations are often provided to an application via dependency injection (DI).In this example, an IDistributedCache is added into the DI container, as is an IChatClient. Theregistration for the IChatClient uses a builder that creates a pipeline containing a cachingclient (which then uses an IDistributedCache retrieved from DI) and the sample client. Theinjected IChatClient can be retrieved and used elsewhere in the app.C#What instance and configuration is injected can differ based on the current needs of theapplication, and multiple pipelines can be injected with different keys.Stateless services require all relevant conversation history to be sent back on every request. Incontrast, stateful services keep track of the history and require only additional messages to besent with a request. The IChatClient interface is designed to handle both stateless and statefulAI services.When working with a stateless service, callers maintain a list of all messages. They add in allreceived response messages and provide the list back on subsequent interactions.C#Dependency injectionusing Microsoft.Extensions.AI;using Microsoft.Extensions.DependencyInjection;using Microsoft.Extensions.Hosting;using OllamaSharp;// App setup.var builder = Host.CreateApplicationBuilder();builder.Services.AddDistributedMemoryCache();builder.Services.AddChatClient(new OllamaApiClient(new Uri("http://localhost:11434"), "llama3.1"))    .UseDistributedCache();var host = builder.Build();// Elsewhere in the app.var chatClient = host.Services.GetRequiredService<IChatClient>();Console.WriteLine(await chatClient.GetResponseAsync("What is AI?"));Stateless vs. stateful clientsList<ChatMessage> history = [];while (true){    Console.Write("Q: ");    history.Add(new(ChatRole.User, Console.ReadLine()));For stateful services, you might already know the identifier used for the relevant conversation.You can put that identifier into ChatOptions.ConversationId. Usage then follows the samepattern, except there's no need to maintain a history manually.C#Some services might support automatically creating a conversation ID for a request thatdoesn't have one, or creating a new conversation ID that represents the current state of theconversation after incorporating the last round of messages. In such cases, you can transfer theChatResponse.ConversationId over to the ChatOptions.ConversationId for subsequentrequests. For example:C#If you don't know ahead of time whether the service is stateless or stateful, you can check theresponse ConversationId and act based on its value. If it's set, then that value is propagated tothe options and the history is cleared so as to not resend the same history again. If theresponse ConversationId isn't set, then the response message is added to the history so thatit's sent back to the service on the next turn.    var response = await client.GetResponseAsync(history);    Console.WriteLine(response);    history.AddMessages(response);}ChatOptions statefulOptions = new() { ConversationId = "my-conversation-id" };while (true){    Console.Write("Q: ");    ChatMessage message = new(ChatRole.User, Console.ReadLine());    Console.WriteLine(await client.GetResponseAsync(message, statefulOptions));}ChatOptions options = new();while (true){    Console.Write("Q: ");    ChatMessage message = new(ChatRole.User, Console.ReadLine());    ChatResponse response = await client.GetResponseAsync(message, options);    Console.WriteLine(response);    options.ConversationId = response.ConversationId;}C#The IEmbeddingGenerator<TInput,TEmbedding> interface represents a generic generator ofembeddings. For the generic type parameters, TInput is the type of input values beingembedded, and TEmbedding is the type of generated embedding, which inherits from theEmbedding class.The Embedding class serves as a base class for embeddings generated by anIEmbeddingGenerator. It's designed to store and manage the metadata and data associatedwith embeddings. Derived types, like Embedding<T>, provide the concrete embedding vectordata. For example, an Embedding<float> exposes a ReadOnlyMemory<float> Vector { get; }property for access to its embedding data.The IEmbeddingGenerator interface defines a method to asynchronously generate embeddingsfor a collection of input values, with optional configuration and cancellation support. It alsoprovides metadata describing the generator and allows for the retrieval of strongly typedservices that can be provided by the generator or its underlying services.Most users don't need to implement the IEmbeddingGenerator interface. However, if you're alibrary author, you can see a simple implementation at Sample implementations of IChatClientand IEmbeddingGenerator.List<ChatMessage> chatHistory = [];ChatOptions chatOptions = new();while (true){    Console.Write("Q: ");    chatHistory.Add(new(ChatRole.User, Console.ReadLine()));    ChatResponse response = await client.GetResponseAsync(chatHistory);    Console.WriteLine(response);    chatOptions.ConversationId = response.ConversationId;    if (response.ConversationId is not null)    {        chatHistory.Clear();    }    else    {        chatHistory.AddMessages(response);    }}The IEmbeddingGenerator interfaceThe primary operation performed with an IEmbeddingGenerator<TInput,TEmbedding> isembedding generation, which is accomplished with its GenerateAsync method.C#Accelerator extension methods also exist to simplify common cases, such as generating anembedding vector from a single input.C#As with IChatClient, IEmbeddingGenerator implementations can be layered.Microsoft.Extensions.AI provides a delegating implementation for IEmbeddingGenerator forcaching and telemetry.C#Create embeddingsusing Microsoft.Extensions.AI;using OllamaSharp;IEmbeddingGenerator<string, Embedding<float>> generator =    new OllamaApiClient(new Uri("http://localhost:11434/"), "phi3:mini");foreach (Embedding<float> embedding in    await generator.GenerateAsync(["What is AI?", "What is .NET?"])){    Console.WriteLine(string.Join(", ", embedding.Vector.ToArray()));}ReadOnlyMemory<float> vector = await generator.GenerateVectorAsync("What is AI?");Pipelines of functionalityusing Microsoft.Extensions.AI;using Microsoft.Extensions.Caching.Distributed;using Microsoft.Extensions.Caching.Memory;using Microsoft.Extensions.Options;using OllamaSharp;using OpenTelemetry.Trace;// Configure OpenTelemetry exporterstring sourceName = Guid.NewGuid().ToString();TracerProvider tracerProvider = OpenTelemetry.Sdk.CreateTracerProviderBuilder()    .AddSource(sourceName)    .AddConsoleExporter()    .Build();The IEmbeddingGenerator enables building custom middleware that extends the functionality ofan IEmbeddingGenerator. The DelegatingEmbeddingGenerator<TInput,TEmbedding> class is animplementation of the IEmbeddingGenerator<TInput, TEmbedding> interface that serves as abase class for creating embedding generators that delegate their operations to anotherIEmbeddingGenerator<TInput, TEmbedding> instance. It allows for chaining multiple generatorsin any order, passing calls through to an underlying generator. The class provides defaultimplementations for methods such as GenerateAsync and Dispose, which forward the calls tothe inner generator instance, enabling flexible and modular embedding generation.The following is an example implementation of such a delegating embedding generator thatrate-limits embedding generation requests:C#// Explore changing the order of the intermediate "Use" calls to see// what impact that has on what gets cached and traced.IEmbeddingGenerator<string, Embedding<float>> generator = new EmbeddingGeneratorBuilder<string, Embedding<float>>(        new OllamaApiClient(new Uri("http://localhost:11434/"), "phi3:mini"))    .UseDistributedCache(        new MemoryDistributedCache(            Options.Create(new MemoryDistributedCacheOptions())))    .UseOpenTelemetry(sourceName: sourceName)    .Build();GeneratedEmbeddings<Embedding<float>> embeddings = await generator.GenerateAsync([    "What is AI?",    "What is .NET?",    "What is AI?"]);foreach (Embedding<float> embedding in embeddings){    Console.WriteLine(string.Join(", ", embedding.Vector.ToArray()));}using Microsoft.Extensions.AI;using System.Threading.RateLimiting;public class RateLimitingEmbeddingGenerator(    IEmbeddingGenerator<string, Embedding<float>> innerGenerator, RateLimiter rateLimiter)        : DelegatingEmbeddingGenerator<string, Embedding<float>>(innerGenerator){    public override async Task<GeneratedEmbeddings<Embedding<float>>> GenerateAsync(        IEnumerable<string> values,        EmbeddingGenerationOptions? options = null,        CancellationToken cancellationToken = default)This can then be layered around an arbitrary IEmbeddingGenerator<string, Embedding<float>>to rate limit all embedding generation operations.C#In this way, the RateLimitingEmbeddingGenerator can be composed with otherIEmbeddingGenerator<string, Embedding<float>> instances to provide rate-limitingfunctionality.    {        using var lease = await rateLimiter.AcquireAsync(permitCount: 1, cancellationToken)            .ConfigureAwait(false);        if (!lease.IsAcquired)        {            throw new InvalidOperationException("Unable to acquire lease.");        }        return await base.GenerateAsync(values, options, cancellationToken);    }    protected override void Dispose(bool disposing)    {        if (disposing)        {            rateLimiter.Dispose();        }        base.Dispose(disposing);    }}using Microsoft.Extensions.AI;using OllamaSharp;using System.Threading.RateLimiting;IEmbeddingGenerator<string, Embedding<float>> generator =    new RateLimitingEmbeddingGenerator(        new OllamaApiClient(new Uri("http://localhost:11434/"), "phi3:mini"),        new ConcurrencyLimiter(new()        {            PermitLimit = 1,            QueueLimit = int.MaxValue        }));foreach (Embedding<float> embedding in    await generator.GenerateAsync(["What is AI?", "What is .NET?"])){    Console.WriteLine(string.Join(", ", embedding.Vector.ToArray()));}You can start building with Microsoft.Extensions.AI in the following ways:Library developers: If you own libraries that provide clients for AI services, considerimplementing the interfaces in your libraries. This allows users to easily integrate yourNuGet package via the abstractions. For example implementations, see Sampleimplementations of IChatClient and IEmbeddingGenerator.Service consumers: If you're developing libraries that consume AI services, use theabstractions instead of hardcoding to a specific AI service. This approach gives yourconsumers the flexibility to choose their preferred provider.Application developers: Use the abstractions to simplify integration into your apps. Thisenables portability across models and services, facilitates testing and mocking, leveragesmiddleware provided by the ecosystem, and maintains a consistent API throughout yourapp, even if you use different services in different parts of your application.Ecosystem contributors: If you're interested in contributing to the ecosystem, considerwriting custom middleware components.For more samples, see the dotnet/ai-samples GitHub repository. For an end-to-end sample,see eShopSupport.Request a response with structured outputBuild an AI chat app with .NETDependency injection in .NETCaching in .NETRate limit an HTTP handler in .NETBuild with Microsoft.Extensions.AISee alsoSemantic Kernel overview for .NETArticle•04/09/2025In this article, you explore Semantic Kernel core concepts and capabilities. Semantic Kernel is apowerful and recommended choice for working with AI in .NET applications. In the sectionsahead, you learn:How to add semantic kernel to your projectSemantic Kernel core conceptsThis article serves as an introductory overview of Semantic Kernel specifically in the context of.NET. For more comprehensive information and training about Semantic Kernel, see thefollowing resources:Semantic Kernel documentationSemantic Kernel trainingThe Semantic Kernel SDK is available as a NuGet package for .NET and integrates with standardapp configurations.Install the Microsoft.SemanticKernel package using the following command:.NET CLIOr, in .NET 10+:.NET CLIAdd Semantic Kernel to a .NET projectdotnet add package Microsoft.SemanticKerneldotnet package add Microsoft.SemanticKernel７ NoteAlthough Microsoft.SemanticKernel provides core features of Semantic Kernel, additionalcapabilities require you to install additional packages. For example, theMicrosoft.SemanticKernel.Plugins.Memory package provides to access memory relatedfeatures. For more information, see the Semantic Kernel documentation.Create and configure a Kernel instance using the KernelBuilder class to access and work withSemantic Kernel. The Kernel holds services, data, and connections to orchestrate integrationsbetween your code and AI models.Configure the Kernel in a .NET console app:C#Configure the Kernel in an ASP.NET Core app:C#Semantic Kernel is an open-source SDK that integrates and orchestrates AI models and serviceslike OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages likeC#, Python, and Java.The Semantic Kernel SDK benefits enterprise developers in the following ways:Streamlines integration of AI capabilities into existing applications to enable a cohesivesolution for enterprise products.Minimizes the learning curve of working with different AI models or services by providingabstractions that reduce complexity.Improves reliability by reducing the unpredictable behavior of prompts and responsesfrom AI models. You can fine-tune prompts and plan tasks to create a controlled andpredictable user experience.Semantic Kernel is built around several core concepts:Connections: Interface with external AI services and data sources.Plugins: Encapsulate functions that applications can use.var builder = Kernel.CreateBuilder();// Add builder configuration and servicesvar kernel = builder.Build();var builder = WebApplication.CreateBuilder();builder.Services.AddKernel();// Add builder configuration and servicesvar app = builder.Build();Understand Semantic KernelPlanner: Orchestrates execution plans and strategies based on user behavior.Memory: Abstracts and simplifies context management for AI apps.These building blocks are explored in more detail in the following sections.The Semantic Kernel SDK includes a set of connectors that enable developers to integrate LLMsand other services into their existing applications. These connectors serve as the bridgebetween the application code and the AI models or services. Semantic Kernel handles manycommon connection concerns and challenges for you so you can focus on building your ownworkflows and features.The following code snippet creates a Kernel and adds a connection to an Azure OpenAImodel:C#Semantic Kernel plugins encapsulate standard language functions for applications and AImodels to consume. You can create your own plugins or rely on plugins provided by the SDK.These plugins streamline tasks where AI models are advantageous and efficiently combinethem with more traditional C# methods. Plugin functions are generally categorized into twotypes: semantic functions and native functions.Semantic functions are essentially AI prompts defined in your code that Semantic Kernel cancustomize and call as needed. You can templatize these prompts to use variables, customprompt and completion formatting, and more.Connectionsusing Microsoft.SemanticKernel;// Create kernelvar builder = Kernel.CreateBuilder();// Add a chat completion service:builder.Services.AddAzureOpenAIChatCompletion(    "your-resource-name",    "your-endpoint",    "your-resource-key",    "deployment-model");var kernel = builder.Build();PluginsSemantic functionsThe following code snippet defines and registers a semantic function:C#Native functions are C# methods that Semantic Kernel can call directly to manipulate orretrieve data. They perform operations that are better suited for traditional code instructionsinstead of LLM prompts.The following code snippet defines and registers a native function:C#var userInput = Console.ReadLine();// Define semantic function inline.string skPrompt = @"Summarize the provided unstructured text in a sentence that is easy to understand.                    Text to summarize: {{$userInput}}";// Register the functionkernel.CreateSemanticFunction(    promptTemplate: skPrompt,    functionName: "SummarizeText",    pluginName: "SemanticFunctions");Native functions// Define native functionpublic class NativeFunctions {    [SKFunction, Description("Retrieve content from local file")]    public async Task<string> RetrieveLocalFile(string fileName, int maxSize = 5000)    {        string content = await File.ReadAllTextAsync(fileName);        if (content.Length <= maxSize) return content;        return content.Substring(0, maxSize);    }}//Import native functionstring plugInName = "NativeFunction";string functionName = "RetrieveLocalFile";var nativeFunctions = new NativeFunctions();kernel.ImportFunctions(nativeFunctions, plugInName);The planner is a core component of Semantic Kernel that provides AI orchestration to manageseamless integration between AI models and plugins. This layer devises execution strategiesfrom user requests and dynamically orchestrates Plugins to perform complex tasks with AI-assisted planning.Consider the following pseudo-code snippet:C#The preceding code creates an executable, sequential plan to read content from a local file andsummarize the content. The plan sets up instructions to read the file using a native functionand then analyze it using an AI model.Semantic Kernel's Vector stores provide abstractions over embedding models, vectordatabases, and other data to simplify context management for AI applications. Vector storesare agnostic to the underlying LLM or Vector database, offering a uniform developerexperience. You can configure memory features to store data in a variety of sources or service,including Azure AI Search and Azure Cache for Redis.Consider the following code snippet:C#Planner// Native function definition and kernel configuration code omitted for brevity// Configure and create the planstring planDefinition = "Read content from a local file and summarize the content.";SequentialPlanner sequentialPlanner = new SequentialPlanner(kernel);string assetsFolder = @"../../assets";string fileName = Path.Combine(assetsFolder,"docs","06_SemanticKernel", "aci_documentation.txt");ContextVariables contextVariables = new ContextVariables();contextVariables.Add("fileName", fileName);var customPlan = await sequentialPlanner.CreatePlanAsync(planDefinition);// Execute the planKernelResult kernelResult = await kernel.RunAsync(contextVariables, customPlan);Console.WriteLine($"Summarization: {kernelResult.GetValue<string>()}");MemoryThe preceding code loads a set of facts into memory so that the data is available to use wheninteracting with AI models and orchestrating tasks. var facts = new Dictionary<string,string>();facts.Add(    "Azure Machine Learning; https://learn.microsoft.com/en-us/azure/machine-learning/",    @"Azure Machine Learning is a cloud service for accelerating and    managing the machine learning project lifecycle. Machine learning professionals,    data scientists, and engineers can use it in their day-to-day workflows");facts.Add(    "Azure SQL Service; https://learn.microsoft.com/en-us/azure/azure-sql/",    @"Azure SQL is a family of managed, secure, and intelligent products    that use the SQL Server database engine in the Azure cloud.");string memoryCollectionName = "SummarizedAzureDocs";foreach (var fact in facts) {    await memoryBuilder.SaveReferenceAsync(        collection: memoryCollectionName,        description: fact.Key.Split(";")[0].Trim(),        text: fact.Value,        externalId: fact.Key.Split(";")[1].Trim(),        externalSourceName: "Azure Documentation"    );}Quickstart - Summarize text with OpenAIQuickstart - Chat with your dataGet started with .NET AI and the ModelContext ProtocolArticle•05/05/2025The Model Context Protocol (MCP) is an open protocol designed to standardize integrationsbetween AI apps and external tools and data sources. By using MCP, developers can enhancethe capabilities of AI models, enabling them to produce more accurate, relevant, and context-aware responses.For example, using MCP, you can connect your LLM to resources such as:Document databases or storage services.Web APIs that expose business data or logic.Tools that manage files or performing local tasks on a user's device.Many Microsoft products already support MCP, including:Copilot StudioVisual Studio Code GitHub Copilot agent modeSemantic Kernel.You can use the MCP C# SDK to quickly create your own MCP integrations and switch betweendifferent AI models without significant code changes.MCP uses a client-server architecture that enables an AI-powered app (the host) to connect tomultiple MCP servers through MCP clients:MCP Hosts: AI tools, code editors, or other software that enhance their AI models usingcontextual resources through MCP. For example, GitHub Copilot in Visual Studio Codecan act as an MCP host and use MCP clients and servers to expand its capabilities.MCP Clients: Clients used by the host application to connect to MCP servers to retrievecontextual data.MCP Servers: Services that expose capabilities to clients through MCP. For example, anMCP server might provide an abstraction over a REST API or local data source to providebusiness data to the AI model.The following diagram illustrates this architecture:MCP client-server architectureMCP client and server can exchange a set of standard messages:MessageDescriptionInitializeRequestThis request is sent by the client to the server when it first connects, asking it tobegin initialization.ListToolsRequestSent by the client to request a list of tools the server has.CallToolRequestUsed by the client to invoke a tool provided by the server.ListResourcesRequestSent by the client to request a list of available server resources.ReadResourceRequestSent by the client to the server to read a specific resource URI.ListPromptsRequestSent by the client to request a list of available prompts and prompt templatesfrom the server.GetPromptRequestUsed by the client to get a prompt provided by the server.PingRequestA ping, issued by either the server or the client, to check that the other party isstill alive.CreateMessageRequestA request by the server to sample an LLM via the client. The client has fulldiscretion over which model to select. The client should also inform the userbefore beginning sampling, to allow them to inspect the request (human in theloop) and decide whether to approve it.SetLevelRequestA request by the client to the server, to enable or adjust logging.ﾉExpand tableDevelop with the MCP C# SDKAs a .NET developer, you can use MCP by creating MCP clients and servers to enhance yourapps with custom integrations. MCP reduces the complexity involved in connecting an AImodel to various tools, services, and data sources.The official MCP C# SDK is available through NuGet and enables you to build MCP clientsand servers for .NET apps and libraries. The SDK is maintained through collaboration betweenMicrosoft, Anthropic, and the MCP open protocol organization.To get started, add the MCP C# SDK to your project:.NET CLIInstead of building unique connectors for each integration point, you can often leverage orreference prebuilt integrations from various providers such as GitHub and Docker:Available MPC clientsAvailable MCP serversThe MCP C# SDK depends on the Microsoft.Extensions.AI libraries to handle various AIinteractions and tasks. These extension libraries provides core types and abstractions forworking with AI services, so developers can focus on coding against conceptual AI capabilitiesrather than specific platforms or provider implementations.View the MCP C# SDK dependencies on the NuGet package page.Various tools, services, and learning resources are available in the .NET and Azure ecosystemsto help you build MCP clients and servers or integrate with existing MCP servers.Get started with the following development tools:Semantic Kernel allows you to add plugins for MCP servers. Semantic Kernel supportsboth local MCP servers through standard I/O and remote servers that connect throughSSE over HTTPS.Azure Functions remote MCP servers combine MCP standards with the flexiblearchitecture of Azure Functions. Visit the Remote MCP functions sample repository forcode examples.dotnet add package ModelContextProtocol --prereleaseIntegration with Microsoft.Extensions.AIMore .NET MCP development resourcesAzure MCP Server implements the MCP specification to seamlessly connect AI agentswith key Azure services like Azure Storage, Cosmos DB, and more.Learn more about .NET and MCP using these resources:Microsoft partners with Anthropic to create official C# SDK for Model Context ProtocolBuild a Model Context Protocol (MCP) server in C#MCP C# SDK READMEOverview of the .NET + AI ecosystemMicrosoft.Extensions.AISemantic Kernel overview for .NETRelated contentBuild an AI chat app with .NETArticle•05/17/2025In this quickstart, you learn how to create a conversational .NET console chat app using anOpenAI or Azure OpenAI model. The app uses the Microsoft.Extensions.AI library so you canwrite code using AI abstractions rather than a specific SDK. AI abstractions enable you tochange the underlying AI model with minimal code changes..NET 8.0 SDK or higher - Install the .NET 8.0 SDK.An API key from OpenAI so you can run this sample.Complete the following steps to create a .NET console app to connect to an AI model.1. In an empty directory on your computer, use the dotnet new command to create a newconsole app:.NET CLI2. Change directory into the app folder:.NET CLI3. Install the required packages:BashPrerequisites７ NoteYou can also use Semantic Kernel to accomplish the tasks in this article. Semantic Kernel isa lightweight, open-source SDK that lets you build AI agents and integrate the latest AImodels into your .NET apps.Create the appdotnet new console -o ChatAppAIcd ChatAppAI4. Open the app in Visual Studio Code (or your editor of choice).Bash1. Navigate to the root of your .NET project from a terminal or command prompt.2. Run the following commands to configure your OpenAI API key as a secret for the sampleapp:BashThis app uses the Microsoft.Extensions.AI package to send and receive requests to the AImodel. The app provides users with information about hiking trails.1. In the Program.cs file, add the following code to connect and authenticate to the AImodel.C#dotnet add package OpenAIdotnet add package Microsoft.Extensions.AI.OpenAI --prereleasedotnet add package Microsoft.Extensions.Configurationdotnet add package Microsoft.Extensions.Configuration.UserSecretscode .Configure the appdotnet user-secrets initdotnet user-secrets set OpenAIKey <your-OpenAI-key>dotnet user-secrets set ModelName <your-OpenAI-model-name>Add the app codevar config = new ConfigurationBuilder().AddUserSecrets<Program>().Build();string model = config["ModelName"];string key = config["OpenAIKey"];// Create the IChatClientIChatClient chatClient =    new OpenAIClient(key).GetChatClient(model).AsIChatClient();2. Create a system prompt to provide the AI model with initial role context and instructionsabout hiking recommendations:C#3. Create a conversational loop that accepts an input prompt from the user, sends theprompt to the model, and prints the response completion:C#// Start the conversation with context for the AI modelList<ChatMessage> chatHistory =    [        new ChatMessage(ChatRole.System, """            You are a friendly hiking enthusiast who helps people discover fun hikes in their area.            You introduce yourself when first saying hello.            When helping people out, you always ask them for this information            to inform the hiking recommendation you provide:            1. The location where they would like to hike            2. What hiking intensity they are looking for            You will then provide three suggestions for nearby hikes that vary in length            after you get that information. You will also share an interesting fact about            the local nature on the hikes when making a recommendation. At the end of your            response, ask if there is anything else you can help with.        """)    ];// Loop to get user input and stream AI responsewhile (true){    // Get user prompt and add to chat history    Console.WriteLine("Your prompt:");    string? userPrompt = Console.ReadLine();    chatHistory.Add(new ChatMessage(ChatRole.User, userPrompt));    // Stream the AI response and add to chat history    Console.WriteLine("AI Response:");    string response = "";    await foreach (ChatResponseUpdate item in        chatClient.GetStreamingResponseAsync(chatHistory))    {        Console.Write(item.Text);        response += item.Text;    }    chatHistory.Add(new ChatMessage(ChatRole.Assistant, response));4. Use the dotnet run command to run the app:.NET CLIThe app prints out the completion response from the AI model. Send additional follow upprompts and ask other questions to experiment with the AI chat functionality.Quickstart - Chat with a local AI modelGenerate images using AI with .NET    Console.WriteLine();}dotnet runNext stepsRequest a response with structured outputArticle•05/17/2025In this quickstart, you create a chat app that requests a response with structured output. Astructured output response is a chat response that's of a type you specify instead of just plaintext. The chat app you create in this quickstart analyzes sentiment of various product reviews,categorizing each review according to the values of a custom enumeration..NET 8 or a later versionVisual Studio Code (optional)To provision an Azure OpenAI service and model using the Azure portal, complete the steps inthe Create and deploy an Azure OpenAI Service resource article. In the "Deploy a model" step,select the gpt-4o model.Complete the following steps to create a console app that connects to the gpt-4o AI model.1. In a terminal window, navigate to the directory where you want to create your app, andcreate a new console app with the dotnet new command:.NET CLI2. Navigate to the SOChat directory, and add the necessary packages to your app:.NET CLIPrerequisitesConfigure the AI serviceCreate the chat appdotnet new console -o SOChatdotnet add package Azure.AI.OpenAIdotnet add package Azure.Identitydotnet add package Microsoft.Extensions.AIdotnet add package Microsoft.Extensions.AI.OpenAI --prereleasedotnet add package Microsoft.Extensions.Configurationdotnet add package Microsoft.Extensions.Configuration.UserSecrets3. Run the following commands to add app secrets for your Azure OpenAI endpoint, modelname, and tenant ID:Bash4. Open the new app in your editor of choice.1. Define the enumeration that describes the different sentiments.C#2. Create the IChatClient that will communicate with the model.C#dotnet user-secrets initdotnet user-secrets set AZURE_OPENAI_ENDPOINT <your-Azure-OpenAI-endpoint>dotnet user-secrets set AZURE_OPENAI_GPT_NAME gpt-4odotnet user-secrets set AZURE_TENANT_ID <your-tenant-ID>７ NoteDepending on your environment, the tenant ID might not be needed. In that case,remove it from the code that instantiates the DefaultAzureCredential.Add the codepublic enum Sentiment{    Positive,    Negative,    Neutral}IConfigurationRoot config = new ConfigurationBuilder()    .AddUserSecrets<Program>()    .Build();string endpoint = config["AZURE_OPENAI_ENDPOINT"];string model = config["AZURE_OPENAI_GPT_NAME"];string tenantId = config["AZURE_TENANT_ID"];// Get a chat client for the Azure OpenAI endpoint.AzureOpenAIClient azureClient =    new(        new Uri(endpoint),3. Send a request to the model with a single product review, and then print the analyzedsentiment to the console. You declare the requested structured output type by passing itas the type argument to theChatClientStructuredOutputExtensions.GetResponseAsync<T>(IChatClient, String,ChatOptions, Nullable<Boolean>, CancellationToken) extension method.C#This code produces output similar to:Output4. Instead of just analyzing a single review, you can analyze a collection of reviews.C#        new DefaultAzureCredential(new DefaultAzureCredentialOptions() { TenantId = tenantId }));IChatClient chatClient = azureClient    .GetChatClient(deploymentName: model)    .AsIChatClient();７ NoteDefaultAzureCredential searches for authentication credentials from yourenvironment or local tooling. You'll need to assign the Azure AI Developer role tothe account you used to sign in to Visual Studio or the Azure CLI. For moreinformation, see Authenticate to Azure AI services with .NET.string review = "I'm happy with the product!";var response = await chatClient.GetResponseAsync<Sentiment>($"What's the sentiment of this review? {review}");Console.WriteLine($"Sentiment: {response.Result}");Sentiment: Positivestring[] inputs = [    "Best purchase ever!",    "Returned it immediately.",    "Hello",    "It works as advertised.",    "The packaging was damaged but otherwise okay."];foreach (var i in inputs)This code produces output similar to:Output5. And instead of requesting just the analyzed enumeration value, you can request the textresponse along with the analyzed value.Define a record type to contain the text response and analyzed sentiment:C#Send the request using the record type as the type argument to GetResponseAsync<T>:C#This code produces output similar to:Output{    var response2 = await chatClient.GetResponseAsync<Sentiment>($"What's the sentiment of this review? {i}");    Console.WriteLine($"Review: {i} | Sentiment: {response2.Result}");}Review: Best purchase ever! | Sentiment: PositiveReview: Returned it immediately. | Sentiment: NegativeReview: Hello | Sentiment: NeutralReview: It works as advertised. | Sentiment: NeutralReview: The packaging was damaged but otherwise okay. | Sentiment: Neutralrecord SentimentRecord(string ResponseText, Sentiment ReviewSentiment);var review3 = "This product worked okay.";var response3 = await chatClient.GetResponseAsync<SentimentRecord>($"What's the sentiment of this review? {review3}");Console.WriteLine($"Response text: {response3.Result.ResponseText}");Console.WriteLine($"Sentiment: {response3.Result.ReviewSentiment}");Response text: Certainly, I have analyzed the sentiment of the review you provided.Sentiment: NeutralClean up resourcesIf you no longer need them, delete the Azure OpenAI resource and GPT-4 model deployment.1. In the Azure Portal, navigate to the Azure OpenAI resource.2. Select the Azure OpenAI resource, and then select Delete.Structured outputs (Azure OpenAI Service)Using JSON schema for structured output in .NET for OpenAI modelsIntroducing Structured Outputs in the API (OpenAI)See alsoBuild a .NET AI vector search app06/06/2025In this quickstart, you create a .NET console app to perform semantic search on a vector storeto find relevant results for the user's query. You learn how to generate embeddings for userprompts and use those embeddings to query the vector data store.Vector stores, or vector databases, are essential for tasks like semantic search, retrievalaugmented generation (RAG), and other scenarios that require grounding generative AIresponses. While relational databases and document databases are optimized for structuredand semi-structured data, vector databases are built to efficiently store, index, and managedata represented as embedding vectors. As a result, the indexing and search algorithms usedby vector databases are optimized to efficiently retrieve data that can be used downstream inyour applications.The app uses the Microsoft.Extensions.AI and Microsoft.Extensions.VectorData libraries so youcan write code using AI abstractions rather than a specific SDK. AI abstractions help createloosely coupled code that allows you to change the underlying AI model with minimal appchanges.📦 Microsoft.Extensions.VectorData.Abstractions is a .NET library developed in collaborationwith Semantic Kernel and the broader .NET ecosystem to provide a unified layer of abstractionsfor interacting with vector stores. The abstractions inMicrosoft.Extensions.VectorData.Abstractions provide library authors and developers withthe following functionality:Perform create-read-update-delete (CRUD) operations on vector stores.Use vector and text search on vector stores..NET 8.0 SDK or higher - Install the .NET 8.0 SDK.An API key from OpenAI so you can run this sample.About the libraries７ NoteThe Microsoft.Extensions.VectorData.Abstractions library is currently in preview.PrerequisitesComplete the following steps to create a .NET console app that can:Create and populate a vector store by generating embeddings for a data set.Generate an embedding for the user prompt.Query the vector store using the user prompt embedding.Display the relevant results from the vector search.1. In an empty directory on your computer, use the dotnet new command to create a newconsole app:.NET CLI2. Change directory into the app folder:.NET CLI3. Install the required packages:BashThe following list describes each package in the VectorDataAI app:Microsoft.Extensions.AI.OpenAI provides AI abstractions for OpenAI-compatiblemodels or endpoints. This library also includes the official OpenAI library for theOpenAI service API as a dependency.Microsoft.Extensions.VectorData.Abstractions enables Create-Read-Update-Delete(CRUD) and search operations on vector stores.Microsoft.SemanticKernel.Connectors.InMemory provides an in-memory vectorstore class to hold queryable vector data records.Microsoft.Extensions.Configuration provides an implementation of key-value pair—based configuration.Create the appdotnet new console -o VectorDataAIcd VectorDataAIdotnet add package Microsoft.Extensions.AI.OpenAI --prereleasedotnet add package Microsoft.Extensions.VectorData.Abstractions --prereleasedotnet add package Microsoft.SemanticKernel.Connectors.InMemory --prereleasedotnet add package Microsoft.Extensions.Configurationdotnet add package Microsoft.Extensions.Configuration.UserSecretsdotnet add package System.Linq.AsyncEnumerableMicrosoft.Extensions.Configuration.UserSecrets is a user secrets configurationprovider implementation for Microsoft.Extensions.Configuration.4. Open the app in Visual Studio Code (or your editor of choice).Bash1. Navigate to the root of your .NET project from a terminal or command prompt.2. Run the following commands to configure your OpenAI API key as a secret for the sampleapp:Bash1. Add a new class named CloudService to your project with the following properties:C#code .Configure the appdotnet user-secrets initdotnet user-secrets set OpenAIKey <your-OpenAI-key>dotnet user-secrets set ModelName <your-OpenAI-model-name>７ NoteFor the model name, you need to specify a text embedding model such as text-embedding-3-small or text-embedding-3-large to generate embeddings for vector searchin the sections that follow. For more information about embedding models, seeEmbeddings.Add the app codeusing Microsoft.Extensions.VectorData;namespace VectorDataAI;internal class CloudService{    [VectorStoreKey]    public int Key { get; set; }The Microsoft.Extensions.VectorData attributes, such as VectorStoreKeyAttribute, influencehow each property is handled when used in a vector store. The Vector property stores agenerated embedding that represents the semantic meaning of the Description value forvector searches.2. In the Program.cs file, add the following code to create a data set that describes acollection of cloud services:C#    [VectorStoreData]    public string Name { get; set; }    [VectorStoreData]    public string Description { get; set; }    [VectorStoreVector(        Dimensions: 384,        DistanceFunction = DistanceFunction.CosineSimilarity)]    public ReadOnlyMemory<float> Vector { get; set; }}List<CloudService> cloudServices =[    new() {            Key = 0,            Name = "Azure App Service",            Description = "Host .NET, Java, Node.js, and Python web applications and APIs in a fully managed Azure service. You only need to deploy your code to Azure. Azure takes care of all the infrastructure management like high availability, load balancing, and autoscaling."    },    new() {            Key = 1,            Name = "Azure Service Bus",            Description = "A fully managed enterprise message broker supporting both point to point and publish-subscribe integrations. It's ideal for building decoupled applications, queue-based load leveling, or facilitating communication between microservices."    },    new() {            Key = 2,            Name = "Azure Blob Storage",            Description = "Azure Blob Storage allows your applications to store and retrieve files in the cloud. Azure Storage is highly scalable to store massive amounts of data and data is stored redundantly to ensure high availability."    },    new() {            Key = 3,3. Create and configure an IEmbeddingGenerator implementation to send requests to anembedding AI model:C#4. Create and populate a vector store with the cloud service data. Use theIEmbeddingGenerator implementation to create and assign an embedding vector for eachrecord in the cloud service data:C#            Name = "Microsoft Entra ID",            Description = "Manage user identities and control access to your apps, data, and resources."    },    new() {            Key = 4,            Name = "Azure Key Vault",            Description = "Store and access application secrets like connection strings and API keys in an encrypted vault with restricted access to make sure your secrets and your application aren't compromised."    },    new() {            Key = 5,            Name = "Azure AI Search",            Description = "Information retrieval at scale for traditional and conversational search applications, with security and options for AI enrichment and vectorization."    }];// Load the configuration values.IConfigurationRoot config = new ConfigurationBuilder().AddUserSecrets<Program>().Build();string model = config["ModelName"];string key = config["OpenAIKey"];// Create the embedding generator.IEmbeddingGenerator<string, Embedding<float>> generator =    new OpenAIClient(new ApiKeyCredential(key))      .GetEmbeddingClient(model: model)      .AsIEmbeddingGenerator();// Create and populate the vector store.var vectorStore = new InMemoryVectorStore();VectorStoreCollection<int, CloudService> cloudServicesStore =    vectorStore.GetCollection<int, CloudService>("cloudServices");await cloudServicesStore.EnsureCollectionExistsAsync();foreach (CloudService service in cloudServices)The embeddings are numerical representations of the semantic meaning for each datarecord, which makes them compatible with vector search features.5. Create an embedding for a search query and use it to perform a vector search on thevector store:C#6. Use the dotnet run command to run the app:.NET CLIThe app prints out the top result of the vector search, which is the cloud service that'smost relevant to the original query. You can modify the query to try different searchscenarios.Quickstart - Chat with a local AI modelGenerate images using AI with .NET{    service.Vector = await generator.GenerateVectorAsync(service.Description);    await cloudServicesStore.UpsertAsync(service);}// Convert a search query to a vector// and search the vector store.string query = "Which Azure service should I use to store my Word documents?";ReadOnlyMemory<float> queryEmbedding = await generator.GenerateVectorAsync(query);IAsyncEnumerable<VectorSearchResult<CloudService>> results =    cloudServicesStore.SearchAsync(queryEmbedding, top: 1);await foreach (VectorSearchResult<CloudService> result in results){    Console.WriteLine($"Name: {result.Record.Name}");    Console.WriteLine($"Description: {result.Record.Description}");    Console.WriteLine($"Vector match score: {result.Score}");}dotnet runNext stepsInvoke .NET functions using an AI modelArticle•05/18/2025In this quickstart, you create a .NET console AI chat app to connect to an AI model with localfunction calling enabled. The app uses the Microsoft.Extensions.AI library so you can write codeusing AI abstractions rather than a specific SDK. AI abstractions enable you to change theunderlying AI model with minimal code changes..NET 8.0 SDK or higher - Install the .NET 8.0 SDK.An API key from OpenAI so you can run this sample.Complete the following steps to create a .NET console app to connect to an AI model.1. In an empty directory on your computer, use the dotnet new command to create a newconsole app:.NET CLI2. Change directory into the app folder:.NET CLI3. Install the required packages:BashPrerequisites７ NoteYou can also use Semantic Kernel to accomplish the tasks in this article. Semantic Kernel isa lightweight, open-source SDK that lets you build AI agents and integrate the latest AImodels into your .NET apps.Create the appdotnet new console -o FunctionCallingAIcd FunctionCallingAI4. Open the app in Visual Studio Code or your editor of choiceBash1. Navigate to the root of your .NET project from a terminal or command prompt.2. Run the following commands to configure your OpenAI API key as a secret for the sampleapp:BashThe app uses the Microsoft.Extensions.AI package to send and receive requests to the AImodel.1. In the Program.cs file, add the following code to connect and authenticate to the AImodel. The ChatClient is also configured to use function invocation, which allows the AImodel to call .NET functions in your code.C#dotnet add package Microsoft.Extensions.AIdotnet add package Microsoft.Extensions.AI.OpenAI --prereleasedotnet add package Microsoft.Extensions.Configurationdotnet add package Microsoft.Extensions.Configuration.UserSecretscode .Configure the appdotnet user-secrets initdotnet user-secrets set OpenAIKey <your-OpenAI-key>dotnet user-secrets set ModelName <your-OpenAI-model-name>Add the app codeusing Microsoft.Extensions.AI;using Microsoft.Extensions.Configuration;using OpenAI;IConfigurationRoot config = new ConfigurationBuilder().AddUserSecrets<Program>().Build();string? model = config["ModelName"];string? key = config["OpenAIKey"];2. Create a new ChatOptions object that contains an inline function the AI model can call toget the current weather. The function declaration includes a delegate to run logic, andname and description parameters to describe the purpose of the function to the AImodel.C#3. Add a system prompt to the chatHistory to provide context and instructions to themodel. Send a user prompt with a question that requires the AI model to call theregistered function to properly answer the question.C#IChatClient client =    new ChatClientBuilder(new OpenAIClient(key).GetChatClient(model ?? "gpt-4o").AsIChatClient())    .UseFunctionInvocation()    .Build();// Add a new plugin with a local .NET function// that should be available to the AI model.var chatOptions = new ChatOptions{    Tools = [AIFunctionFactory.Create((string location, string unit) =>    {        // Here you would call a weather API        // to get the weather for the location.        return "Periods of rain or drizzle, 15 C";    },    "get_current_weather",    "Get the current weather in a given location")]};// System prompt to provide context.List<ChatMessage> chatHistory = [new(ChatRole.System, """    You are a hiking enthusiast who helps people discover fun hikes in their area. You are upbeat and friendly.    """)];// Weather conversation relevant to the registered function.chatHistory.Add(new ChatMessage(ChatRole.User,    "I live in Montreal and I'm looking for a moderate intensity hike. What's the current weather like?"));Console.WriteLine($"{chatHistory.Last().Role} >>> {chatHistory.Last()}");ChatResponse response = await client.GetResponseAsync(chatHistory, chatOptions);Console.WriteLine($"Assistant >>> {response.Text}");4. Use the dotnet run command to run the app:.NET CLIThe app prints the completion response from the AI model, which includes data providedby the .NET function. The AI model understood that the registered function was availableand called it automatically to generate a proper response.Quickstart - Build an AI chat app with .NETGenerate text and conversations with .NET and Azure OpenAI Completionsdotnet runNext stepsGenerate images using AI with .NETArticle•05/18/2025In this quickstart, you learn how to create a .NET console app to generate images using anOpenAI or Azure OpenAI DALLe AI model, which are specifically designed to generate imagesbased on text prompts..NET 8.0 SDK or higher - Install the .NET 8.0 SDK.An API key from OpenAI so you can run this sample.Complete the following steps to create a .NET console app to connect to an AI model.1. In an empty directory on your computer, use the dotnet new command to create a newconsole app:.NET CLI2. Change directory into the app folder:.NET CLI3. Install the required packages:BashPrerequisites７ NoteYou can also use Semantic Kernel to accomplish the tasks in this article. Semantic Kernel isa lightweight, open-source SDK that lets you build AI agents and integrate the latest AImodels into your .NET apps.Create the appdotnet new console -o ImagesAIcd ImagesAIdotnet add package OpenAIdotnet add package Microsoft.Extensions.Configuration4. Open the app in Visual Studio Code or your editor of choice.Bash1. Navigate to the root of your .NET project from a terminal or command prompt.2. Run the following commands to configure your OpenAI API key as a secret for the sampleapp:Bash1. In the Program.cs file, add the following code to connect and authenticate to the AImodel.C#dotnet add package Microsoft.Extensions.Configuration.UserSecretscode .Configure the appdotnet user-secrets initdotnet user-secrets set OpenAIKey <your-OpenAI-key>dotnet user-secrets set ModelName <your-OpenAI-model-name>Add the app code// Licensed to the .NET Foundation under one or more agreements.// The .NET Foundation licenses this file to you under the MIT license.// See the LICENSE file in the project root for more information.using Microsoft.Extensions.Configuration;using OpenAI.Images;// Retrieve the local secrets that were set from the command line, using:// dotnet user-secrets init// dotnet user-secrets set OpenAIKey <your-openai-key>var config = new ConfigurationBuilder().AddUserSecrets<Program>().Build();string key = config["OpenAIKey"];string modelName = config["ModelName"];// Create the OpenAI ImageClientImageClient client = new(modelName, key);// Generate the imageGeneratedImage generatedImage = await client.GenerateImageAsync("""    A postal card with a happy hiker waving and a beautiful mountain in the The preceding code:Reads essential configuration values from the project user secrets to connect to theAI model.Creates an OpenAI.Images.ImageClient to connect to the AI model.Sends a prompt to the model that describes the desired image.Prints the URL of the generated image to the console output.2. Run the app:.NET CLINavigate to the image URL in the console output to view the generated image. Customizethe text content of the prompt to create new images or modify the original.Quickstart - Build an AI chat app with .NETGenerate text and conversations with .NET and Azure OpenAI Completionsbackground.    There is a trail visible in the foreground.    The postal card has text in red saying: 'You are invited for a hike!'    """,    new ImageGenerationOptions     {        Size = GeneratedImageSize.W1024xH1024     });Console.WriteLine($"The generated image is ready at:\n{generatedImage.ImageUri}");dotnet runNext stepsChat with a local AI model using .NET05/29/2025In this quickstart, you learn how to create a conversational .NET console chat app using anOpenAI or Azure OpenAI model. The app uses the Microsoft.Extensions.AI library so you canwrite code using AI abstractions rather than a specific SDK. AI abstractions enable you tochange the underlying AI model with minimal code changes.Install .NET 8.0 or higherInstall Ollama locally on your deviceVisual Studio Code (optional)Complete the following steps to configure and run a local AI model on your device. Manydifferent AI models are available to run locally and are trained for different tasks, such asgenerating code, analyzing images, generative chat, or creating embeddings. For thisquickstart, you'll use the general purpose phi3:mini model, which is a small but capablegenerative AI created by Microsoft.1. Open a terminal window and verify that Ollama is available on your device:BashIf Ollama is available, it displays a list of available commands.2. Start Ollama:BashIf Ollama is running, it displays a list of available commands.3. Pull the phi3:mini model from the Ollama registry and wait for it to download:BashPrerequisitesRun the local AI modelollamaollama serve4. After the download completes, run the model:BashOllama starts the phi3:mini model and provides a prompt for you to interact with it.Complete the following steps to create a .NET console app that connects to your localphi3:mini AI model.1. In a terminal window, navigate to an empty directory on your device and create a newapp with the dotnet new command:.NET CLI2. Add the OllamaSharp package to your app:.NET CLI3. Open the new app in your editor of choice, such as Visual Studio Code..NET CLIThe Semantic Kernel SDK provides many services and features to connect to AI models andmanage interactions. In the steps ahead, you'll create a simple app that connects to the local AIand stores conversation history to improve the chat experience.1. Open the Program.cs file and replace the contents of the file with the following code:ollama pull phi3:miniollama run phi3:miniCreate the .NET appdotnet new console -o LocalAIdotnet add package OllamaSharpcode .Connect to and chat with the AI modelC#The preceding code accomplishes the following:Creates an OllamaChatClient that implements the IChatClient interface.This interface provides a loosely coupled abstraction you can use to chat with AIModels.You can later change the underlying chat client implementation to anothermodel, such as Azure OpenAI, without changing any other code.Creates a ChatHistory object to store the messages between the user and the AImodel.Retrieves a prompt from the user and stores it in the ChatHistory.Sends the chat data to the AI model to generate a response.using Microsoft.Extensions.AI;using OllamaSharp;IChatClient chatClient =    new OllamaApiClient(new Uri("http://localhost:11434/"), "phi3:mini");// Start the conversation with context for the AI modelList<ChatMessage> chatHistory = new();while (true){    // Get user prompt and add to chat history    Console.WriteLine("Your prompt:");    var userPrompt = Console.ReadLine();    chatHistory.Add(new ChatMessage(ChatRole.User, userPrompt));    // Stream the AI response and add to chat history    Console.WriteLine("AI Response:");    var response = "";    await foreach (ChatResponseUpdate item in        chatClient.GetStreamingResponseAsync(chatHistory))    {        Console.Write(item.Text);        response += item.Text;    }    chatHistory.Add(new ChatMessage(ChatRole.Assistant, response));    Console.WriteLine();}７ NoteOllama runs on port 11434 by default, which is why the AI model endpoint is set tohttp://localhost:11434.2. Run the app and enter a prompt into the console to receive a response from the AI, suchas the following:Output3. The response from the AI is accurate, but also verbose. The stored chat history enablesthe AI to modify its response. Instruct the AI to shorten the list it provided:OutputYour prompt:Tell me three facts about .NET.AI response:1. **Cross-Platform Development:** One of the significant strengths of .NET,particularly its newer iterations (.NET Core and .NET 5+), is cross-platform support.It allows developers to build applications that run on Windows, Linux, macOS,and various other operating systems seamlessly, enhancing flexibility andreducing barriers for a wider range of users.2. **Rich Ecosystem and Library Support:** .NET has a rich ecosystem,comprising an extensive collection of libraries (such as those provided by theofficial NuGet Package Manager), tools, and services. This allows developersto work on web applications (.NET for desktop apps and ASP.NET Corefor modern web applications), mobile applications (.NET MAUI),IoT solutions, AI/ML projects, and much more with a vast array of prebuiltcomponents available at their disposal.3. **Type Safety:** .NET operates under the Common Language Infrastructure (CLI)model and employs managed code for executing applications. This approach inherentlyoffers strong type safety checks which help in preventing many runtime errors thatare common in languages like C/C++. It also enables features such as garbage collection,thus relieving developers from manual memory management. These characteristics enhancethe reliability of .NET-developed software and improve productivity by catchingissues early during development.Your prompt:Shorten the length of each item in the previous response.AI Response: **Cross-platform Capabilities:** .NET allows building for various operating systemsthrough platforms like .NET Core, promoting accessibility (Windows, Linux, macOS).The updated response from the AI is much shorter the second time. Due to the availablechat history, the AI was able to assess the previous result and provide shorter summaries.Generate text and conversations with .NET and Azure OpenAI Completions**Extensive Ecosystem:** Offers a vast library selection via NuGet and tools for web(.NET Framework), mobile development (.NET MAUI), IoT, AI, providing richcapabilities to developers.**Type Safety & Reliability:** .NET's CLI model enforces strong typing and automaticgarbage collection, mitigating runtime errors, thus enhancing application stability.Next stepsCreate a minimal AI assistant using .NETArticle•02/28/2025In this quickstart, you'll learn how to create a minimal AI assistant using the OpenAI orAzure OpenAI SDK libraries. AI assistants provide agentic functionality to help userscomplete tasks using AI tools and models. In the sections ahead, you'll learn thefollowing:Core components and concepts of AI assistantsHow to create an assistant using the Azure OpenAI SDKHow to enhance and customize the capabilities of an assistantInstall .NET 8.0 or higherVisual Studio Code (optional)Visual Studio (optional)An access key for an OpenAI modelAI assistants are based around conversational threads with a user. The user sendsprompts to the assistant on a conversation thread, which direct the assistant tocomplete tasks using the tools it has available. Assistants can process and analyze data,make decisions, and interact with users or other systems to achieve specific goals. Mostassistants include the following components:ComponentDescriptionAssistantThe core AI client and logic that uses Azure OpenAI models, manages conversationthreads, and utilizes configured tools.ThreadA conversation session between an assistant and a user. Threads store messagesand automatically handle truncation to fit content into a model's context.MessageA message created by an assistant or a user. Messages can include text, images,and other files. Messages are stored as a list on the thread.RunActivation of an assistant to begin running based on the contents of the thread.The assistant uses its configuration and the thread's messages to perform tasks byPrerequisitesCore components of AI assistantsﾉExpand tableComponentDescriptioncalling models and tools. As part of a run, the assistant appends messages to thethread.Run stepsA detailed list of steps the assistant took as part of a run. An assistant can call toolsor create messages during its run. Examining run steps allows you to understandhow the assistant is getting to its final results.Assistants can also be configured to use multiple tools in parallel to complete tasks,including the following:Code interpreter tool: Writes and runs code in a sandboxed executionenvironment.Function calling: Runs local custom functions you define in your code.File search capabilities: Augments the assistant with knowledge from outside itsmodel.By understanding these core components and how they interact, you can build andcustomize powerful AI assistants to meet your specific needs.Complete the following steps to create a .NET console app and add the package neededto work with assistants:1. In a terminal window, navigate to an empty directory on your device and create anew app with the dotnet new command:.NET CLI2. Add the OpenAI package to your app:.NET CLI3. Open the new app in your editor of choice, such as Visual Studio Code..NET CLICreate the .NET appdotnet new console -o AIAssistantdotnet package add OpenAI --prereleasecode .1. Open the Program.cs file and replace the contents of the file with the followingcode to create the required clients:C#2. Create an in-memory sample document and upload it to the OpenAIFileClient:C#Create the AI assistant clientusing OpenAI;using OpenAI.Assistants;using OpenAI.Files;using Azure.AI.OpenAI;using Azure.Identity;// Create the OpenAI clientOpenAIClient openAIClient = new("your-apy-key");// For Azure OpenAI, use the following client instead:AzureOpenAIClient azureAIClient = new(        new Uri("your-azure-openai-endpoint"),        new DefaultAzureCredential());#pragma warning disable OPENAI001AssistantClient assistantClient = openAIClient.GetAssistantClient();OpenAIFileClient fileClient = openAIClient.GetOpenAIFileClient();// Create an in-memory document to upload to the file clientusing Stream document = BinaryData.FromBytes("""    {        "description": "This document contains the sale history data for Contoso products.",        "sales": [            {                "month": "January",                "by_product": {                    "113043": 15,                    "113045": 12,                    "113049": 2                }            },            {                "month": "February",                "by_product": {                    "113045": 22                }            },            {                "month": "March",3. Enable file search and code interpreter tooling capabilities via theAssistantCreationOptions:C#4. Create the Assistant and a thread to manage interactions between the user andthe assistant:                "by_product": {                    "113045": 16,                    "113055": 5                }            }        ]    }    """u8.ToArray()).ToStream();// Upload the document to the file clientOpenAIFile salesFile = fileClient.UploadFile(    document,    "monthly_sales.json",    FileUploadPurpose.Assistants);// Configure the assistant optionsAssistantCreationOptions assistantOptions = new(){    Name = "Example: Contoso sales RAG",    Instructions =        "You are an assistant that looks up sales data and helps visualize the information based"        + " on user queries. When asked to generate a graph, chart, or other visualization, use"        + " the code interpreter tool to do so.",    Tools =    {        new FileSearchToolDefinition(), // Enable the assistant to search and access files        new CodeInterpreterToolDefinition(), // Enable the assistant to run code for data analysis    },    ToolResources = new()    {        FileSearch = new()        {            NewVectorStores =            {                new VectorStoreCreationHelper([salesFile.Id]),            }        }    },};C#5. Print the messages and save the generated image from the conversation with theassistant:C#// Create the assistantAssistant assistant = assistantClient.CreateAssistant("gpt-4o", assistantOptions);// Configure and create the conversation threadThreadCreationOptions threadOptions = new(){    InitialMessages = { "How well did product 113045 sell in February? Graph its trend over time." }};ThreadRun threadRun = assistantClient.CreateThreadAndRun(assistant.Id, threadOptions);// Sent the prompt and monitor progress until the thread run is completedo{    Thread.Sleep(TimeSpan.FromSeconds(1));    threadRun = assistantClient.GetRun(threadRun.ThreadId, threadRun.Id);}while (!threadRun.Status.IsTerminal);// Get the messages from the thread runvar messages = assistantClient.GetMessagesAsync(    threadRun.ThreadId,    new MessageCollectionOptions()    {         Order = MessageCollectionOrder.Ascending    });await foreach (ThreadMessage message in messages){    // Print out the messages from the assistant    Console.Write($"[{message.Role.ToString().ToUpper()}]: ");    foreach (MessageContent contentItem in message.Content)    {        if (!string.IsNullOrEmpty(contentItem.Text))        {            Console.WriteLine($"{contentItem.Text}");            if (contentItem.TextAnnotations.Count > 0)            {                Console.WriteLine();Locate and open the saved image in the app bin directory, which should resemblethe following:            }            // Include annotations, if any            foreach (TextAnnotation annotation in contentItem.TextAnnotations)            {                if (!string.IsNullOrEmpty(annotation.InputFileId))                {                    Console.WriteLine($"* File citation, file ID: {annotation.InputFileId}");                }                if (!string.IsNullOrEmpty(annotation.OutputFileId))                {                    Console.WriteLine($"* File output, new file ID: {annotation.OutputFileId}");                }            }        }        // Save the generated image file        if (!string.IsNullOrEmpty(contentItem.ImageFileId))        {            OpenAIFile imageInfo = fileClient.GetFile(contentItem.ImageFileId);            BinaryData imageBytes = fileClient.DownloadFile(contentItem.ImageFileId);            using FileStream stream = File.OpenWrite($"{imageInfo.Filename}.png");            imageBytes.ToStream().CopyTo(stream);            Console.WriteLine($"<image: {imageInfo.Filename}.png>");        }    }    Console.WriteLine();}Generate text and conversations with .NET and Azure OpenAI CompletionsNext stepsCreate a .NET AI app to chat with customdata using the AI app template extensions08/01/2025In this quickstart, you learn how to create a .NET AI app to chat with custom data using the.NET AI app template. The template is designed to streamline the getting started experiencefor building AI apps with .NET by handling common setup tasks and configurations for you..NET 9.0 SDKOne of the following IDEs (optional):Visual Studio 2022Visual Studio Code with C# Dev KitThe AI Chat Web App template is available as a template package through NuGet. Use thedotnet new install command to install the package:.NET CLIAfter you install the AI app templates, you can use them to create starter apps through VisualStudio UI, Visual Studio Code, or the .NET CLI.1. Inside Visual Studio, navigate to File > New > Project.2. On the Create a new project screen, search for AI Chat Web App. Select thematching result and then choose Next.3. On the Configure your new project screen, enter the desired name and location foryour project and then choose Next.4. On the Additional information screen:For the Framework option, select .NET 9.0.PrerequisitesInstall the .NET AI app templatedotnet new install Microsoft.Extensions.AI.TemplatesCreate the .NET AI appVisual StudioFor the AI service provider option, select GitHub Models.For the Vector store option, select Local on-disc (for prototyping).5. Select Create to complete the process.The sample app you created is a Blazor Interactive Server web app preconfigured with commonAI and data services. The app handles the following concerns for you:Includes essential Microsoft.Extensions.AI packages and other dependencies in thecsproj file to help you get started working with AI.Creates various AI services and registers them for dependency injection in the Program.csfile:An IChatClient service to chat back and forth with the generative AI modelAn IEmbeddingGenerator service that's used to generate embeddings, which areessential for vector search functionalityA JsonVectorStore to act as an in-memory vector storeRegisters a SQLite database context service to handle ingesting documents. The app ispreconfigured to ingest whatever documents you add to the Data folder of the project,including the provided example files.Provides a complete chat UI using Blazor components. The UI handles rich formatting forthe AI responses and provides features such as citations for response data.To authenticate to GitHub models from your code, you'll need to create a GitHub personalaccess token:1. Navigate to the Personal access tokens page of your GitHub account settings underDeveloper Settings.2. Select Generate new token.3. Enter a name for the token, and under Permissions, set Models to Access: Read-only.4. Select Generate token at the bottom of the page.5. Copy the token for use in the steps ahead.The AI Chat Web App app is almost ready to go as soon as it's created. However, you need toconfigure the app to use the personal access token you set up for GitHub Models. By default,Explore the sample appConfigure access to GitHub ModelsConfigure the appthe app template searches for this value in the project's local .NET user secrets. You canmanage user secrets using either the Visual Studio UI or the .NET CLI.1. In Visual Studio, right-click on your project in the Solution Explorer and selectManage User Secrets. This opens a secrets.json file where you can store your APIkeys without them being tracked in source control.2. Add the following key and value:JSONBy default, the app template uses the gpt-4o-mini and text-embedding-3-small models. To tryother models, update the name parameters in Program.cs:C#1. Select the run button at the top of Visual Studio to launch the app. After a moment, youshould see the following UI load in the browser:Visual Studio{    "GitHubModels:Token": "<your-personal-access-token>"}var chatClient = ghModelsClient.AsChatClient("gpt-4o-mini");var embeddingGenerator = ghModelsClient.AsEmbeddingGenerator("text-embedding-3-small");Run and test the app2. Enter a prompt into the input box such as "What are some essential tools in the survivalkit?" to ask your AI model a question about the ingested data from the example files.The app responds with an answer to the question and provides citations of where it foundthe data. You can click on one of the citations to be directed to the relevant section of theexample files.Generate text and conversations with .NET and Azure OpenAI CompletionsNext stepsCreate a minimal MCP server using C# andpublish to NuGet07/15/2025In this quickstart, you create a minimal Model Context Protocol (MCP) server using the C# SDKfor MCP, connect to it using GitHub Copilot, and publish it to NuGet. MCP servers areservices that expose capabilities to clients through the Model Context Protocol (MCP)..NET 10.0 SDK (preview 6 or higher)Visual Studio CodeGitHub Copilot extension for Visual Studio CodeNuGet.org account1. In a terminal window, install the MCP Server template (version 9.7.0-preview.2.25356.2 ornewer):Bash2. Create a new MCP server app with the dotnet new mcpserver command:Bash3. Navigate to the SampleMcpServer directory:Bash７ NoteThe Microsoft.Extensions.AI.Templates experience is currently in preview. The templateuses the ModelContextProtocol library and the MCP registry server.json schema,which are both in preview.PrerequisitesCreate the projectdotnet new install Microsoft.Extensions.AI.Templatesdotnet new mcpserver -n SampleMcpServer4. Build the project:Bash5. Update the <PackageId> in the .csproj file to be unique on NuGet.org, for example<NuGet.org username>.SampleMcpServer.Configure GitHub Copilot for Visual Studio Code to use your custom MCP server:1. If you haven't already, open your project folder in Visual Studio Code.2. Create a .vscode folder at the root of your project.3. Add an mcp.json file in the .vscode folder with the following content:JSON4. Save the file.The MCP server template includes a tool called get_random_number you can use for testing andas a starting point for development.cd SampleMcpServerdotnet buildConfigure the MCP server in Visual Studio Code{  "servers": {    "SampleMcpServer": {      "type": "stdio",      "command": "dotnet",      "args": [        "run",        "--project",        "<RELATIVE PATH TO PROJECT DIRECTORY>"      ]    }  }}Test the MCP server1. Open GitHub Copilot in Visual Studio Code and switch to chat mode.2. Select the Select tools icon to verify your SampleMcpServer is available with the sampletool listed.3. Enter a prompt to run the get_random_number tool:Console4. GitHub Copilot requests permission to run the get_random_number tool for yourprompt. Select Continue or use the arrow to select a more specific behavior:Current session always runs the operation in the current GitHub Copilot AgentMode session.Current workspace always runs the command for the current Visual Studio Codeworkspace.Always allow sets the operation to always run for any GitHub Copilot Agent Modesession or any Visual Studio Code workspace.Give me a random number between 1 and 100.5. Verify that the server responds with a random number:OutputIn this example, you enhance the MCP server to use a configuration value set in anenvironment variable. This could be configuration needed for the functioning of your MCPserver, such as an API key, an endpoint to connect to, or a local directory path.1. Add another tool method after the GetRandomNumber method inTools/RandomNumberTools.cs. Update the tool code to use an environment variable.C#2. Update the .vscode/mcp.json to set the WEATHER_CHOICES environment variable fortesting.JSONYour random number is 42.Add inputs and configuration options[McpServerTool][Description("Describes random weather in the provided city.")]public string GetCityWeather(    [Description("Name of the city to return weather for")] string city){    // Read the environment variable during tool execution.    // Alternatively, this could be read during startup and passed via IOptions dependency injection    var weather = Environment.GetEnvironmentVariable("WEATHER_CHOICES");    if (string.IsNullOrWhiteSpace(weather))    {        weather = "balmy,rainy,stormy";    }    var weatherChoices = weather.Split(",");    var selectedWeatherIndex =  Random.Shared.Next(0, weatherChoices.Length);    return $"The weather in {city} is {weatherChoices[selectedWeatherIndex]}.";}{   "servers": {     "SampleMcpServer": {       "type": "stdio",3. Try another prompt with Copilot in VS Code, such as:ConsoleVS Code should return a random weather description.4. Update the .mcp/server.json to declare your environment variable input. Theserver.json file schema is defined by the MCP Registry project and is used byNuGet.org to generate VS Code MCP configuration.Use the environment_variables property to declare environment variables used byyour app that will be set by the client using the MCP server (for example, VS Code).Use the package_arguments property to define CLI arguments that will be passed toyour app. For more examples, see the MCP Registry project.JSON       "command": "dotnet",       "args": [         "run",         "--project",         "<RELATIVE PATH TO PROJECT DIRECTORY>"       ],       "env": {          "WEATHER_CHOICES": "sunny,humid,freezing"       }     }   } }What is the weather in Redmond, Washington?{  "$schema": "https://modelcontextprotocol.io/schemas/draft/2025-07-09/server.json",  "description": "<your description here>",  "name": "io.github.<your GitHub username here>/<your repo name>",  "packages": [    {      "registry_name": "nuget",      "name": "<your package ID here>",      "version": "<your package version here>",      "package_arguments": [],      "environment_variables": [        {          "name": "WEATHER_CHOICES",          "value": "{weather_choices}",          "variables": {The only information used by NuGet.org in the server.json is the first packages arrayitem with the registry_name value matching nuget. The other top-level properties asidefrom the packages property are currently unused and are intended for the upcomingcentral MCP Registry. You can leave the placeholder values until the MCP Registry is liveand ready to accept MCP server entries.You can test your MCP server again before moving forward.1. Pack the project:Bash2. Publish the package to NuGet:BashIf you want to test the publishing flow before publishing to NuGet.org, you can registeran account on the NuGet Gallery integration environment: https://int.nugettest.org. Thepush command would be modified to:            "weather_choices": {              "description": "Comma separated list of weather descriptions to randomly select.",              "is_required": true,              "is_secret": false            }          }        }      ]    }  ],  "repository": {    "url": "https://github.com/<your GitHub username here>/<your repo name>",    "source": "github"  },  "version_detail": {    "version": "<your package version here>"  }}Pack and publish to NuGetdotnet pack -c Releasedotnet nuget push bin/Release/*.nupkg --api-key <your-api-key> --source https://api.nuget.org/v3/index.jsonBashFor more information, see Publish a package.1. Search for your MCP server package on NuGet.org (or int.nugettest.org if youpublished to the integration environment) and select it from the list.2. View the package details and copy the JSON from the "MCP Server" tab.dotnet nuget push bin/Release/*.nupkg --api-key <your-api-key> --source https://apiint.nugettest.org/v3/index.jsonDiscover MCP servers on NuGet.org3. In your mcp.json file in the .vscode folder, add the copied JSON, which looks like this:JSON{  "inputs": [    {      "type": "promptString",      "id": "weather_choices",      "description": "Comma separated list of weather descriptions to randomly select.",      "password": false    }  ],  "servers": {    "Contoso.SampleMcpServer": {      "type": "stdio",      "command": "dnx",      "args": ["Contoso.SampleMcpServer@0.0.1-beta", "--yes"],      "env": {        "WEATHER_CHOICES": "${input:weather_choices}"      }If you published to the NuGet Gallery integration environment, you need to add "--add-source", "https://apiint.nugettest.org/v3/index.json" at the end of the "args" array.4. Save the file.5. In GitHub Copilot, select the Select tools icon to verify your SampleMcpServer isavailable with the tools listed.6. Enter a prompt to run the new get_city_weather tool:Console7. If you added inputs to your MCP server (for example, WEATHER_CHOICES), you will beprompted to provide values.8. Verify that the server responds with the random weather:OutputIf VS Code shows this error when starting the MCP server, you need to install a compatibleversion of the .NET SDK.    }  }}What is the weather in Redmond?The weather in Redmond is balmy.Common issuesThe command "dnx" needed to run SampleMcpServer wasnot found.The dnx command is shipped as part of the .NET SDK, starting with version 10 preview 6. Installthe .NET 10 SDK to resolve this issue.Generally speaking, an AI agent like GitHub Copilot is informed that it has some tools availableby the client application, such as VS Code. Some tools, such as the sample random numbertool, might not be leveraged by the AI agent because it has similar functionality built in.If your tool is not being used, check the following:1. Verify that your tool appears in the list of tools that VS Code has enabled. See thescreenshot in Test the MCP server for how to check this.2. Explicitly reference the name of the tool in your prompt. In VS Code, you can referenceyour tool by name. For example, Using #get_random_weather, what is the weather inRedmond?.3. Verify your MCP server is able to start. You can check this by clicking the "Start" buttonvisible above your MCP server configuration in the VS Code user or workspace settings.GitHub Copilot does not use your tool (an answer is providedwithout invoking your tool).Related contentGet started with .NET AI and the Model Context ProtocolModel Context Protocol .NET samplesBuild a minimal MCP clientPublish a packageFind and evaluate NuGet packages for your projectWhat's new in .NET 10Create a minimal MCP client using .NETArticle•05/30/2025In this quickstart, you build a minimal Model Context Protocol (MCP) client using the C# SDKfor MCP. You also learn how to configure the client to connect to an MCP server, such as theone created in the Build a minimal MCP server quickstart..NET 8.0 SDK or higherVisual Studio CodeComplete the following steps to create a .NET console app. The app acts as a host for an MCPclient that connects to an MCP server.1. In a terminal window, navigate to the directory where you want to create your app, andcreate a new console app with the dotnet new command:Console2. Navigate into the newly created project folder:Console3. Run the following commands to add the necessary NuGet packages:Prerequisites７ NoteThe MCP client you build in the sections ahead connects to the sample MCP server fromthe Build a minimal MCP server quickstart. You can also use your own MCP server if youprovide your own connection configuration.Create the .NET host appCreate the projectdotnet new console -n MCPHostAppcd MCPHostAppConsole4. Open the project folder in your editor of choice, such as Visual Studio Code:ConsoleReplace the contents of Program.cs with the following code:C#dotnet add package Azure.AI.OpenAI --prereleasedotnet add package Azure.Identitydotnet add package Microsoft.Extensions.AIdotnet add package Microsoft.Extensions.AI.OpenAI --prereleasedotnet add package ModelContextProtocol --prereleasecode .Add the app codeusing Azure.AI.OpenAI;using Azure.Identity;using Microsoft.Extensions.AI;using ModelContextProtocol.Client;using ModelContextProtocol.Protocol.Transport;// Create an IChatClient using Azure OpenAI.IChatClient client =    new ChatClientBuilder(        new AzureOpenAIClient(new Uri("<your-azure-openai-endpoint>"),        new DefaultAzureCredential())        .GetChatClient("gpt-4o").AsIChatClient())    .UseFunctionInvocation()    .Build();// Create the MCP client// Configure it to start and connect to your MCP server.var mcpClient = await McpClientFactory.CreateAsync(    new StdioClientTransport(new()    {        Command = "dotnet run",        Arguments = ["--project", "<path-to-your-mcp-server-project>"],        Name = "Minimal MCP Server",    }));// List all available tools from the MCP server.Console.WriteLine("Available tools:");var tools = await mcpClient.ListToolsAsync();foreach (var tool in tools){The preceding code accomplishes the following tasks:Initializes an IChatClient abstraction using the Microsoft.Extensions.AI libraries.Creates an MCP client and configures it to connect to your MCP server.Retrieves and displays a list of available tools from the MCP server, which is a standardMCP function.Implements a conversational loop that processes user prompts and utilizes the tools forresponses.Complete the following steps to test your .NET host app:1. In a terminal window open to the root of your project, run the following command tostart the app:Console2. Once the app is running, enter a prompt to run the ReverseEcho tool:Console    Console.WriteLine($"{tool}");}Console.WriteLine();// Conversational loop that can utilize the tools via prompts.List<ChatMessage> messages = [];while (true){    Console.Write("Prompt: ");    messages.Add(new(ChatRole.User, Console.ReadLine()));    List<ChatResponseUpdate> updates = [];    await foreach (var update in client        .GetStreamingResponseAsync(messages, new() { Tools = [.. tools] }))    {        Console.Write(update);        updates.Add(update);    }    Console.WriteLine();    messages.AddMessages(updates);}Run and test the appdotnet run3. Verify that the server responds with the echoed message:OutputGet started with .NET AI and the Model Context ProtocolReverse the following: "Hello, minimal MCP server!"!revres PCM laminim ,olleHRelated contentHow generative AI and LLMs work05/29/2025Generative AI is a type of artificial intelligence capable of creating original content, such asnatural language, images, audio, and code. The output of a generative AI is based on theinputs provided by the user. One common way for users to interact with generative AI isthrough chat applications that use natural language as their input. ChatGPT, developed byOpenAI, is a popular example of this. Generative AI applications that use natural language asan input are powered by large language models (LLMs) to perform natural languageprocessing (NLP).All generative AI is built on top of models. These models are trained with large sets of data inthe form of content, such as natural language, images, audio, and code. Generative AI modelsuse the patterns identified in the training data to produce new, statistically similar content.The input provided by the user is used by the AI model to build an output. The input is firstparsed into a form of data that the model can understand. The model then uses that data toidentify matching patterns from its training that it combines to build the final output.Generative AI models are designed to produce unique content, so they won't generate thesame output for identical inputs.Generative AI applications that support natural language as an input or output utilize LLMs todo so. The LLM is used to perform NLP, which classifies the input text and determines itssentiment. That classification and sentiment analysis is used by the generative AI model toidentify patterns and build the output. If the output is text, the LLM alone can be used togenerate it. If the output is audio or images, additional models are used to provide the dataand patterns for generating outputs in that format.Generative AI applications support a variety of potential use cases and potential outputs, whichare explored in the following sections.Some generative AI applications can produce natural language outputs, such as:Professionally worded summaries for a LinkedIn profileHow generative AI worksCommon uses of generative AINatural language generationRich descriptions of items in an online storefrontIdeas for healthy mealsFor example, given the user input Give me three ideas for a healthy breakfast including peppers,a generative AI application might produce the following output:OK, here are three healthy breakfasts that include peppers:1. Egg and Pepper Scramble: Dice up some bell peppers and sauté them in a pan until they'reslightly softened. Then, add in some beaten eggs and scramble everything together. Servewith whole-grain toast for a nutritious breakfast.2. Pepper and Avocado Toast: Toast a slice of whole-grain bread and top it with sliced bellpeppers, mashed avocado, and a sprinkle of salt and pepper. This is a great way to get somehealthy fats from the avocado along with the vitamin C from the peppers.3. Breakfast Burrito with Peppers: Sauté some diced bell peppers, onions, and scrambled eggstogether in a pan. Then, spoon the mixture onto a whole-grain tortilla and top with somesalsa or hot sauce. Roll it up and enjoy a satisfying and nutritious breakfast.Some generative AI applications produce image outputs from natural language inputs.Depending on the application and its models, users might be able to specify:The people or things to appear the imageThe artistic style to create the image inReferences for generating similar imagesImage generation can create virtual avatars for online accounts, design logos for a business, orprovide artistic inspiration for creators. For example, a user may input the request, Create animage of an elephant eating a burger. A generative AI application might produce the followingoutput:Image generationSome generative AI applications produce audio outputs from natural language inputs.Depending on the application and its models, users might be able to:Synthesize natural sounding voices from input textCreate music in a specific style or featuring certain instrumentsModify input audio files based on a set criteria provided in natural languageAudio generation can provide spoken responses from digital voice assistants, add backinginstruments to songs for music production, or reproduce a user's original voice from referencerecordings.Some generative AI applications produce code from natural language inputs. Depending onthe application and its models, users might be able to generate code that:Is in the language of their choiceAccepts a specific input and output structureUses specific technologies based on their reference documentationCode generation can provide quick implementations of common operations, such as sort orfilter data, speed up translating code from one language to another, or answer questions abouta technology based on specified reference documentation. For example, given the input "Showme how to code a game of tic-tac-toe with C#", a generative AI application might produce thefollowing output:Audio generationCode generationHere's a simple example of how you can code a game of tic-tac-toe in C#:C#This code creates a simple console-based tic-tac-toe game in C#. It uses a single-dimensionalarray to represent the board and checks for a win or draw after each move.When training an LLM, the training text is first broken down into tokens. Each token identifies aunique text value. A token can be a distinct word, a partial word, or a combination of wordsand punctuation. Each token is assigned an ID, which enables the text to be represented as asequence of token IDs.After the text has been broken down into tokens, a contextual vector, known as an embedding,is assigned to each token. These embedding vectors are multi-valued numeric data where eachelement of a token's vector represents a semantic attribute of the token. The elements of atoken's vector are determined based on how commonly tokens are used together or in similarcontexts.using System;namespace TicTacToe{    class Program    {        static char[] board = { '1', '2', '3', '4', '5', '6', '7', '8', '9' };        static int player = 1; // Player 1 will start        static int choice; // The number the player chooses        static int flag = 0; // Set the flag value to 0        // Check if the position is already marked or not        private static int CheckPosition(char mark)        {            for (int i = 0; i < 9; i++)            {                if (board[i] == mark)                {                    return 1;                }            }            return 0;        }        // The rest of the generated code has been omitted for brevity        // ...    }}How LLMs workThe goal is to predict the next token in the sequence based on the preceding tokens. A weightis assigned to each token in the existing sequence that represents its relative influence on thenext token. A calculation is then performed that uses the preceding tokens' weights andembeddings to predict the next vector value. The model then selects the most probable tokento continue the sequence based on the predicted vector.This process continues iteratively for each token in the sequence, with the output sequencebeing used regressively as the input for the next iteration. The output is built one token at atime. This strategy is analogous to how auto-complete works, where suggestions are based onwhat's been typed so far and updated with each new input.During training, the complete sequence of tokens is known, but all tokens that come after theone currently being considered are ignored. The predicted value for the next token's vector iscompared to the actual value and the loss is calculated. The weights are then incrementallyadjusted to reduce the loss and improve the model.Understand TokensPrompt engineeringLarge language modelsRelated contentUnderstand tokens05/29/2025Tokens are words, character sets, or combinations of words and punctuation that are generatedby large language models (LLMs) when they decompose text. Tokenization is the first step intraining. The LLM analyzes the semantic relationships between tokens, such as how commonlythey're used together or whether they're used in similar contexts. After training, the LLM usesthose patterns and relationships to generate a sequence of output tokens based on the inputsequence.The set of unique tokens that an LLM is trained on is known as its vocabulary.For example, consider the following sentence:I heard a dog bark loudly at a catThis text could be tokenized as:IheardadogbarkloudlyatacatBy having a sufficiently large set of training text, tokenization can compile a vocabulary ofmany thousands of tokens.The specific tokenization method varies by LLM. Common tokenization methods include:Word tokenization (text is split into individual words based on a delimiter)Character tokenization (text is split into individual characters)Subword tokenization (text is split into partial words or character sets)Turn text into tokensCommon tokenization methodsFor example, the GPT models, developed by OpenAI, use a type of subword tokenization that'sknown as Byte-Pair Encoding (BPE). OpenAI provides a tool to visualize how text will betokenized.There are benefits and disadvantages to each tokenization method:Token sizeProsConsSmaller tokens(character or subwordtokenization)- Enables the model to handle a widerrange of inputs, such as unknownwords, typos, or complex syntax.- Might allow the vocabulary size tobe reduced, requiring fewer memoryresources.- A given text is broken into moretokens, requiring additionalcomputational resources whileprocessing.- Given a fixed token limit, themaximum size of the model's inputand output is smaller.Larger tokens (wordtokenization)- A given text is broken into fewertokens, requiring fewer computationalresources while processing.- Given the same token limit, themaximum size of the model's inputand output is larger.- Might cause an increased vocabularysize, requiring more memoryresources.- Can limit the models ability tohandle unknown words, typos, orcomplex syntax.After the LLM completes tokenization, it assigns an ID to each unique token.Consider our example sentence:I heard a dog bark loudly at a catAfter the model uses a word tokenization method, it could assign token IDs as follows:I (1)heard (2)a (3)dog (4)bark (5)loudly (6)at (7)a (the "a" token is already assigned an ID of 3)cat (8)ﾉExpand tableHow LLMs use tokensBy assigning IDs, text can be represented as a sequence of token IDs. The example sentencewould be represented as [1, 2, 3, 4, 5, 6, 7, 3, 8]. The sentence "I heard a cat" would berepresented as [1, 2, 3, 8].As training continues, the model adds any new tokens in the training text to its vocabulary andassigns it an ID. For example:meow (9)run (10)The semantic relationships between the tokens can be analyzed by using these token IDsequences. Multi-valued numeric vectors, known as embeddings, are used to represent theserelationships. An embedding is assigned to each token based on how commonly it's usedtogether with, or in similar contexts to, the other tokens.After it's trained, a model can calculate an embedding for text that contains multiple tokens.The model tokenizes the text, then calculates an overall embeddings value based on thelearned embeddings of the individual tokens. This technique can be used for semanticdocument searches or adding vector stores to an AI.During output generation, the model predicts a vector value for the next token in thesequence. The model then selects the next token from its vocabulary based on this vectorvalue. In practice, the model calculates multiple vectors by using various elements of theprevious tokens' embeddings. The model then evaluates all potential tokens from these vectorsand selects the most probable one to continue the sequence.Output generation is an iterative operation. The model appends the predicted token to thesequence so far and uses that as the input for the next iteration, building the final output onetoken at a time.LLMs have limitations regarding the maximum number of tokens that can be used as input orgenerated as output. This limitation often causes the input and output tokens to be combinedinto a maximum context window. Taken together, a model's token limit and tokenizationmethod determine the maximum length of text that can be provided as input or generated asoutput.For example, consider a model that has a maximum context window of 100 tokens. The modelprocesses the example sentences as input text:I heard a dog bark loudly at a catToken limitsBy using a word-based tokenization method, the input is nine tokens. This leaves 91 wordtokens available for the output.By using a character-based tokenization method, the input is 34 tokens (including spaces). Thisleaves only 66 character tokens available for the output.Generative AI services often use token-based pricing. The cost of each request depends on thenumber of input and output tokens. The pricing might differ between input and output. Forexample, see Azure OpenAI Service pricing.Generative AI services might also be limited regarding the maximum number of tokens perminute (TPM). These rate limits can vary depending on the service region and LLM. For moreinformation about specific regions, see Azure OpenAI Service quotas and limits.How generative AI and LLMs workUnderstand embeddingsWork with vector databasesToken-based pricing and rate limitingRelated contentEmbeddings in .NET05/29/2025Embeddings are the way LLMs capture semantic meaning. They are numeric representations ofnon-numeric data that an LLM can use to determine relationships between concepts. You canuse embeddings to help an AI model understand the meaning of inputs so that it can performcomparisons and transformations, such as summarizing text or creating images from textdescriptions. LLMs can use embeddings immediately, and you can store embeddings in vectordatabases to provide semantic memory for LLMs as-needed.This section lists the main use cases for embeddings.Use your own databases to generate embeddings for your data and integrate it with an LLM tomake it available for completions. This use of embeddings is an important component ofretrieval-augmented generation.Use embeddings to increase the amount of context you can fit in a prompt without increasingthe number of tokens required.For example, suppose you want to include 500 pages of text in a prompt. The number oftokens for that much raw text will exceed the input token limit, making it impossible to directlyinclude in a prompt. You can use embeddings to summarize and break down large amounts ofthat text into pieces that are small enough to fit in one input, and then assess the similarity ofeach piece to the entire raw text. Then you can choose a piece that best preserves the semanticmeaning of the raw text and use it in your prompt without hitting the token limit.Use embeddings to help a model understand the meaning and context of text, and thenclassify, summarize, or translate that text. For example, you can use embeddings to helpmodels classify texts as positive or negative, spam or not spam, or news or opinion.Use cases for embeddingsUse your own data to improve completion relevanceIncrease the amount of text you can fit in a promptPerform text classification, summarization, or translationGenerate and transcribe audioUse audio embeddings to process audio files or inputs in your app.For example, Azure AI Speech supports a range of audio embeddings, including speech to textand text to speech. You can process audio in real-time or in batches.Semantic image processing requires image embeddings, which most LLMs can't generate. Usean image-embedding model such as ViT to create vector embeddings for images. Then youcan use those embeddings with an image generation model to create or modify images usingtext or vice versa. For example, you can use the DALL·E model to generate images such aslogos, faces, animals, and landscapes.Use embeddings to help a model create code from text or vice versa, by converting differentcode or text expressions into a common representation. For example, you can use embeddingsto help a model generate or document code in C# or Python.You generate embeddings for your raw data by using an AI embedding model, which canencode non-numeric data into a vector (a long array of numbers). The model can also decodean embedding into non-numeric data that has the same or similar meaning as the original, rawdata. There are many embedding models available for you to use, with OpenAI's text-embedding-ada-002 model being one of the common models that's used. For more examples,see the list of Embedding models available on Azure OpenAI.After you generate embeddings, you'll need a way to store them so you can later retrieve themwith calls to an LLM. Vector databases are designed to store and process vectors, so they're anatural home for embeddings. Different vector databases offer different processingcapabilities, so you should choose one based on your raw data and your goals. For informationabout your options, see available vector database solutions.When building LLM-based applications, you can use Semantic Kernel to integrate embeddingmodels and vector stores, so you can quickly pull in text data, and generate and storeTurn text into images or images into textGenerate or document codeChoose an embedding modelStore and process embeddings in a vector databaseUsing embeddings in your LLM solutionembeddings. This lets you use a vector database solution to store and retrieve semanticmemories.How GenAI and LLMs workRetrieval-augmented generationTraining: Develop AI agents with Azure OpenAI and Semantic KernelRelated contentVector databases for .NET + AI05/29/2025Vector databases are designed to store and manage vector embeddings. Embeddings arenumeric representations of non-numeric data that preserve semantic meaning. Words,documents, images, audio, and other types of data can all be vectorized. You can useembeddings to help an AI model understand the meaning of inputs so that it can performcomparisons and transformations, such as summarizing text, finding contextually related data,or creating images from text descriptions.For example, you can use a vector database to:Identify similar images, documents, and songs based on their contents, themes,sentiments, and styles.Identify similar products based on their characteristics, features, and user groups.Recommend content, products, or services based on user preferences.Identify the best potential options from a large pool of choices to meet complexrequirements.Identify data anomalies or fraudulent activities that are dissimilar from predominant ornormal patterns.Vector databases provide vector search capabilities to find similar items based on their datacharacteristics rather than by exact matches on a property field. Vector search works byanalyzing the vector representations of your data that you created using an AI embeddingmodel such the Azure OpenAI embedding models. The search process measures the distancebetween the data vectors and your query vector. The data vectors that are closest to your queryvector are the ones that are found to be most similar semantically.Some services such as Azure Cosmos DB for MongoDB vCore provide native vector searchcapabilities for your data. Other databases can be enhanced with vector search by indexing thestored data using a service such as Azure AI Search, which can scan and index your data toprovide vector search capabilities.Vector databases and their search features are especially useful in RAG pattern workflows withAzure OpenAI. This pattern allows you to augment or enhance your AI model with additionalsemantically rich knowledge of your data. A common AI workflow using vector databases mightinclude the following steps:Understand vector searchVector search workflows with .NET and OpenAI1. Create embeddings for your data using an OpenAI embedding model.2. Store and index the embeddings in a vector database or search service.3. Convert user prompts from your application to embeddings.4. Run a vector search across your data, comparing the user prompt embedding to theembeddings your database.5. Use a language model such as GPT-35 or GPT-4 to assembly a user friendly completionfrom the vector search results.Visit the Implement Azure OpenAI with RAG using vector search in a .NET app tutorial for ahands-on example of this flow.Other benefits of the RAG pattern include:Generate contextually relevant and accurate responses to user prompts from AI models.Overcome LLM tokens limits - the heavy lifting is done through the database vectorsearch.Reduce the costs from frequent fine-tuning on updated data.AI applications often use data vector databases and services to improve relevancy and providecustomized functionality. Many of these services provide a native SDK for .NET, while othersoffer a REST service you can connect to through custom code. Semantic Kernel provides anextensible component model that enables you to use different vector stores without needing tolearn each SDK.Semantic Kernel provides connectors for the following vector databases and services:VectorserviceSemantic Kernel connector.NET SDKAzure AISearchMicrosoft.SemanticKernel.Connectors.AzureAISearchAzure.Search.DocumentsAzureCosmos DBfor NoSQLMicrosoft.SemanticKernel.Connectors.AzureCosmosDBNoSQLMicrosoft.Azure.CosmosAzureCosmos DBforMongoDBMicrosoft.SemanticKernel.Connectors.AzureCosmosDBMongoDBMongoDb.DriverAvailable vector database solutionsﾉExpand tableVectorserviceSemantic Kernel connector.NET SDKAzurePostgreSQLServerMicrosoft.SemanticKernel.Connectors.PostgresNpgsqlAzure SQLDatabaseMicrosoft.SemanticKernel.Connectors.SqlServerMicrosoft.Data.SqlClientChromaMicrosoft.SemanticKernel.Connectors.ChromaNADuckDBMicrosoft.SemanticKernel.Connectors.DuckDBDuckDB.NET.Data.FullMilvusMicrosoft.SemanticKernel.Connectors.MilvusMilvus.ClientMongoDBAtlasVectorSearchMicrosoft.SemanticKernel.Connectors.MongoDBMongoDb.DriverPineconeMicrosoft.SemanticKernel.Connectors.PineconeREST APIPostgresMicrosoft.SemanticKernel.Connectors.PostgresNpgsqlQdrantMicrosoft.SemanticKernel.Connectors.QdrantQdrant.ClientRedisMicrosoft.SemanticKernel.Connectors.RedisStackExchange.RedisWeaviateMicrosoft.SemanticKernel.Connectors.WeaviateREST APITo discover .NET SDK and API support, visit the documentation for each respective service.Implement Azure OpenAI with RAG using vector search in a .NET appMore Semantic Kernel .NET connectorsRelated contentPrompt engineering in .NETArticle•04/09/2025In this article, you explore essential prompt engineering concepts. Many AI models are prompt-based, meaning they respond to user input text (a prompt) with a response generated bypredictive algorithms (a completion). Newer models also often support completions in chatform, with messages based on roles (system, user, assistant) and chat history to preserveconversations.Consider this text generation example where prompt is the user input and completion is themodel output:Prompt: "The president who served the shortest term was "Completion: "Pedro Lascurain."The completion appears correct, but what if your app is supposed to help U.S. history students?Pedro Lascurain's 45-minute term is the shortest term for any president, but he served as thepresident of Mexico. The U.S. history students are probably looking for "William HenryHarrison". Clearly, the app could be more helpful to its intended users if you gave it somecontext.Prompt engineering adds context to the prompt by providing instructions, examples, and cuesto help the model produce better completions.Models that support text generation often don't require any specific format, but you shouldorganize your prompts so it's clear what's an instruction and what's an example. Models thatsupport chat-based apps use three roles to organize completions: a system role that controlsthe chat, a user role to represent user input, and an assistant role for responding to users.Divide your prompts into messages for each role:System messages give the model instructions about the assistant. A prompt can have onlyone system message, and it must be the first message.User messages include prompts from the user and show examples, historical prompts, orcontain instructions for the assistant. An example chat completion must have at least oneuser message.Assistant messages show example or historical completions, and must contain a responseto the preceding user message. Assistant messages aren't required, but if you include oneit must be paired with a user message to form an example.Work with promptsAn instruction is text that tells the model how to respond. An instruction can be a directive oran imperative:Directives tell the model how to behave, but aren't simple commands—think charactersetup for an improv actor: "You're helping students learn about U.S. history, so talkabout the U.S. unless they specifically ask about other countries."Imperatives are unambiguous commands for the model to follow. "Translate to Tagalog:"Directives are more open-ended and flexible than imperatives:You can combine several directives in one instruction.Instructions usually work better when you use them with examples. However, becauseimperatives are unambiguous commands, models don't need examples to understandthem (though you might use an example to show the model how to format responses).Because a directive doesn't tell the model exactly what to do, each example can help themodel work better.It's usually better to break down a difficult instruction into a series of steps, which you cando with a sequence of directives. You should also tell the model to output the result ofeach step, so that you can easily make granular adjustments. Although you can breakdown the instruction into steps yourself, it's easier to just tell the model to do it, and tooutput the result of each step. This approach is called chain of thought prompting.You can provide content to add more context to instructions.Primary content is text that you want the model to process with an instruction. Whatever actionthe instruction entails, the model will perform it on the primary content to produce acompletion.Supporting content is text that you refer to in an instruction, but which isn't the target of theinstruction. The model uses the supporting content to complete the instruction, which meansthat supporting content also appears in completions, typically as some kind of structure (suchas in headings or column labels).Use labels with your instructional content to help the model figure out how to use it with theinstruction. Don't worry too much about precision—labels don't have to match instructionsexactly because the model will handle things like word form and capitalization.Suppose you use the instruction "Summarize US Presidential accomplishments" to produce alist. The model might organize and order it in any number of ways. But what if you want the listUse instructions to improve the completionPrimary and supporting content add contextto group the accomplishments by a specific set of categories? Use supporting content to addthat information to the instruction.Adjust your instruction so the model groups by category, and append supporting content thatspecifies those categories:C#An example is text that shows the model how to respond by providing sample user input andmodel output. The model uses examples to infer what to include in completions. Examples cancome either before or after the instructions in an engineered prompt, but the two shouldn't beinterspersed.An example starts with a prompt and can optionally include a completion. A completion in anexample doesn't have to include the verbatim response—it might just contain a formattedword, the first bullet in an unordered list, or something similar to indicate how each completionshould start.Examples are classified as zero-shot learning or few-shot learning based on whether theycontain verbatim completions.Zero-shot learning examples include a prompt with no verbatim completion. Thisapproach tests a model's responses without giving it example data output. Zero-shotprompts can have completions that include cues, such as indicating the model shouldoutput an ordered list by including "1." as the completion.Few-shot learning examples include several pairs of prompts with verbatim completions.Few-shot learning can change the model's behavior by adding to its existing knowledge.prompt = """Instructions: Summarize US Presidential accomplishments, grouped by category.Categories: Domestic Policy, US Economy, Foreign Affairs, Space Exploration.Accomplishments: 'George Washington- First president of the United States.- First president to have been a military veteran.- First president to be elected to a second term in office.- Received votes from every presidential elector in an election.- Filled the entire body of the United States federal judges; including the Supreme Court.- First president to be declared an honorary citizen of a foreign country, and an honorary citizen of France.John Adams ...' ///Text truncated""";Use examples to guide the modelA cue is text that conveys the desired structure or format of output. Like an instruction, a cueisn't processed by the model as if it were user input. Like an example, a cue shows the modelwhat you want instead of telling it what to do. You can add as many cues as you want, so youcan iterate to get the result you want. Cues are used with an instruction or an example andshould be at the end of the prompt.Suppose you use an instruction to tell the model to produce a list of presidentialaccomplishments by category, along with supporting content that tells the model whatcategories to use. You decide that you want the model to produce a nested list with all caps forcategories, with each president's accomplishments in each category listed on one line thatbegins with their name, with presidents listed chronologically. After your instruction andsupporting content, you could add three cues to show the model how to structure and formatthe list:C#DOMESTIC POLICY shows the model that you want it to start each group with thecategory in all caps.- George Washington: shows the model to start each section with George Washington'saccomplishments listed on one line.- John Adams: shows the model that it should list remaining presidents in chronologicalorder.Understand cuesprompt = """Instructions: Summarize US Presidential accomplishments, grouped by category.Categories: Domestic Policy, US Economy, Foreign Affairs, Space Exploration.Accomplishments: George WashingtonFirst president of the United States.First president to have been a military veteran.First president to be elected to a second term in office.First president to receive votes from every presidential elector in an election.First president to fill the entire body of the United States federal judges; including the Supreme Court.First president to be declared an honorary citizen of a foreign country, and an honorary citizen of France.John Adams ...  /// Text truncatedDOMESTIC POLICY- George Washington: - John Adams:""";Example prompt using .NET.NET provides various tools to prompt and chat with different AI models. Use Semantic Kernelto connect to a wide variety of AI models and services, as well as other SDKs such as the officialOpenAI .NET library. Semantic Kernel includes tools to create prompts with different rolesand maintain chat history, as well as many other features.Consider the following code example:C#The preceding code provides examples of the following concepts:Creates a chat history service to prompt the AI model for completions based on authorroles.Configures the AI with an AuthorRole.System message.using Microsoft.SemanticKernel;using Microsoft.SemanticKernel.ChatCompletion;// Create a kernel with OpenAI chat completion#pragma warning disable SKEXP0010Kernel kernel = Kernel.CreateBuilder()                    .AddOpenAIChatCompletion(                        modelId: "phi3:mini",                        endpoint: new Uri("http://localhost:11434"),                        apiKey: "")                    .Build();var aiChatService = kernel.GetRequiredService<IChatCompletionService>();var chatHistory = new ChatHistory();chatHistory.Add(    new ChatMessageContent(AuthorRole.System, "You are a helpful AI Assistant."));while (true){    // Get user prompt and add to chat history    Console.WriteLine("Your prompt:");    chatHistory.Add(new ChatMessageContent(AuthorRole.User, Console.ReadLine()));    // Stream the AI response and add to chat history    Console.WriteLine("AI Response:");    var response = "";    await foreach (var item in        aiChatService.GetStreamingChatMessageContentsAsync(chatHistory))    {        Console.Write(item.Content);        response += item.Content;    }    chatHistory.Add(new ChatMessageContent(AuthorRole.Assistant, response));    Console.WriteLine();}Accepts user input to allow for different types of prompts in the context of anAuthorRole.User.Asynchronously streams the completion from the AI to provide a dynamic chatexperience.You can also increase the power of your prompts with more advanced prompt engineeringtechniques that are covered in depth in their own articles.LLMs have token input limits that constrain the amount of text you can fit in a prompt.Use embeddings and vector database solutions to reduce the number of tokens you needto represent a given piece of text.LLMs aren't trained on your data unless you train them yourself, which can be costly andtime-consuming. Use retrieval augmented generation (RAG) to make your data availableto an LLM without training it.Prompt engineering techniquesConfigure prompts in Semantic KernelExtend your prompt engineering techniquesRelated contentChain of thought prompting05/29/2025GPT model performance and response quality benefits from prompt engineering, which is thepractice of providing instructions and examples to a model to prime or refine its output. Asthey process instructions, models make more reasoning errors when they try to answer rightaway rather than taking time to work out an answer. You can help the model reason its waytoward correct answers more reliably by asking for the model to include its chain of thought—that is, the steps it took to follow an instruction, along with the results of each step.Chain of thought prompting is the practice of prompting a model to perform a task step-by-step and to present each step and its result in order in the output. This simplifies promptengineering by offloading some execution planning to the model, and makes it easier toconnect any problem to a specific step so you know where to focus further efforts.It's generally simpler to just instruct the model to include its chain of thought, but you can useexamples to show the model how to break down tasks. The following sections show both ways.To use an instruction for chain of thought prompting, include a directive that tells the model toperform the task step-by-step and to output the result of each step.C#You can use examples to indicate the steps for chain of thought prompting, which the modelwill interpret to mean it should also output step results. Steps can include formatting cues.C#Use chain of thought prompting in instructionsprompt= """Instructions: Compare the pros and cons of EVs and petroleum-fueled vehicles.Break the task into steps, and output the result of each step as you perform it."""; Use chain of thought prompting in examplesprompt= """        Instructions: Compare the pros and cons of EVs and petroleum-fueled vehicles.        Differences between EVs and petroleum-fueled vehicles:        - Prompt engineering techniques        Differences ordered according to overall impact, highest-impact first:         1.                 Summary of vehicle type differences as pros and cons:        Pros of EVs        1.        Pros of petroleum-fueled vehicles        1.         """;Related contentZero-shot and few-shot learning05/29/2025This article explains zero-shot learning and few-shot learning for prompt engineering in .NET,including their primary use cases.GPT model performance benefits from prompt engineering, the practice of providinginstructions and examples to a model to refine its output. Zero-shot learning and few-shotlearning are techniques you can use when providing examples.Zero-shot learning is the practice of passing prompts that aren't paired with verbatimcompletions, although you can include completions that consist of cues. Zero-shot learningrelies entirely on the model's existing knowledge to generate responses, which reduces thenumber of tokens created and can help you control costs. However, zero-shot learning doesn'tadd to the model's knowledge or context.Here's an example zero-shot prompt that tells the model to evaluate user input to determinewhich of four possible intents the input represents, and then to preface the response with"Intent: ".C#There are two primary use cases for zero-shot learning:Work with fined-tuned LLMs - Because it relies on the model's existing knowledge, zero-shot learning is not as resource-intensive as few-shot learning, and it works well withLLMs that have already been fined-tuned on instruction datasets. You might be able torely solely on zero-shot learning and keep costs relatively low.Establish performance baselines - Zero-shot learning can help you simulate how yourapp would perform for actual users. This lets you evaluate various aspects of your model'scurrent performance, such as accuracy or precision. In this case, you typically use zero-shot learning to establish a performance baseline and then experiment with few-shotlearning to improve performance.Zero-shot learningprompt = $"""Instructions: What is the intent of this request?If you don't know the intent, don't guess; instead respond with "Unknown".Choices: SendEmail, SendMessage, CompleteTask, CreateDocument, Unknown.User Input: {request}Intent: """;Few-shot learning is the practice of passing prompts paired with verbatim completions (few-shot prompts) to show your model how to respond. Compared to zero-shot learning, thismeans few-shot learning produces more tokens and causes the model to update itsknowledge, which can make few-shot learning more resource-intensive. However, few-shotlearning also helps the model produce more relevant responses.C#Few-shot learning has two primary use cases:Tuning an LLM - Because it can add to the model's knowledge, few-shot learning canimprove a model's performance. It also causes the model to create more tokens thanzero-shot learning does, which can eventually become prohibitively expensive or eveninfeasible. However, if your LLM isn't fined-tuned yet, you won't always get goodperformance with zero-shot prompts, and few-shot learning is warranted.Fixing performance issues - You can use few-shot learning as a follow-up to zero-shotlearning. In this case, you use zero-shot learning to establish a performance baseline, andthen experiment with few-shot learning based on the zero-shot prompts you used. Thislets you add to the model's knowledge after seeing how it currently responds, so you caniterate and improve performance while minimizing the number of tokens you introduce.Example-based learning doesn't work well for complex reasoning tasks. However, addinginstructions can help address this.Few-shot learning requires creating lengthy prompts. Prompts with large number oftokens can increase computation and latency. This typically means increased costs.Few-shot learningprompt = $"""Instructions: What is the intent of this request?If you don't know the intent, don't guess; instead respond with "Unknown".Choices: SendEmail, SendMessage, CompleteTask, CreateDocument, Unknown.User Input: Can you send a very quick approval to the marketing team?Intent: SendMessageUser Input: Can you send the full update to the marketing team?Intent: SendEmailUser Input: {request}Intent:""";CaveatsThere's also a limit to the length of the prompts.When you use several examples the model can learn false patterns, such as "Sentimentsare twice as likely to be positive than negative."Prompt engineering techniquesHow GenAI and LLMs workRelated contentRetrieval-augmented generation (RAG)provides LLM knowledge05/29/2025This article describes how retrieval-augmented generation lets LLMs treat your data sources asknowledge without having to train.LLMs have extensive knowledge bases through training. For most scenarios, you can select anLLM that is designed for your requirements, but those LLMs still require additional training tounderstand your specific data. Retrieval-augmented generation lets you make your dataavailable to LLMs without training them on it first.To perform retrieval-augmented generation, you create embeddings for your data along withcommon questions about it. You can do this on the fly or you can create and store theembeddings by using a vector database solution.When a user asks a question, the LLM uses your embeddings to compare the user's question toyour data and find the most relevant context. This context and the user's question then go tothe LLM in a prompt, and the LLM provides a response based on your data.To perform RAG, you must process each data source that you want to use for retrievals. Thebasic process is as follows:1. Chunk large data into manageable pieces.2. Convert the chunks into a searchable format.3. Store the converted data in a location that allows efficient access. Additionally, it'simportant to store relevant metadata for citations or references when the LLM providesresponses.4. Feed your converted data to LLMs in prompts.How RAG worksBasic RAG processSource data: This is where your data exists. It could be a file/folder on your machine, a filein cloud storage, an Azure Machine Learning data asset, a Git repository, or an SQLdatabase.Data chunking: The data in your source needs to be converted to plain text. For example,word documents or PDFs need to be cracked open and converted to text. The text is thenchunked into smaller pieces.Converting the text to vectors: These are embeddings. Vectors are numericalrepresentations of concepts converted to number sequences, which make it easy forcomputers to understand the relationships between those concepts.Links between source data and embeddings: This information is stored as metadata onthe chunks you created, which are then used to help the LLMs generate citations whilegenerating responses.Prompt engineeringRelated contentUnderstand OpenAI function calling05/29/2025Function calling is an OpenAI model feature that lets you describe functions and theirarguments in prompts using JSON. Instead of invoking the function itself, the model returns aJSON output describing what functions should be called and the arguments to use.Function calling simplifies how you connect external tools to your AI model. First, you specifyeach tool's functions to the model. Then the model decides which functions should be called,based on the prompt question. The model uses the function call results to build a moreaccurate and consistent response.Potential use cases for function calling include:Answering questions by calling external APIs, for example, sending emails or getting theweather forecast.Answering questions with info from an internal datastore, for example, aggregating salesdata to answer, "What are my best-selling products?".Creating structured data from text info, for example, building a user info object withdetails from the chat history.The general steps for calling functions with an OpenAI model are:1. Send the user's question as a request with functions defined in the tools parameters.2. The model decides which functions, if any, to call. The output contains a JSON object thatlists the function calls and their arguments.3. Parse the output and call the requested functions with their specified arguments.4. Send another request with the function results included as a new message.5. The model responds with more function call requests or an answer to the user's question.Continue invoking the requested function calls until the model responds with ananswer.You can force the model to request a specific function by setting the tool_choice parameter tothe function's name. You can also force the model to respond with a message for the user byCall functions with OpenAI７ NoteThe model might hallucinate additional arguments.setting the tool_choice parameter to "none".Some models support parallel function calling, which enables the model to request multiplefunction calls in one output. The results of each function call are included together in oneresponse back to the model. Parallel function calling reduces the number of API requests andtime needed to generate an answer. Each function result is included as a new message in theconversation with a tool_call_id matching the id of the function call request.Not all OpenAI models are trained to support function calling. For a list of models that supportfunction calling or parallel function calling, see OpenAI - Supported Models.The Semantic Kernel SDK supports describing which functions are available to your AI using theKernelFunction decorator.The Kernel builds the tools parameter of a request based on your decorators, orchestrates therequested function calls to your code, and returns results back to the model.Function descriptions are include in the system message of your request to a model. Thesefunction descriptions count against your model's token limit and are included in the cost of therequest.If your request exceeds the model's token limit, try the following modifications:Reduce the number of functions.Shorten the function and argument descriptions in your JSON.Understanding tokensCreating native functions for AI to callPrompt engineeringCall functions in parallelSupported modelsFunction calling with the Semantic Kernel SDKToken countsRelated contentGet started with the 'Chat using your owndata sample' for .NET05/28/2025This article shows you how to deploy and run the Chat with your own data sample for .NET.This sample implements a chat app using C#, Azure OpenAI Service, and Retrieval AugmentedGeneration (RAG) in Azure AI Search to get answers about employee benefits at a fictitiouscompany. The employee benefits chat app is seeded with PDF files including an employeehandbook, a benefits document and a list of company roles and expectations.Demo videoBy following the instructions in this article, you will:Deploy a chat app to Azure.Get answers about employee benefits.Change settings to change behavior of responses.Once you complete this procedure, you can start modifying the new project with your customcode.This article is part of a collection of articles that show you how to build a chat app using AzureOpen AI Service and Azure AI Search.Other articles in the collection include:PythonJavaScriptJavaIn this sample application, a fictitious company called Contoso Electronics provides the chatapp experience to its employees to ask questions about the benefits, internal policies, and jobdescriptions and roles.The architecture of the chat app is shown in the following diagram:Architectural overviewUser interface - The application's chat interface is a Blazor WebAssembly application. Thisinterface is what accepts user queries, routes request to the application backend, anddisplays generated responses.Backend - The application backend is an ASP.NET Core Minimal API. The backend hoststhe Blazor static web application and is what orchestrates the interactions among thedifferent services. Services used in this application include:Azure Cognitive Search – Indexes documents from the data stored in an Azure StorageAccount. This makes the documents searchable using vector search capabilities.Azure OpenAI Service – Provides the Large Language Models (LLM) to generateresponses. Semantic Kernel is used in conjunction with the Azure OpenAI Service toorchestrate the more complex AI workflows.Most resources in this architecture use a basic or consumption pricing tier. Consumptionpricing is based on usage, which means you only pay for what you use. To complete this article,there will be a charge, but it will be minimal. When you are done with the article, you candelete the resources to stop incurring charges.For more information, see Azure Samples: Cost in the sample repo.A development container environment is available with all dependencies required tocomplete this article. You can run the development container in GitHub Codespaces (in aCostPrerequisitesbrowser) or locally using Visual Studio Code.To follow along with this article, you need the following prerequisites:An Azure subscription - Create one for freeAzure account permissions - Your Azure account must haveMicrosoft.Authorization/roleAssignments/write permissions, such as User AccessAdministrator or Owner.GitHub accountBegin now with a development environment that has all the dependencies installed tocomplete this article.GitHub Codespaces runs a development container managed by GitHub with VisualStudio Code for the Web as the user interface. For the most straightforwarddevelopment environment, use GitHub Codespaces so that you have the correct developertools and dependencies preinstalled to complete this article.1. Start the process to create a new GitHub codespace on the main branch of theAzure-Samples/azure-search-openai-demo-csharp GitHub repository.2. To have both the development environment and the documentation available at thesame time, right-click on the following Open in GitHub Codespaces button, andselect Open link in new windows.Codespaces (recommended)Open development environmentGitHub Codespaces (recommended)） ImportantAll GitHub accounts can use Codespaces for up to 60 hours free each month with 2core instances. For more information, see GitHub Codespaces monthly includedstorage and core hours.3. On the Create codespace page, review the codespace configuration settings andthen select Create new codespace:4. Wait for the codespace to start. This startup process can take a few minutes.5. In the terminal at the bottom of the screen, sign in to Azure with the AzureDeveloper CLI.Bash6. Copy the code from the terminal and then paste it into a browser. Follow theinstructions to authenticate with your Azure account.7. The remaining tasks in this article take place in the context of this developmentcontainer.The sample repository contains all the code and configuration files you need to deploy a chatapp to Azure. The following steps walk you through the process of deploying the sample toAzure.azd auth loginDeploy and run1. Run the following Azure Developer CLI command to provision the Azure resources anddeploy the source code:Bash2. When you're prompted to enter an environment name, keep it short and lowercase. Forexample, myenv. Its used as part of the resource group name.3. When prompted, select a subscription to create the resources in.4. When you're prompted to select a location the first time, select a location near you. Thislocation is used for most the resources including hosting.5. If you're prompted for a location for the OpenAI model, select a location that is near you.If the same location is available as your first location, select that.6. Wait until app is deployed. It may take up to 20 minutes for the deployment to complete.7. After the application has been successfully deployed, you see a URL displayed in theterminal.8. Select that URL labeled Deploying service web to open the chat application in a browser.Deploy chat app to Azure） ImportantAzure resources created in this section incur immediate costs, primarily from the Azure AISearch resource. These resources may accrue costs even if you interrupt the commandbefore it is fully executed.azd upThe chat app is preloaded with employee benefits information from PDF files. You can usethe chat app to ask questions about the benefits. The following steps walk you through theprocess of using the chat app.1. In the browser, navigate to the Chat page using the left navigation.2. Select or enter "What is included in my Northwind Health Plus plan that is not instandard?" in the chat text box. Your response is similar to the following image.Use chat app to get answers from PDF files3. From the answer, select a citation. A pop-up window will open displaying the source ofthe information.4. Navigate between the tabs at the top of the answer box to understand how the answerwas generated.TabDescriptionThoughtprocessThis is a script of the interactions in chat. You can view the system prompt(content) and your user question (content).SupportingcontentThis includes the information to answer your question and the source material.The number of source material citations is noted in the Developer settings. Thedefault value is 3.CitationThis displays the source page that contains the citation.5. When you're done, navigate back to the answer tab.The intelligence of the chat is determined by the OpenAI model and the settings that are usedto interact with the model.ﾉExpand tableUse chat app settings to change behavior of responsesﾉExpand tableSettingDescriptionOverride prompttemplateThis is the prompt that is used to generate the answer.Retrieve this manysearch resultsThis is the number of search results that are used to generate the answer. Youcan see these sources returned in the Thought process and Supporting contenttabs of the citation.Exclude categoryThis is the category of documents that are excluded from the search results.Use semantic rankerfor retrievalThis is a feature of Azure AI Search that uses machine learning to improve therelevance of search results.Retrieval modeVectors + Text means that the search results are based on the text of thedocuments and the embeddings of the documents. Vectors means that thesearch results are based on the embeddings of the documents. Text means thatthe search results are based on the text of the documents.Use query-contextualsummaries instead ofwhole documentsWhen both Use semantic ranker and Use query-contextual summaries arechecked, the LLM uses captions extracted from key passages, instead of all thepassages, in the highest ranked documents.Suggest follow-upquestionsHave the chat app suggest follow-up questions based on the answer.The following steps walk you through the process of changing the settings.1. In the browser, select the gear icon in the upper right of the page.2. If not selected, select the Suggest follow-up questions checkbox and ask the samequestion again.TextThe chat might return with follow-up question suggestions.3. In the Settings tab, deselect Use semantic ranker for retrieval.4. Ask the same question again.Text5. What is the difference in the answers?What is included in my Northwind Health Plus plan that is not in standard?What is my deductible?The response that used the Semantic ranker provided a single answer. The responsewithout semantic ranking returned a less direct answer.To finish, clean up the Azure and GitHub CodeSpaces resources you used.The Azure resources created in this article are billed to your Azure subscription. If you don'texpect to need these resources in the future, delete them to avoid incurring more charges.Run the following Azure Developer CLI command to delete the Azure resources and removethe source code:BashDeleting the GitHub Codespaces environment ensures that you can maximize the amountof free per-core hours entitlement you get for your account.1. Sign into the GitHub Codespaces dashboard (https://github.com/codespaces).2. Locate your currently running codespaces sourced from the Azure-Samples/azure-search-openai-demo-csharp GitHub repository.Clean up resourcesClean up Azure resourcesazd down --purgeClean up GitHub CodespacesGitHub Codespaces） ImportantFor more information about your GitHub account's entitlements, see GitHubCodespaces monthly included storage and core hours.3. Open the context menu for the codespace and then select Delete.This sample repository offers troubleshooting information.If your issue isn't addressed, log your issue to the repository's Issues.Get helpNext stepsGet the source code for the sample used in this articleBuild a chat app with Azure OpenAI best practice solution architectureAccess control in Generative AI Apps with Azure AI SearchBuild an Enterprise ready OpenAI solution with Azure API ManagementOutperforming vector search with hybrid retrieval and ranking capabilitiesImplement Azure OpenAI with RAGusing vector search in a .NET appArticle•11/24/2024This tutorial explores integration of the RAG pattern using Open AI models and vectorsearch capabilities in a .NET app. The sample application performs vector searches oncustom data stored in Azure Cosmos DB for MongoDB and further refines the responsesusing generative AI models, such as GPT-35 and GPT-4. In the sections that follow, you'llset up a sample application and explore key code examples that demonstrate theseconcepts..NET 8.0 installedAn Azure AccountAn Azure Cosmos DB for MongoDB vCore serviceAn Azure Open AI serviceDeploy text-embedding-ada-002 model for embeddingsDeploy gpt-35-turbo model for chat completionsThe Cosmos Recipe Guide app allows you to perform vector and AI driven searchesagainst a set of recipe data. You can search directly for available recipes or prompt theapp with ingredient names to find related recipes. The app and the sections ahead guideyou through the following workflow to demonstrate this type of functionality:1. Upload sample data to an Azure Cosmos DB for MongoDB database.2. Create embeddings and a vector index for the uploaded sample data using theAzure OpenAI text-embedding-ada-002 model.3. Perform vector similarity search based on the user prompts.4. Use the Azure OpenAI gpt-35-turbo completions model to compose moremeaningful answers based on the search results data.PrerequisitesApp overview1. Clone the following GitHub repository:Bash2. In the C#/CosmosDB-MongoDBvCore folder, open the CosmosRecipeGuide.sln file.3. In the appsettings.json file, replace the following config values with your AzureOpenAI and Azure CosmosDB for MongoDb values:JSON4. Launch the app by pressing the Start button at the top of Visual Studio.When you run the app for the first time, it connects to Azure Cosmos DB and reportsthat there are no recipes available yet. Follow the steps displayed by the app to beginGet startedgit clone https://github.com/microsoft/AzureDataRetrievalAugmentedGenerationSamples.git"OpenAIEndpoint": "https://<your-service-name>.openai.azure.com/","OpenAIKey": "<your-api-key>","OpenAIEmbeddingDeployment": "<your-ada-deployment-name>","OpenAIcompletionsDeployment": "<your-gpt-deployment-name>","MongoVcoreConnection": "<your-mongo-connection-string>"Explore the appthe core workflow.1. Select Upload recipe(s) to Cosmos DB and press Enter. This command readssample JSON files from the local project and uploads them to the Cosmos DBaccount.The code from the Utility.cs class parses the local JSON files.C#The UpsertVectorAsync method in the VCoreMongoService.cs file uploads thedocuments to Azure Cosmos DB for MongoDB.C#public static List<Recipe> ParseDocuments(string Folderpath){    List<Recipe> recipes = new List<Recipe>();    Directory.GetFiles(Folderpath)        .ToList()        .ForEach(f =>        {            var jsonString= System.IO.File.ReadAllText(f);            Recipe recipe = JsonConvert.DeserializeObject<Recipe>(jsonString);            recipe.id = recipe.name.ToLower().Replace(" ", "");            ret.Add(recipe);        }    );    return recipes;}public async Task UpsertVectorAsync(Recipe recipe)    {        BsonDocument document = recipe.ToBsonDocument();        if (!document.Contains("_id"))        {            Console.WriteLine("UpsertVectorAsync: Document does not contain _id.");            throw new ArgumentException("UpsertVectorAsync: Document does not contain _id.");        }        string? _idValue = document["_id"].ToString();        try        {            var filter = Builders<BsonDocument>.Filter.Eq("_id", 2. Select Vectorize the recipe(s) and store them in Cosmos DB.The JSON items uploaded to Cosmos DB do not contain embeddings andtherefore are not optimized for RAG via vector search. An embedding is aninformation-dense, numerical representation of the semantic meaning of a pieceof text. Vector searches are able to find items with contextually similarembeddings.The GetEmbeddingsAsync method in the OpenAIService.cs file creates an embeddingfor each item in the database.C#_idValue);            var options = new ReplaceOptions { IsUpsert = true };            await _recipeCollection.ReplaceOneAsync(filter, document, options);        }        catch (Exception ex)        {            Console.WriteLine($"Exception: UpsertVectorAsync(): {ex.Message}");            throw;        }    }public async Task<float[]?> GetEmbeddingsAsync(dynamic data){    try    {        EmbeddingsOptions options = new EmbeddingsOptions(data)        {            Input = data        };        var response = await _openAIClient.GetEmbeddingsAsync(openAIEmbeddingDeployment, options);        Embeddings embeddings = response.Value;        float[] embedding = embeddings.Data[0].Embedding.ToArray();        return embedding;    }    catch (Exception ex)    {        Console.WriteLine($"GetEmbeddingsAsync Exception: {ex.Message}");        return null;    }}The CreateVectorIndexIfNotExists in the VCoreMongoService.cs file creates avector index, which enables you to perform vector similarity searches.C#3. Select the Ask AI Assistant (search for a recipe by name or description, or ask aquestion) option in the application to run a user query.public void CreateVectorIndexIfNotExists(string vectorIndexName){    try    {        //Find if vector index exists in vectors collection        using (IAsyncCursor<BsonDocument> indexCursor = _recipeCollection.Indexes.List())        {            bool vectorIndexExists = indexCursor.ToList().Any(x => x["name"] == vectorIndexName);            if (!vectorIndexExists)            {                BsonDocumentCommand<BsonDocument> command = new BsonDocumentCommand<BsonDocument>(                    BsonDocument.Parse(@"                        { createIndexes: 'Recipe',                            indexes: [{                            name: 'vectorSearchIndex',                            key: { embedding: 'cosmosSearch' },                            cosmosSearchOptions: {                                kind: 'vector-ivf',                                numLists: 5,                                similarity: 'COS',                                dimensions: 1536 }                            }]                        }"));                BsonDocument result = _database.RunCommand(command);                if (result["ok"] != 1)                {                    Console.WriteLine("CreateIndex failed with response: " + result.ToJson());                }            }        }    }    catch (MongoException ex)    {        Console.WriteLine("MongoDbService InitializeVectorIndex: " + ex.Message);        throw;    }}The user query is converted to an embedding using the Open AI service and theembedding model. The embedding is then sent to Azure Cosmos DB for MongoDBand is used to perform a vector search. The VectorSearchAsync method in theVCoreMongoService.cs file performs a vector search to find vectors that are close tothe supplied vector and returns a list of documents from Azure Cosmos DB forMongoDB vCore.C#public async Task<List<Recipe>> VectorSearchAsync(float[] queryVector)    {        List<string> retDocs = new List<string>();        string resultDocuments = string.Empty;        try        {            //Search Azure Cosmos DB for MongoDB vCore collection for similar embeddings            //Project the fields that are needed            BsonDocument[] pipeline = new BsonDocument[]            {                BsonDocument.Parse(                    @$"{{$search: {{                            cosmosSearch:                                {{ vector: [{string.Join(',', queryVector)}],                                   path: 'embedding',                                   k: {_maxVectorSearchResults}}},                                   returnStoredSource:true                                }}                            }}"),                BsonDocument.Parse($"{{$project: {{embedding: 0}}}}"),            };            var bsonDocuments = await _recipeCollection                .Aggregate<BsonDocument>(pipeline).ToListAsync();            var recipes = bsonDocuments                .ToList()                .ConvertAll(bsonDocument =>                    BsonSerializer.Deserialize<Recipe>(bsonDocument));            return recipes;        }        catch (MongoException ex)        {            Console.WriteLine($"Exception: VectorSearchAsync(): {ex.Message}");            throw;        }    }The GetChatCompletionAsync method generates an improved chat completionresponse based on the user prompt and the related vector search results.C#public async Task<(string response, int promptTokens, int responseTokens)> GetChatCompletionAsync(string userPrompt, string documents){    try    {        ChatMessage systemMessage = new ChatMessage(            ChatRole.System, _systemPromptRecipeAssistant + documents);        ChatMessage userMessage = new ChatMessage(            ChatRole.User, userPrompt);        ChatCompletionsOptions options = new()        {            Messages =            {                systemMessage,                userMessage            },            MaxTokens = openAIMaxTokens,            Temperature = 0.5f, //0.3f,            NucleusSamplingFactor = 0.95f,            FrequencyPenalty = 0,            PresencePenalty = 0        };        Azure.Response<ChatCompletions> completionsResponse =            await openAIClient.GetChatCompletionsAsync(openAICompletionDeployment, options);        ChatCompletions completions = completionsResponse.Value;        return (            response: completions.Choices[0].Message.Content,            promptTokens: completions.Usage.PromptTokens,            responseTokens: completions.Usage.CompletionTokens        );    }    catch (Exception ex)    {        string message = $"OpenAIService.GetChatCompletionAsync(): {ex.Message}";        Console.WriteLine(message);        throw;    }}The app also uses prompt engineering to ensure Open AI service limits andformats the response for supplied recipes.C#//System prompts to send with user prompts to instruct the model for chat sessionprivate readonly string _systemPromptRecipeAssistant = @"    You are an intelligent assistant for Contoso Recipes.    You are designed to provide helpful answers to user questions about    recipes, cooking instructions provided in JSON format below.    Instructions:    - Only answer questions related to the recipe provided below.    - Don't reference any recipe not provided below.    - If you're unsure of an answer, say ""I don't know"" and recommend users search themselves.    - Your response  should be complete.    - List the Name of the Recipe at the start of your response followed by step by step cooking instructions.    - Assume the user is not an expert in cooking.    - Format the content so that it can be printed to the Command Line console.    - In case there is more than one recipe you find, let the user pick the most appropriate recipe.";Scale Azure OpenAI for .NET chat usingRAG with Azure Container Apps05/29/2025Learn how to add load balancing to your application to extend the chat app beyond the AzureOpenAI Service token and model quota limits. This approach uses Azure Container Apps tocreate three Azure OpenAI endpoints and a primary container to direct incoming traffic to oneof the three endpoints.This article requires you to deploy two separate samples:Chat appIf you haven't deployed the chat app yet, wait until after the load balancer sample isdeployed.If you already deployed the chat app once, change the environment variable tosupport a custom endpoint for the load balancer and redeploy it again.The chat app is available in these languages:.NETJavaScriptPythonLoad balancer appBecause the Azure OpenAI resource has specific token and model quota limits, a chat app thatuses a single Azure OpenAI resource is prone to have conversation failures because of thoselimits.７ NoteThis article uses one or more AI app templates as the basis for the examples andguidance in the article. AI app templates provide you with well-maintained referenceimplementations that are easy to deploy. They help to ensure a high-quality starting pointfor your AI apps.Architecture for load balancing Azure OpenAI withAzure Container AppsTo use the chat app without hitting those limits, use a load-balanced solution with ContainerApps. This solution seamlessly exposes a single endpoint from Container Apps to your chatapp server.The container app sits in front of a set of Azure OpenAI resources. The container app solvestwo scenarios: normal and throttled. During a normal scenario where token and model quota isavailable, the Azure OpenAI resource returns a 200 back through the container app and appserver.When a resource is in a throttled scenario because of quota limits, the container app can retry adifferent Azure OpenAI resource immediately to fulfill the original chat app request.Azure subscription. Create one for free.Dev containers are available for both samples, with all dependencies required to completethis article. You can run the dev containers in GitHub Codespaces (in a browser) or locally usingPrerequisitesVisual Studio Code.Only a GitHub account is required to use CodeSpacesGitHub Codespaces runs a development container managed by GitHub with VisualStudio Code for the Web as the user interface. For the most straightforwarddevelopment environment, use GitHub Codespaces so that you have the correct developertools and dependencies preinstalled to complete this article.1. Sign in to the Azure Developer CLI to provide authentication to the provisioning anddeployment steps:Bash2. Set an environment variable to use Azure CLI authentication to the post provision step:BashCodespaces (recommended)Open the Container Apps load balancer sampleappGitHub Codespaces (recommended)） ImportantAll GitHub accounts can use GitHub Codespaces for up to 60 hours free each monthwith two core instances. For more information, see GitHub Codespaces monthlyincluded storage and core hours.Deploy the Azure Container Apps load balancerazd auth login --use-device-code3. Deploy the load balancer app:BashSelect a subscription and region for the deployment. They don't have to be the samesubscription and region as the chat app.4. Wait for the deployment to finish before you continue.1. Use the following command to display the deployed endpoint for the container app:Bash2. Copy the CONTAINER_APP_URL value. You use it in the next section.These examples are completed on the chat app sample.1. Open the chat app sample's dev container by using one of the following choices.LanguageGitHub CodespacesVisual Studio Code.NETDev ContainersDev ContainersOpenOpenJavaScriptDev ContainersDev ContainersOpenOpenazd config set auth.useAzCliAuth "true"azd upGet the deployment endpointazd env get-valuesRedeploy the chat app with the load balancerendpointInitial deploymentﾉExpand tableLanguageGitHub CodespacesVisual Studio CodePythonDev ContainersDev ContainersOpenOpen2. Sign in to the Azure Developer CLI (AZD):BashFinish the sign-in instructions.3. Create an AZD environment with a name such as chat-app:Bash4. Add the following environment variable, which tells the chat app's backend to use acustom URL for the Azure OpenAI requests:Bash5. Add the following environment variable. Substitute <CONTAINER_APP_URL> for the URLfrom the previous section. This action tells the chat app's backend what the value isof the custom URL for the Azure OpenAI request.Bash6. Deploy the chat app:BashYou can now use the chat app with the confidence that it's built to scale across many userswithout running out of quota.azd auth loginazd env new <name>azd env set OPENAI_HOST azure_customazd env set AZURE_OPENAI_CUSTOM_URL <CONTAINER_APP_URL>azd up1. In the Azure portal, search your resource group.2. From the list of resources in the group, select the Azure Container Apps resource.3. Select Monitoring > Log stream to view the log.4. Use the chat app to generate traffic in the log.5. Look for the logs, which reference the Azure OpenAI resources. Each of the threeresources has its numeric identity in the log comment that begins with Proxying tohttps://openai3, where 3 indicates the third Azure OpenAI resource.When the load balancer receives status that the request exceeds quota, the load balancerautomatically rotates to another resource.By default, each of the Azure OpenAI instances in the load balancer is deployed with a capacityof 30,000 tokens per minute (TPM). You can use the chat app with the confidence that it's builtto scale across many users without running out of quota. Change this value when:Stream logs to see the load balancer resultsConfigure the TPM quotaYou get deployment capacity errors: Lower the value.You need higher capacity: Raise the value.1. Use the following command to change the value:Bash2. Redeploy the load balancer:BashWhen you're finished with the chat app and the load balancer, clean up the resources. TheAzure resources created in this article are billed to your Azure subscription. If you don't expectto need these resources in the future, delete them to avoid incurring more charges.Return to the chat app article to clean up the resources:.NETJavaScriptPythonRun the following Azure Developer CLI command to delete the Azure resources and removethe source code:BashThe switches provide:purge: Deleted resources are immediately purged so that you can reuse the AzureOpenAI Service tokens per minute.azd env set OPENAI_CAPACITY 50azd upClean up resourcesClean up chat app resourcesClean upload balancer resourcesazd down --purge --forceforce: The deletion happens silently, without requiring user consent.Deleting the GitHub Codespaces environment ensures that you can maximize the amountof free per-core hours entitlement that you get for your account.1. Sign in to the GitHub Codespaces dashboard.2. Locate your currently running codespaces that are sourced from the azure-samples/openai-aca-lb GitHub repository.3. Open the context menu for the codespace, and then select Delete.Clean up GitHub Codespaces and Visual Studio CodeGitHub Codespaces） ImportantFor more information about your GitHub account's entitlements, see GitHubCodespaces monthly included storage and core hours.If you have trouble deploying the Azure API Management load balancer, add your issue to therepository's Issues webpage.Samples used in this article include:.NET chat app with RAGLoad Balancer with Azure Container AppsUse Azure Load Testing to load test your chat appGet helpSample codeNext stepAzure AI services authentication andauthorization using .NETArticle•04/09/2025Application requests to Azure AI Services must be authenticated. In this article, you explore theoptions available to authenticate to Azure OpenAI and other AI services using .NET. Theseconcepts apply to the Semantic Kernel SDK, as well as SDKs from specific services such asAzure OpenAI. Most AI services offer two primary ways to authenticate apps and users:Key-based authentication provides access to an Azure service using secret key values.These secret values are sometimes known as API keys or access keys depending on theservice.Microsoft Entra ID provides a comprehensive identity and access management solutionto ensure that the correct identities have the correct level of access to different Azureresources.The sections ahead provide conceptual overviews for these two approaches, rather thandetailed implementation steps. For more detailed information about connecting to Azureservices, visit the following resources:Authenticate .NET apps to Azure servicesIdentity fundamentalsWhat is Azure RBAC?Access keys allow apps and tools to authenticate to an Azure AI service, such as Azure OpenAI,using a secret key provided by the service. Retrieve the secret key using tools such as the Azureportal or Azure CLI and use it to configure your app code to connect to the AI service:C#７ NoteThe examples in this article focus primarily on connections to Azure OpenAI, but the sameconcepts and implementation steps directly apply to many other Azure AI services as well.Authentication using keysbuilder.Services.AddAzureOpenAIChatCompletion(    "deployment-model",    "service-endpoint",Using keys is a straightforward option, but this approach should be used with caution. Keysaren't the recommended authentication option because they:Don't follow the principle of least privilege. They provide elevated permissions regardlessof who uses them or for what task.Can accidentally be checked into source control or stored in unsafe locations.Can easily be shared with or sent to parties who shouldn't have access.Often require manual administration and rotation.Instead, consider using Microsoft Entra ID for authentication, which is the recommendedsolution for most scenarios.Microsoft Entra ID is a cloud-based identity and access management service that provides avast set of features for different business and app scenarios. Microsoft Entra ID is therecommended solution to connect to Azure OpenAI and other AI services and provides thefollowing benefits:Keyless authentication using identities.Role-based access control (RBAC) to assign identities the minimum required permissions.Can use the Azure.Identity client library to detect different credentials acrossenvironments without requiring code changes.Automatically handles administrative maintenance tasks such as rotating underlying keys.The workflow to implement Microsoft Entra authentication in your app generally includes thefollowing steps:Local development:1. Sign-in to Azure using a local dev tool such as the Azure CLI or Visual Studio.2. Configure your code to use the Azure.Identity client library andDefaultAzureCredential class.3. Assign Azure roles to the account you signed-in with to enable access to the AIservice.Azure-hosted app:1. Deploy the app to Azure after configuring it to authenticate using theAzure.Identity client library.    "service-key"); // Secret keyvar kernel = builder.Build();Authentication using Microsoft Entra ID2. Assign a managed identity to the Azure-hosted app.3. Assign Azure roles to the managed identity to enable access to the AI service.The key concepts of this workflow are explored in the following sections.When developing apps locally that connect to Azure AI services, authenticate to Azure using atool such as Visual Studio or the Azure CLI. Your local credentials can be discovered by theAzure.Identity client library and used to authenticate your app to Azure services, as describedin the Configure the app code section.For example, to authenticate to Azure locally using the Azure CLI, run the following command:Azure CLIUse the Azure.Identity client library from the Azure SDK to implement Microsoft Entraauthentication in your code. The Azure.Identity libraries include the DefaultAzureCredentialclass, which automatically discovers available Azure credentials based on the currentenvironment and tooling available. Visit the Azure SDK for .NET documentation for the full setof supported environment credentials and the order in which they are searched.For example, configure Semantic Kernel to authenticate using DefaultAzureCredential usingthe following code:C#DefaultAzureCredential enables apps to be promoted from local development to productionwithout code changes. For example, during development DefaultAzureCredential uses yourlocal user credentials from Visual Studio or the Azure CLI to authenticate to the AI service.Authenticate to Azure locallyaz loginConfigure the app codeKernel kernel = Kernel    .CreateBuilder()    .AddAzureOpenAITextGeneration(        "your-model",        "your-endpoint",        new DefaultAzureCredential())    .Build();When the app is deployed to Azure, DefaultAzureCredential uses the managed identity that isassigned to your app.Azure role-based access control (Azure RBAC) is a system that provides fine-grained accessmanagement of Azure resources. Assign a role to the security principal used byDefaultAzureCredential to connect to an Azure AI service, whether that's an individual user,group, service principal, or managed identity. Azure roles are a collection of permissions thatallow the identity to perform various tasks, such as generate completions or create and deleteresources.Assign roles such as Cognitive Services OpenAI User (role ID: 5e0bd9bd-7b93-4f28-af87-19fc36ad61bd) to the relevant identity using tools such as the Azure CLI, Bicep, or the AzurePortal. For example, use the az role assignment create command to assign a role using theAzure CLI:Azure CLILearn more about Azure RBAC using the following resources:What is Azure RBAC?Grant a user accessRBAC best practicesIn most scenarios, Azure-hosted apps should use a managed identity to connect to otherservices such as Azure OpenAI. Managed identities provide a fully managed identity inMicrosoft Entra ID for apps to use when connecting to resources that support Microsoft Entraauthentication. DefaultAzureCredential discovers the identity associated with your app anduses it to authenticate to other Azure services.There are two types of managed identities you can assign to your app:Assign roles to your identityaz role assignment create \        --role "5e0bd9bd-7b93-4f28-af87-19fc36ad61bd" \        --assignee-object-id "$PRINCIPAL_ID" \        --scope /subscriptions/"$SUBSCRIPTION_ID"/resourceGroups/"$RESOURCE_GROUP" \        --assignee-principal-type UserAssign a managed identity to your appA system-assigned identity is tied to your application and is deleted if your app isdeleted. An app can only have one system-assigned identity.A user-assigned identity is a standalone Azure resource that can be assigned to your app.An app can have multiple user-assigned identities.Assign roles to a managed identity just like you would an individual user account, such as theCognitive Services OpenAI User role. learn more about working with managed identities usingthe following resources:Managed identities overviewAuthenticate App Service to Azure OpenAI using Microsoft Entra IDHow to use managed identities for App Service and Azure FunctionsAuthenticate to Azure OpenAI from anAzure hosted app using Microsoft Entra ID05/29/2025This article demonstrates how to use Microsoft Entra ID managed identities and theMicrosoft.Extensions.AI library to authenticate an Azure hosted app to an Azure OpenAIresource.A managed identity from Microsoft Entra ID allows your app to easily access other MicrosoftEntra protected resources such as Azure OpenAI. The identity is managed by the Azureplatform and doesn't require you to provision, manage, or rotate any secrets.An Azure account that has an active subscription. Create an account for free..NET SDKCreate and deploy an Azure OpenAI Service resourceCreate and deploy a .NET application to App ServiceManaged identities provide an automatically managed identity in Microsoft Entra ID forapplications to use when connecting to resources that support Microsoft Entra authentication.Applications can use managed identities to obtain Microsoft Entra tokens without having tomanage any credentials. Your application can be assigned two types of identities:A system-assigned identity is tied to your application and is deleted if your app isdeleted. An app can have only one system-assigned identity.A user-assigned identity is a standalone Azure resource that can be assigned to your app.An app can have multiple user-assigned identities.1. Navigate to your app's page in the Azure portal, and then scroll down to theSettings group.2. Select Identity.3. On the System assigned tab, toggle Status to On, and then select Save.PrerequisitesAdd a managed identity to App ServiceSystem-assigned1. In the Azure Portal, navigate to the scope that you want to grant Azure OpenAI accessto. The scope can be a Management group, Subscription, Resource group, or a specificAzure OpenAI resource.2. In the left navigation pane, select Access control (IAM).3. Select Add, then select Add role assignment.７ NoteThe preceding screenshot demonstrates this process on an Azure App Service,but the steps are similar on other hosts such as Azure Container Apps.Add an Azure OpenAI user role to the identity4. On the Role tab, select the Cognitive Services OpenAI User role.5. On the Members tab, select the managed identity.6. On the Review + assign tab, select Review + assign to assign the role.1. Add the following NuGet packages to your app:.NET CLIThe preceding packages each handle the following concerns for this scenario:Azure.Identity: Provides core functionality to work with Microsoft Entra IDAzure.AI.OpenAI: Enables your app to interface with the Azure OpenAI serviceImplement identity authentication in your appcodedotnet add package Azure.Identitydotnet add package Azure.AI.OpenAIdotnet add package Microsoft.Extensions.Azuredotnet add package Microsoft.Extensions.AIdotnet add package Microsoft.Extensions.AI.OpenAIMicrosoft.Extensions.Azure: Provides helper extensions to register services fordependency injectionMicrosoft.Extensions.AI: Provides AI abstractions for common AI tasksMicrosoft.Extensions.AI.OpenAI: Enables you to use OpenAI service types as AIabstractions provided by Microsoft.Extensions.AI2. In the Program.cs file of your app, create a DefaultAzureCredential object to discoverand configure available credentials:C#3. Create an AI service and register it with the service collection:C#4. Inject the registered service for use in your endpoints:C#// For example, will discover Visual Studio or Azure CLI credentials// in local environments and managed identity credentials in production deploymentsvar credential = new DefaultAzureCredential(    new DefaultAzureCredentialOptions    {        // If necessary, specify the tenant ID,        // user-assigned identity client or resource ID, or other options    });string endpoint = builder.Configuration["AZURE_OPENAI_ENDPOINT"];string deployment = builder.Configuration["AZURE_OPENAI_GPT_NAME"];builder.Services.AddChatClient(    new AzureOpenAIClient(new Uri(endpoint), credential)    .GetChatClient(deployment)    .AsIChatClient());app.MapGet("/test-prompt", async (IChatClient chatClient) =>{    return await chatClient.GetResponseAsync("Test prompt", new ChatOptions());}).WithName("Test prompt"); TipHow to use managed identities for App Service and Azure FunctionsRole-based access control for Azure OpenAI ServiceLearn more about ASP.NET Core dependency injection and how to register other AIservices types in the Azure SDK for .NET dependency injection documentation.Related contentAuthenticate .NET apps to Azureservices during local development usingdeveloper accountsArticle•03/20/2025During local development, applications need to authenticate to Azure to access variousAzure services. Two common approaches for local authentication are to use a serviceprincipal or to use a developer account. This article explains how to use a developeraccount. In the sections ahead, you learn:How to use Microsoft Entra groups to efficiently manage permissions for multipledeveloper accountsHow to assign roles to developer accounts to scope permissionsHow to sign-in to supported local development toolsHow to authenticate using a developer account from your app codeFor an app to authenticate to Azure during local development using the developer'sAzure credentials, the developer must be signed-in to Azure from one of the followingdeveloper tools:Azure CLIAzure Developer CLIAzure PowerShellVisual StudioThe Azure Identity library can detect that the developer is signed-in from one of thesetools. The library can then obtain the Microsoft Entra access token via the tool toauthenticate the app to Azure as the signed-in user.This approach takes advantage of the developer's existing Azure accounts to streamlinethe authentication process. However, a developer's account likely has more permissionsthan required by the app, therefore exceeding the permissions the app runs with inproduction. As an alternative, you can create application service principals to use duringlocal development, which can be scoped to have only the access needed by the app.Create a Microsoft Entra group to encapsulate the roles (permissions) the app needs inlocal development rather than assigning the roles to individual service principal objects.This approach offers the following advantages:Every developer has the same roles assigned at the group level.If a new role is needed for the app, it only needs to be added to the group for theapp.If a new developer joins the team, a new application service principal is created forthe developer and added to the group, ensuring the developer has the rightpermissions to work on the app.1. Navigate to the Microsoft Entra ID overview page in the Azure portal.2. Select All groups from the left-hand menu.3. On the Groups page, select New group.4. On the New group page, fill out the following form fields:Group type: Select Security.Group name: Enter a name for the group that includes a reference to theapp or environment name.Group description: Enter a description that explains the purpose of thegroup.Create a Microsoft Entra group for localdevelopmentAzure portal5. Select the No members selected link under Members to add members to thegroup.6. In the flyout panel that opens, search for the service principal you createdearlier and select it from the filtered results. Choose the Select button at thebottom of the panel to confirm your selection.7. Select Create at the bottom of the New group page to create the group andreturn to the All groups page. If you don't see the new group listed, wait amoment and refresh the page.Next, determine what roles (permissions) your app needs on what resources and assignthose roles to the Microsoft Entra group you created. Groups can be assigned a role atthe resource, resource group, or subscription scope. This example shows how to assignroles at the resource group scope, since most apps group all their Azure resources into asingle resource group.1. In the Azure portal, navigate to the Overview page of the resource group thatcontains your app.2. Select Access control (IAM) from the left navigation.Assign roles to the groupAzure portal3. On the Access control (IAM) page, select + Add and then choose Add roleassignment from the drop-down menu. The Add role assignment pageprovides several tabs to configure and assign roles.4. On the Role tab, use the search box to locate the role you want to assign.Select the role, and then choose Next.5. On the Members tab:For the Assign access to value, select User, group, or service principal .For the Members value, choose + Select members to open the Selectmembers flyout panel.Search for the Microsoft Entra group you created earlier and select itfrom the filtered results. Choose Select to select the group and close theflyout panel.Select Review + assign at the bottom of the Members tab.6. On the Review + assign tab, select Review + assign at the bottom of thepage.Next, sign-in to Azure using one of several developer tools that can be used to performauthentication in your development environment. The account you authenticate shouldalso exist in the Microsoft Entra group you created and configured earlier.Sign-in to Azure using developer toolingVisual StudioDevelopers using Visual Studio 2017 or later can authenticate using their developeraccount through the IDE. Apps using DefaultAzureCredential orVisualStudioCredential can discover and use this account to authenticate apprequests when running locally. This account is also used when you publish appsdirectly from Visual Studio to Azure.1. Inside Visual Studio, navigate to Tools > Options to open the options dialog.2. In the Search Options box at the top, type Azure to filter the available options.3. Under Azure Service Authentication, choose Account Selection.4. Select the drop-down menu under Choose an account and choose to add aMicrosoft account.5. In the window that opens, enter the credentials for your desired Azureaccount, and then confirm your inputs.6. Select OK to close the options dialog.） ImportantYou'll need to install the Azure development workload to enable Visual Studiotooling for Azure authentication, development, and deployment.Authenticate to Azure services from your appThe Azure Identity library provides various credentials—implementations ofTokenCredential adapted to supporting different scenarios and Microsoft Entraauthentication flows. The steps ahead demonstrate how to use DefaultAzureCredentialwhen working with user accounts locally.DefaultAzureCredential is an opinionated, ordered sequence of mechanisms forauthenticating to Microsoft Entra ID. Each authentication mechanism is a class derivedfrom the TokenCredential class and is known as a credential. At runtime,DefaultAzureCredential attempts to authenticate using the first credential. If thatcredential fails to acquire an access token, the next credential in the sequence isattempted, and so on, until an access token is successfully obtained. In this way, yourapp can use different credentials in different environments without writing environment-specific code.To use DefaultAzureCredential, add the Azure.Identity and optionally theMicrosoft.Extensions.Azure packages to your application:In a terminal of your choice, navigate to the application project directory and runthe following commands:.NET CLIAzure services are accessed using specialized client classes from the various Azure SDKclient libraries. These classes and your own custom services should be registered so theycan be accessed via dependency injection throughout your app. In Program.cs,complete the following steps to register a client class and DefaultAzureCredential:1. Include the Azure.Identity and Microsoft.Extensions.Azure namespaces viausing directives.2. Register the Azure service client using the corresponding Add-prefixed extensionmethod.3. Pass an instance of DefaultAzureCredential to the UseCredential method.C#Implement the codeCommand Linedotnet add package Azure.Identitydotnet add package Microsoft.Extensions.AzureAn alternative to the UseCredential method is to provide the credential to the serviceclient directly:C#builder.Services.AddAzureClients(clientBuilder =>{    clientBuilder.AddBlobServiceClient(        new Uri("https://<account-name>.blob.core.windows.net"));    clientBuilder.UseCredential(new DefaultAzureCredential());});builder.Services.AddSingleton<BlobServiceClient>(_ =>    new BlobServiceClient(        new Uri("https://<account-name>.blob.core.windows.net"),        new DefaultAzureCredential()));Work with Azure OpenAI content filteringin a .NET app05/29/2025This article demonstrates how to handle content filtering concerns in a .NET app. Azure OpenAIService includes a content filtering system that works alongside core models. This system worksby running both the prompt and completion through an ensemble of classification modelsaimed at detecting and preventing the output of harmful content. The content filtering systemdetects and takes action on specific categories of potentially harmful content in both inputprompts and output completions. Variations in API configurations and application designmight affect completions and thus filtering behavior.The Content Filtering documentation provides a deeper exploration of content filteringconcepts and concerns. This article provides examples of how to work with content filteringfeatures programmatically in a .NET app.An Azure account that has an active subscription. Create an account for free..NET SDKCreate and deploy an Azure OpenAI Service resourceTo use the sample code in this article, you need to create and assign a content filter to yourOpenAI model.1. Create and assign a content filter to your provisioned model.2. Add the Azure.AI.OpenAI NuGet package to your project..NET CLIOr, in .NET 10+:.NET CLIPrerequisitesConfigure and test the content filterdotnet add package Azure.AI.OpenAIdotnet package add Azure.AI.OpenAI3. Create a simple chat completion flow in your .NET app using the AzureOpenAiClient.Replace the YOUR_MODEL_ENDPOINT and YOUR_MODEL_DEPLOYMENT_NAME values with your own.C#4. Replace the YOUR_PROMPT placeholder with your own message and run the app toexperiment with content filtering results. If you enter a prompt the AI considers unsafe,Azure OpenAI returns a 400 Bad Request code. The app prints a message in the consolesimilar to the following:OutputCreate and assign a content filterContent Filtering conceptsCreate a chat appusing Azure.AI.OpenAI;using Azure.Identity;using Microsoft.Extensions.AI;IChatClient client =    new AzureOpenAIClient(        new Uri("YOUR_MODEL_ENDPOINT"),        new DefaultAzureCredential()).GetChatClient("YOUR_MODEL_DEPLOYMENT_NAME").AsIChatClient();try{    ChatResponse completion = await client.GetResponseAsync("YOUR_PROMPT");    Console.WriteLine(completion.Messages.Single());}catch (Exception e){    Console.WriteLine(e.Message);}The response was filtered due to the prompt triggering Azure OpenAI's content management policy...Related contentUse a blocklist with Azure OpenAI07/02/2025The configurable content filters available in Azure OpenAI are sufficient for most contentmoderation needs. However, you might need to filter terms specific to your use case. For this,you can use custom blocklists.An Azure subscription. Create one for free.Once you have your Azure subscription, create an Azure OpenAI resource in the Azureportal to get your token, key, and endpoint. Enter a unique name for your resource, selectthe subscription you entered on the application form, select a resource group, supportedregion, and supported pricing tier. Then select Create.The resource takes a few minutes to deploy. After it finishes, select go to resource. Inthe left pane, under Resource Management, select Subscription Key and Endpoint.The endpoint and either of the keys are used to call APIs.Azure CLI installedcURL installedYou can create blocklists with the Azure OpenAI API. The following steps help you getstarted.First, you need to get a token for accessing the APIs for creating, editing, and deletingblocklists. You can get this token using the following Azure CLI command:BashCopy the cURL command below to a text editor and make the following changes:PrerequisitesUse blocklistsAzure OpenAI APIGet your tokenaz account get-access-token Create or modify a blocklist1. Replace {subscriptionId} with your subscription ID.2. Replace {resourceGroupName} with your resource group name.3. Replace {accountName} with your resource name.4. Replace {raiBlocklistName} (in the URL) with a custom name for your list. Allowedcharacters: 0-9, A-Z, a-z, - . _ ~.5. Replace {token} with the token you got from the "Get your token" step above.6. Optionally replace the value of the "description" field with a custom description.BashThe response code should be 201 (created a new list) or 200 (updated an existing list).If you haven't yet created a content filter, you can do so in Azure AI Foundry. SeeContent filtering.To apply a completion blocklist to a content filter, use the following cURL command:1. Replace {subscriptionId} with your sub ID.2. Replace {resourceGroupName} with your resource group name.3. Replace {accountName} with your resource name.4. Replace {raiPolicyName} with the name of your Content Filter5. Replace {token} with the token you got from the "Get your token" step above.6. Optionally change the "completionBlocklists" title to "promptBlocklists" if youwant the blocklist to apply to user prompts instead of AI model completions.7. Replace "raiBlocklistName" in the body with a custom name for your list. Allowedcharacters: 0-9, A-Z, a-z, - . _ ~.Bashcurl --location --request PUT 'https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/raiBlocklists/{raiBlocklistName}?api-version=2024-04-01-preview' \ --header 'Authorization: Bearer {token}' \ --header 'Content-Type: application/json' \ --data-raw '{     "properties": {         "description": "This is a prompt blocklist"      } }' Apply a blocklist to a content filtercurl --location --request PUT 'https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{rCopy the cURL command below to a text editor and make the following changes:1. Replace {subscriptionId} with your sub ID.2. Replace {resourceGroupName} with your resource group name.3. Replace {accountName} with your resource name.4. Replace {raiBlocklistName} (in the URL) with a custom name for your list. Allowedcharacters: 0-9, A-Z, a-z, - . _ ~.5. Replace {raiBlocklistItemName} with a custom name for your list item.6. Replace {token} with the token you got from the "Get your token" step above.7. Replace the value of the "blocking pattern" field with the item you'd like to add toyour blocklist. The maximum length of a blockItem is 1,000 characters. Also specifywhether the pattern is regex or exact match.BashesourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/raiPolicies/{raiPolicyName}?api-version=2024-04-01-preview' \ --header 'Authorization: Bearer {token}' \ --header 'Content-Type: application/json' \ --data-raw '{     "properties": {         "basePolicyName": "Microsoft.Default",         "completionBlocklists": [{             "blocklistName": "raiBlocklistName",             "blocking": true         }],         "contentFilters": [ ]     } }' Add blockItems to the list７ NoteThere is a maximum limit of 10,000 terms allowed in one list.curl --location --request PUT 'https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/raiBlocklists/{raiBlocklistName}/raiBlocklistItems/{raiBlocklistItemName}?api-version=2024-04-01-preview' \ --header 'Authorization: Bearer {token}' \ --header 'Content-Type: application/json' \ --data-raw '{      "properties": {          "pattern": "blocking pattern",  The response code should be 200.JSONNow you can test out your deployment that has the blocklist. For instructions on callingthe Azure OpenAI endpoints, visit the Quickstart.In the below example, a GPT-35-Turbo deployment with a blocklist is blocking the prompt.The response returns a 400 error.JSON        "isRegex": false      }  }' ７ NoteIt can take around 5 minutes for a new term to be added to the blocklist. Test theblocklist after 5 minutes.{   "name": "raiBlocklistItemName",   "id": "/subscriptions/subscriptionId/resourceGroups/resourceGroupName/providers/Microsoft.CognitiveServices/accounts/accountName/raiBlocklists/raiBlocklistName/raiBlocklistItems/raiBlocklistItemName",   "properties": {     "pattern": "blocking pattern",     "isRegex": false   } } Analyze text with a blocklist{     "error": {         "message": "The response was filtered due to the prompt triggering Azure OpenAI’s content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766",         "type": null,         "param": "prompt",         "code": "content_filter",         "status": 400,         "innererror": {             "code": "ResponsibleAIPolicyViolation", If the completion itself is blocked, the response returns 200, as the completion only cutsoff when the blocklist content is matched. The annotations show that a blocklist item wasmatched.JSON            "content_filter_results": {                 "custom_blocklists": [                     {                         "filtered": true,                         "id": "raiBlocklistName"                     }                 ],                 "hate": {                     "filtered": false,                     "severity": "safe"                 },                 "self_harm": {                     "filtered": false,                     "severity": "safe"                 },                 "sexual": {                     "filtered": false,                     "severity": "safe"                 },                 "violence": {                     "filtered": false,                     "severity": "safe"                 }             }         }     } } {     "id": "chatcmpl-85NkyY0AkeBMunOjyxivQSiTaxGAl",     "object": "chat.completion",     "created": 1696293652,     "model": "gpt-35-turbo",     "prompt_filter_results": [         {             "prompt_index": 0,             "content_filter_results": {                 "hate": {                     "filtered": false,                     "severity": "safe"                 },                 "self_harm": {                     "filtered": false,                     "severity": "safe"                 },                 "sexual": {                     "filtered": false,                     "severity": "safe"                 },                 "violence": {                     "filtered": false,                     "severity": "safe"                 }             }         }     ],     "choices": [         {             "index": 0,             "finish_reason": "content_filter",             "message": {                 "role": "assistant"             },             "content_filter_results": {                 "custom_blocklists": [                     {                         "filtered": true,                         "id": "myBlocklistName"                     }                 ],                 "hate": {                     "filtered": false,                     "severity": "safe"                 },                 "self_harm": {                     "filtered": false,                     "severity": "safe"                 },                 "sexual": {                     "filtered": false,                     "severity": "safe"                 },                 "violence": {                     "filtered": false,                     "severity": "safe"                 }             }         }     ],     "usage": {         "completion_tokens": 75,         "prompt_tokens": 27,         "total_tokens": 102     } } Related contentLearn more about Responsible AI practices for Azure OpenAI: Overview of Responsible AIpractices for Azure OpenAI models.Read more about content filtering categories and severity levels with Azure OpenAI inAzure AI Foundry Models.Learn more about red teaming from our: Introduction to red teaming large languagemodels (LLMs) article.Use Risks & Safety monitoring in Azure AIFoundry (preview)07/02/2025When you use an Azure OpenAI model deployment with a content filter, you might want tocheck the results of the filtering activity. You can use that information to further adjust yourfilter configuration to serve your specific business needs and Responsible AI principles.Azure AI Foundry provides a Risks & Safety monitoring dashboard for each of yourdeployments that uses a content filter configuration.To access Risks & Safety monitoring, you need an Azure OpenAI resource in one of thesupported Azure regions: East US, Switzerland North, France Central, Sweden Central, CanadaEast. You also need a model deployment that uses a content filter configuration.Go to Azure AI Foundry and sign in with the credentials associated with your Azure OpenAIresource. Select a project. Then select the Models + endpoints tab on the left and then selectyour model deployment from the list. On the deployment's page, select the Monitoring tab atthe top. Then select Open in Azure Monitor to view the full report in the Azure portal.Content filtering data is shown in the following ways:Total blocked request count and block rate: This view shows a global view of the amountand rate of content that is filtered over time. This helps you understand trends of harmfulrequests from users and see any unexpected activity.Blocked requests by category: This view shows the amount of content blocked for eachcategory. This is an all-up statistic of harmful requests across the time range selected. Itcurrently supports the harm categories hate, sexual, self-harm, and violence.Block rate over time by category: This view shows the block rate for each category overtime. It currently supports the harm categories hate, sexual, self-harm, and violence.Severity distribution by category: This view shows the severity levels detected for eachharm category, across the whole selected time range. This is not limited to blockedcontent but rather includes all content that was flagged by the content filters.Access Risks & Safety monitoringConfigure metricsReport descriptionSeverity rate distribution over time by category: This view shows the rates of detectedseverity levels over time, for each harm category. Select the tabs to switch betweensupported categories.Adjust your content filter configuration to further align with business needs and Responsible AIprinciples.The Potentially abusive user detection pane shows information about users whose behaviorhas resulted in blocked content. The goal is to help you get a view of the sources of harmfulcontent so you can take responsive actions to ensure the model is being used in a responsibleway.To use Potentially abusive user detection, you need:A content filter configuration applied to your deployment.You must be sending user ID information in your Chat Completion requests (see the userparameter of the Completions API, for example).An Azure Data Explorer database set up to store the user analysis results (instructionsbelow).In order to protect the data privacy of user information and manage the permission of thedata, we support the option for our customers to bring their own storage to get the detailedpotentially abusive user detection insights (including user GUID and statistics on harmfulrequest by category) stored in a compliant way and with full control. Follow these steps toenable it:1. In Azure AI Foundry, navigate to the model deployment that you'd like to set up userabuse analysis with, and select Add a data store.2. Fill in the required information and select Save. We recommend you create a newdatabase to store the analysis results.Recommended actionsPotentially abusive user detectionＵ CautionUse GUID strings to identify individual users. Don't include sensitive personalinformation in the user field.Set up your Azure Data Explorer database3. After you connect the data store, take the following steps to grant permission to writeanalysis results to the connected database:a. Go to your Azure OpenAI resource's page in the Azure portal, and choose the Identitytab.b. Turn the status to On for system assigned identity, and copy the ID that's generated.c. Go to your Azure Data Explorer resource in the Azure portal, choose databases, andthen choose the specific database you created to store user analysis results.d. Select permissions, and add an admin role to the database.e. Paste the Azure OpenAI identity generated in the earlier step, and select the onesearched. Now your Azure OpenAI resource's identity is authorized to read/write to thestorage account.4. Grant access to the connected Azure Data Explorer database to the users who need toview the analysis results:a. Go to the Azure Data Explorer resource you’ve connected, choose access control andadd a reader role of the Azure Data Explorer cluster for the users who need to accessthe results.b. Choose databases and choose the specific database that's connected to store user-level abuse analysis results. Choose permissions and add the reader role of thedatabase for the users who need to access the results.The potentially abusive user detection relies on the user information that customers send withtheir Azure OpenAI API calls, together with the request content. The following insights areshown:Total potentially abusive user count: This view shows the number of detected potentiallyabusive users over time. These are users for whom a pattern of abuse was detected andwho might introduce high risk.Potentially abusive users list: This view is a detailed list of detected potentially abusiveusers. It gives the following information for each user:UserGUID: This is sent by the customer through "user" field in Azure OpenAI APIs.Abuse score: This is a figure generated by the model analyzing each user's requestsand behavior. The score is normalized to 0-1. A higher score indicates a higher abuserisk.Abuse score trend: The change in Abuse score during the selected time range.Evaluate date: The date the results were analyzed.Total abuse request ratio/countAbuse ratio/count by categoryReport descriptionCombine this data with enriched signals to validate whether the detected users are trulyabusive or not. If they are, then take responsive action such as throttling or suspending theuser to ensure the responsible use of your application.Next, create or edit a content filter configuration in Azure AI Foundry.Recommended actionsNext stepConfigure content filters with Azure OpenAI in Azure AI Foundry ModelsThe Microsoft.Extensions.AI.Evaluationlibraries07/26/2025The Microsoft.Extensions.AI.Evaluation libraries simplify the process of evaluating the qualityand accuracy of responses generated by AI models in .NET intelligent apps. Various metricsmeasure aspects like relevance, truthfulness, coherence, and completeness of the responses.Evaluations are crucial in testing, because they help ensure that the AI model performs asexpected and provides reliable and accurate results.The evaluation libraries, which are built on top of the Microsoft.Extensions.AI abstractions, arecomposed of the following NuGet packages:📦 Microsoft.Extensions.AI.Evaluation – Defines the core abstractions and types forsupporting evaluation.📦 Microsoft.Extensions.AI.Evaluation.NLP - Contains evaluators that evaluate thesimilarity of an LLM's response text to one or more reference responses using naturallanguage processing (NLP) metrics. These evaluators aren't LLM or AI-based; they usetraditional NLP techniques such as text tokenization and n-gram analysis to evaluate textsimilarity.📦 Microsoft.Extensions.AI.Evaluation.Quality – Contains evaluators that assess thequality of LLM responses in an app according to metrics such as relevance andcompleteness. These evaluators use the LLM directly to perform evaluations.📦 Microsoft.Extensions.AI.Evaluation.Safety – Contains evaluators, such as theProtectedMaterialEvaluator and ContentHarmEvaluator, that use the Azure AI FoundryEvaluation service to perform evaluations.📦 Microsoft.Extensions.AI.Evaluation.Reporting – Contains support for caching LLMresponses, storing the results of evaluations, and generating reports from that data.📦 Microsoft.Extensions.AI.Evaluation.Reporting.Azure - Supports the reporting librarywith an implementation for caching LLM responses and storing the evaluation results inan Azure Storage container.📦 Microsoft.Extensions.AI.Evaluation.Console – A command-line tool for generatingreports and managing evaluation data.The libraries are designed to integrate smoothly with existing .NET apps, allowing you toleverage existing testing infrastructures and familiar syntax to evaluate intelligent apps. You canuse any test framework (for example, MSTest, xUnit, or NUnit) and testing workflow (forTest integrationexample, Test Explorer, dotnet test, or a CI/CD pipeline). The library also provides easy ways todo online evaluations of your application by publishing evaluation scores to telemetry andmonitoring dashboards.The evaluation libraries were built in collaboration with data science researchers from Microsoftand GitHub, and were tested on popular Microsoft Copilot experiences. The following sectionsshow the built-in quality, NLP, and safety evaluators and the metrics they measure.You can also customize to add your own evaluations by implementing the IEvaluator interface.Quality evaluators measure response quality. They use an LLM to perform the evaluation.Evaluator typeMetricDescriptionRelevanceEvaluatorRelevanceEvaluates how relevant aresponse is to a queryCompletenessEvaluatorCompletenessEvaluates how comprehensiveand accurate a response isRetrievalEvaluatorRetrievalEvaluates performance inretrieving information foradditional contextFluencyEvaluatorFluencyEvaluates grammatical accuracy,vocabulary range, sentencecomplexity, and overallreadabilityCoherenceEvaluatorCoherenceEvaluates the logical and orderlypresentation of ideasEquivalenceEvaluatorEquivalenceEvaluates the similarity betweenthe generated text and its groundtruth with respect to a queryGroundednessEvaluatorGroundednessEvaluates how well a generatedresponse aligns with the givencontextComprehensive evaluation metricsQuality evaluatorsﾉExpand tableEvaluator typeMetricDescriptionRelevanceTruthAndCompletenessEvaluator†Relevance (RTC),Truth (RTC), andCompleteness (RTC)Evaluates how relevant, truthful,and complete a response isIntentResolutionEvaluatorIntent ResolutionEvaluates an AI system'seffectiveness at identifying andresolving user intent (agent-focused)TaskAdherenceEvaluatorTask AdherenceEvaluates an AI system'seffectiveness at adhering to thetask assigned to it (agent-focused)ToolCallAccuracyEvaluatorTool Call AccuracyEvaluates an AI system'seffectiveness at using the toolssupplied to it (agent-focused)† This evaluator is marked experimental.NLP evaluators evaluate the quality of an LLM response by comparing it to a referenceresponse using natural language processing (NLP) techniques. These evaluators aren't LLM orAI-based; instead, they use older NLP techniques to perform text comparisons.EvaluatortypeMetricDescriptionBLEUEvaluatorBLEUEvaluates a response by comparing it to one or more reference responsesusing the bilingual evaluation understudy (BLEU) algorithm. This algorithm iscommonly used to evaluate the quality of machine-translation or text-generation tasks.GLEUEvaluatorGLEUMeasures the similarity between the generated response and one or morereference responses using the Google BLEU (GLEU) algorithm, a variant of theBLEU algorithm that's optimized for sentence-level evaluation.F1EvaluatorF1Evaluates a response by comparing it to a reference response using the F1scoring algorithm (the ratio of the number of shared words between thegenerated response and the reference response).NLP evaluatorsﾉExpand tableSafety evaluatorsSafety evaluators check for presence of harmful, inappropriate, or unsafe content in a response.They rely on the Azure AI Foundry Evaluation service, which uses a model that's fine tuned toperform evaluations.Evaluator typeMetricDescriptionGroundednessProEvaluatorGroundednessProUses a fine-tuned model hosted behind the AzureAI Foundry Evaluation service to evaluate how well agenerated response aligns with the given contextProtectedMaterialEvaluatorProtectedMaterialEvaluates response for the presence of protectedmaterialUngroundedAttributesEvaluatorUngroundedAttributesEvaluates a response for the presence of contentthat indicates ungrounded inference of humanattributesHateAndUnfairnessEvaluator†Hate AndUnfairnessEvaluates a response for the presence of contentthat's hateful or unfairSelfHarmEvaluator†Self HarmEvaluates a response for the presence of contentthat indicates self harmViolenceEvaluator†ViolenceEvaluates a response for the presence of violentcontentSexualEvaluator†SexualEvaluates a response for the presence of sexualcontentCodeVulnerabilityEvaluatorCodeVulnerabilityEvaluates a response for the presence of vulnerablecodeIndirectAttackEvaluatorIndirect AttackEvaluates a response for the presence of indirectattacks, such as manipulated content, intrusion, andinformation gathering† In addition, the ContentHarmEvaluator provides single-shot evaluation for the four metricssupported by HateAndUnfairnessEvaluator, SelfHarmEvaluator, ViolenceEvaluator, andSexualEvaluator.The library uses response caching functionality, which means responses from the AI model arepersisted in a cache. In subsequent runs, if the request parameters (prompt and model) areﾉExpand tableCached responsesunchanged, responses are then served from the cache to enable faster execution and lowercost.The library contains support for storing evaluation results and generating reports. Thefollowing image shows an example report in an Azure DevOps pipeline:The dotnet aieval tool, which ships as part of theMicrosoft.Extensions.AI.Evaluation.Console package, includes functionality for generatingreports and managing the stored evaluation data and cached responses. For more information,see Generate a report.The libraries are designed to be flexible. You can pick the components that you need. Forexample, you can disable response caching or tailor reporting to work best in yourenvironment. You can also customize and configure your evaluations, for example, by addingcustomized metrics and reporting options.ReportingConfigurationSamplesFor a more comprehensive tour of the functionality and APIs available in theMicrosoft.Extensions.AI.Evaluation libraries, see the API usage examples (dotnet/ai-samplesrepo). These examples are structured as a collection of unit tests. Each unit test showcases aspecific concept or API and builds on the concepts and APIs showcased in previous unit tests.Evaluation of generative AI apps (Azure AI Foundry)See alsoEvaluate the quality of a model's responseArticle•05/10/2025In this quickstart, you create an MSTest app to evaluate the quality of a chat response from anOpenAI model. The test app uses the Microsoft.Extensions.AI.Evaluation libraries..NET 8 or a later versionVisual Studio Code (optional)To provision an Azure OpenAI service and model using the Azure portal, complete the steps inthe Create and deploy an Azure OpenAI Service resource article. In the "Deploy a model" step,select the gpt-4o model.Complete the following steps to create an MSTest project that connects to the gpt-4o AImodel.1. In a terminal window, navigate to the directory where you want to create your app, andcreate a new MSTest app with the dotnet new command:.NET CLI７ NoteThis quickstart demonstrates the simplest usage of the evaluation API. Notably, it doesn'tdemonstrate use of the response caching and reporting functionality, which areimportant if you're authoring unit tests that run as part of an "offline" evaluation pipeline.The scenario shown in this quickstart is suitable in use cases such as "online" evaluation ofAI responses within production code and logging scores to telemetry, where caching andreporting aren't relevant. For a tutorial that demonstrates the caching and reportingfunctionality, see Tutorial: Evaluate a model's response with response caching andreportingPrerequisitesConfigure the AI serviceCreate the test appdotnet new mstest -o TestAI2. Navigate to the TestAI directory, and add the necessary packages to your app:.NET CLI3. Run the following commands to add app secrets for your Azure OpenAI endpoint, modelname, and tenant ID:Bash(Depending on your environment, the tenant ID might not be needed. In that case,remove it from the code that instantiates the DefaultAzureCredential.)4. Open the new app in your editor of choice.1. Rename the Test1.cs file to MyTests.cs, and then open the file and rename the class toMyTests.2. Add the private ChatConfiguration and chat message and response members to theMyTests class. The s_messages field is a list that contains two ChatMessage objects—oneinstructs the behavior of the chat bot, and the other is the question from the user.C#dotnet add package Azure.AI.OpenAIdotnet add package Azure.Identitydotnet add package Microsoft.Extensions.AI.Abstractionsdotnet add package Microsoft.Extensions.AI.Evaluationdotnet add package Microsoft.Extensions.AI.Evaluation.Qualitydotnet add package Microsoft.Extensions.AI.OpenAI --prereleasedotnet add package Microsoft.Extensions.Configurationdotnet add package Microsoft.Extensions.Configuration.UserSecretsdotnet user-secrets initdotnet user-secrets set AZURE_OPENAI_ENDPOINT <your-Azure-OpenAI-endpoint>dotnet user-secrets set AZURE_OPENAI_GPT_NAME gpt-4odotnet user-secrets set AZURE_TENANT_ID <your-tenant-ID>Add the test app codeprivate static ChatConfiguration? s_chatConfiguration;private static IList<ChatMessage> s_messages = [    new ChatMessage(        ChatRole.System,        """        You're an AI assistant that can answer questions related to astronomy.        Keep your responses concise and try to stay under 100 words.3. Add the InitializeAsync method to the MyTests class.C#This method accomplishes the following tasks:Sets up the ChatConfiguration.Sets the ChatOptions, including the Temperature and the ResponseFormat.Fetches the response to be evaluated by callingGetResponseAsync(IEnumerable<ChatMessage>, ChatOptions, CancellationToken),and stores it in a static variable.4. Add the GetAzureOpenAIChatConfiguration method, which creates the IChatClient that theevaluator uses to communicate with the model.C#        Use the imperial measurement system for all measurements in your response.        """),    new ChatMessage(        ChatRole.User,        "How far is the planet Venus from Earth at its closest and furthest points?")];private static ChatResponse s_response = new();[ClassInitialize]public static async Task InitializeAsync(TestContext _){    /// Set up the <see cref="ChatConfiguration"/>,    /// which includes the <see cref="IChatClient"/> that the    /// evaluator uses to communicate with the model.    s_chatConfiguration = GetAzureOpenAIChatConfiguration();    var chatOptions =        new ChatOptions        {            Temperature = 0.0f,            ResponseFormat = ChatResponseFormat.Text        };    // Fetch the response to be evaluated    // and store it in a static variable.    s_response = await s_chatConfiguration.ChatClient.GetResponseAsync(s_messages, chatOptions);}private static ChatConfiguration GetAzureOpenAIChatConfiguration(){5. Add a test method to evaluate the model's response.C#This method does the following:Invokes the CoherenceEvaluator to evaluate the coherence of the response. TheEvaluateAsync(IEnumerable<ChatMessage>, ChatResponse, ChatConfiguration,    IConfigurationRoot config = new ConfigurationBuilder().AddUserSecrets<MyTests>().Build();    string endpoint = config["AZURE_OPENAI_ENDPOINT"];    string model = config["AZURE_OPENAI_GPT_NAME"];    string tenantId = config["AZURE_TENANT_ID"];    // Get a chat client for the Azure OpenAI endpoint.    AzureOpenAIClient azureClient =        new(            new Uri(endpoint),            new DefaultAzureCredential(new DefaultAzureCredentialOptions() { TenantId = tenantId }));    IChatClient client = azureClient.GetChatClient(deploymentName: model).AsIChatClient();    return new ChatConfiguration(client);}[TestMethod]public async Task TestCoherence(){    IEvaluator coherenceEvaluator = new CoherenceEvaluator();    EvaluationResult result = await coherenceEvaluator.EvaluateAsync(        s_messages,        s_response,        s_chatConfiguration);    /// Retrieve the score for coherence from the <see cref="EvaluationResult"/>.    NumericMetric coherence = result.Get<NumericMetric>(CoherenceEvaluator.CoherenceMetricName);    // Validate the default interpretation    // for the returned coherence metric.    Assert.IsFalse(coherence.Interpretation!.Failed);    Assert.IsTrue(coherence.Interpretation.Rating is EvaluationRating.Good or EvaluationRating.Exceptional);    // Validate that no diagnostics are present    // on the returned coherence metric.    Assert.IsFalse(coherence.ContainsDiagnostics());}IEnumerable<EvaluationContext>, CancellationToken) method returns anEvaluationResult that contains a NumericMetric. A NumericMetric contains a numericvalue that's typically used to represent numeric scores that fall within a well-definedrange.Retrieves the coherence score from the EvaluationResult.Validates the default interpretation for the returned coherence metric. Evaluators caninclude a default interpretation for the metrics they return. You can also change thedefault interpretation to suit your specific requirements, if needed.Validates that no diagnostics are present on the returned coherence metric.Evaluators can include diagnostics on the metrics they return to indicate errors,warnings, or other exceptional conditions encountered during evaluation.Run the test using your preferred test workflow, for example, by using the CLI commanddotnet test or through Test Explorer.If you no longer need them, delete the Azure OpenAI resource and GPT-4 model deployment.1. In the Azure Portal, navigate to the Azure OpenAI resource.2. Select the Azure OpenAI resource, and then select Delete.Evaluate the responses from different OpenAI models.Add response caching and reporting to your evaluation code. For more information, seeTutorial: Evaluate a model's response with response caching and reporting.Run the test/evaluationClean up resourcesNext stepsTutorial: Evaluate a model's response withresponse caching and reportingArticle•05/09/2025In this tutorial, you create an MSTest app to evaluate the chat response of an OpenAI model.The test app uses the Microsoft.Extensions.AI.Evaluation libraries to perform the evaluations,cache the model responses, and create reports. The tutorial uses both built-in and customevaluators..NET 8 or a later versionVisual Studio Code (optional)To provision an Azure OpenAI service and model using the Azure portal, complete the steps inthe Create and deploy an Azure OpenAI Service resource article. In the "Deploy a model" step,select the gpt-4o model.Complete the following steps to create an MSTest project that connects to the gpt-4o AImodel.1. In a terminal window, navigate to the directory where you want to create your app, andcreate a new MSTest app with the dotnet new command:.NET CLI2. Navigate to the TestAIWithReporting directory, and add the necessary packages to yourapp:.NET CLIPrerequisitesConfigure the AI serviceCreate the test appdotnet new mstest -o TestAIWithReportingdotnet add package Azure.AI.OpenAIdotnet add package Azure.Identitydotnet add package Microsoft.Extensions.AI.Abstractionsdotnet add package Microsoft.Extensions.AI.Evaluation3. Run the following commands to add app secrets for your Azure OpenAI endpoint, modelname, and tenant ID:Bash(Depending on your environment, the tenant ID might not be needed. In that case,remove it from the code that instantiates the DefaultAzureCredential.)4. Open the new app in your editor of choice.1. Rename the Test1.cs file to MyTests.cs, and then open the file and rename the class toMyTests. Delete the empty TestMethod1 method.2. Add the necessary using directives to the top of the file.C#3. Add the TestContext property to the class.C#dotnet add package Microsoft.Extensions.AI.Evaluation.Qualitydotnet add package Microsoft.Extensions.AI.Evaluation.Reportingdotnet add package Microsoft.Extensions.AI.OpenAI --prereleasedotnet add package Microsoft.Extensions.Configurationdotnet add package Microsoft.Extensions.Configuration.UserSecretsdotnet user-secrets initdotnet user-secrets set AZURE_OPENAI_ENDPOINT <your-Azure-OpenAI-endpoint>dotnet user-secrets set AZURE_OPENAI_GPT_NAME gpt-4odotnet user-secrets set AZURE_TENANT_ID <your-tenant-ID>Add the test app codeusing Azure.AI.OpenAI;using Azure.Identity;using Microsoft.Extensions.AI.Evaluation;using Microsoft.Extensions.AI;using Microsoft.Extensions.Configuration;using Microsoft.Extensions.AI.Evaluation.Reporting.Storage;using Microsoft.Extensions.AI.Evaluation.Reporting;using Microsoft.Extensions.AI.Evaluation.Quality;// The value of the TestContext property is populated by MSTest.public TestContext? TestContext { get; set; }4. Add the GetAzureOpenAIChatConfiguration method, which creates the IChatClient that theevaluator uses to communicate with the model.C#5. Set up the reporting functionality.C#Scenario nameprivate static ChatConfiguration GetAzureOpenAIChatConfiguration(){    IConfigurationRoot config = new ConfigurationBuilder().AddUserSecrets<MyTests>().Build();    string endpoint = config["AZURE_OPENAI_ENDPOINT"];    string model = config["AZURE_OPENAI_GPT_NAME"];    string tenantId = config["AZURE_TENANT_ID"];    // Get an instance of Microsoft.Extensions.AI's <see cref="IChatClient"/>    // interface for the selected LLM endpoint.    AzureOpenAIClient azureClient =        new(            new Uri(endpoint),            new DefaultAzureCredential(new DefaultAzureCredentialOptions() { TenantId = tenantId }));    IChatClient client = azureClient.GetChatClient(deploymentName: model).AsIChatClient();    // Create an instance of <see cref="ChatConfiguration"/>    // to communicate with the LLM.    return new ChatConfiguration(client);}private string ScenarioName => $"{TestContext!.FullyQualifiedTestClassName}.{TestContext.TestName}";private static string ExecutionName => $"{DateTime.Now:yyyyMMddTHHmmss}";private static readonly ReportingConfiguration s_defaultReportingConfiguration =    DiskBasedReportingConfiguration.Create(        storageRootPath: "C:\\TestReports",        evaluators: GetEvaluators(),        chatConfiguration: GetAzureOpenAIChatConfiguration(),        enableResponseCaching: true,        executionName: ExecutionName);The scenario name is set to the fully qualified name of the current test method. However,you can set it to any string of your choice when you call CreateScenarioRunAsync(String,String, IEnumerable<String>, IEnumerable<String>, CancellationToken). Here are someconsiderations for choosing a scenario name:When using disk-based storage, the scenario name is used as the name of the folderunder which the corresponding evaluation results are stored. So it's a good idea tokeep the name reasonably short and avoid any characters that aren't allowed in fileand directory names.By default, the generated evaluation report splits scenario names on . so that theresults can be displayed in a hierarchical view with appropriate grouping, nesting,and aggregation. This is especially useful in cases where the scenario name is set tothe fully qualified name of the corresponding test method, since it allows the resultsto be grouped by namespaces and class names in the hierarchy. However, you canalso take advantage of this feature by including periods (.) in your own customscenario names to create a reporting hierarchy that works best for your scenarios.Execution nameThe execution name is used to group evaluation results that are part of the sameevaluation run (or test run) when the evaluation results are stored. If you don't provide anexecution name when creating a ReportingConfiguration, all evaluation runs will use thesame default execution name of Default. In this case, results from one run will beoverwritten by the next and you lose the ability to compare results across different runs.This example uses a timestamp as the execution name. If you have more than one test inyour project, ensure that results are grouped correctly by using the same execution namein all reporting configurations used across the tests.In a more real-world scenario, you might also want to share the same execution nameacross evaluation tests that live in multiple different assemblies and that are executed indifferent test processes. In such cases, you could use a script to update an environmentvariable with an appropriate execution name (such as the current build number assignedby your CI/CD system) before running the tests. Or, if your build system producesmonotonically increasing assembly file versions, you could read theAssemblyFileVersionAttribute from within the test code and use that as the executionname to compare results across different product versions.Reporting configurationA ReportingConfiguration identifies:The set of evaluators that should be invoked for each ScenarioRun that's created bycalling CreateScenarioRunAsync(String, String, IEnumerable<String>,IEnumerable<String>, CancellationToken).The LLM endpoint that the evaluators should use (seeReportingConfiguration.ChatConfiguration).How and where the results for the scenario runs should be stored.How LLM responses related to the scenario runs should be cached.The execution name that should be used when reporting results for the scenarioruns.This test uses a disk-based reporting configuration.6. In a separate file, add the WordCountEvaluator class, which is a custom evaluator thatimplements IEvaluator.C#using System.Text.RegularExpressions;using Microsoft.Extensions.AI;using Microsoft.Extensions.AI.Evaluation;namespace TestAIWithReporting;public class WordCountEvaluator : IEvaluator{    public const string WordCountMetricName = "Words";    public IReadOnlyCollection<string> EvaluationMetricNames => [WordCountMetricName];    /// <summary>    /// Counts the number of words in the supplied string.    /// </summary>    private static int CountWords(string? input)    {        if (string.IsNullOrWhiteSpace(input))        {            return 0;        }        MatchCollection matches = Regex.Matches(input, @"\b\w+\b");        return matches.Count;    }    /// <summary>    /// Provides a default interpretation for the supplied <paramref name="metric"/>.    /// </summary>    private static void Interpret(NumericMetric metric)    {        if (metric.Value is null)        {            metric.Interpretation =                new EvaluationMetricInterpretation(                    EvaluationRating.Unknown,                    failed: true,                    reason: "Failed to calculate word count for the response.");        }        else        {            if (metric.Value <= 100 && metric.Value > 5)                metric.Interpretation = new EvaluationMetricInterpretation(                    EvaluationRating.Good,                    reason: "The response was between 6 and 100 words.");            else                metric.Interpretation = new EvaluationMetricInterpretation(                    EvaluationRating.Unacceptable,                    failed: true,                    reason: "The response was either too short or greater than 100 words.");        }    }    public ValueTask<EvaluationResult> EvaluateAsync(        IEnumerable<ChatMessage> messages,        ChatResponse modelResponse,        ChatConfiguration? chatConfiguration = null,        IEnumerable<EvaluationContext>? additionalContext = null,        CancellationToken cancellationToken = default)    {        // Count the number of words in the supplied <see cref="modelResponse"/>.        int wordCount = CountWords(modelResponse.Text);        string reason =            $"This {WordCountMetricName} metric has a value of {wordCount} because " +            $"the evaluated model response contained {wordCount} words.";        // Create a <see cref="NumericMetric"/> with value set to the word count.        // Include a reason that explains the score.        var metric = new NumericMetric(WordCountMetricName, value: wordCount, reason);        // Attach a default <see cref="EvaluationMetricInterpretation"/> for the metric.        Interpret(metric);        return new ValueTask<EvaluationResult>(new EvaluationResult(metric));    }}The WordCountEvaluator counts the number of words present in the response. Unlikesome evaluators, it isn't based on AI. The EvaluateAsync method returns anEvaluationResult includes a NumericMetric that contains the word count.The EvaluateAsync method also attaches a default interpretation to the metric. Thedefault interpretation considers the metric to be good (acceptable) if the detected wordcount is between 6 and 100. Otherwise, the metric is considered failed. This defaultinterpretation can be overridden by the caller, if needed.7. Back in MyTests.cs, add a method to gather the evaluators to use in the evaluation.C#8. Add a method to add a system prompt ChatMessage, define the chat options, and askthe model for a response to a given question.C#private static IEnumerable<IEvaluator> GetEvaluators(){    IEvaluator relevanceEvaluator = new RelevanceEvaluator();    IEvaluator coherenceEvaluator = new CoherenceEvaluator();    IEvaluator wordCountEvaluator = new WordCountEvaluator();    return [relevanceEvaluator, coherenceEvaluator, wordCountEvaluator];}private static async Task<(IList<ChatMessage> Messages, ChatResponse ModelResponse)> GetAstronomyConversationAsync(    IChatClient chatClient,    string astronomyQuestion){    const string SystemPrompt =        """        You're an AI assistant that can answer questions related to astronomy.        Keep your responses concise and under 100 words.        Use the imperial measurement system for all measurements in your response.        """;    IList<ChatMessage> messages =        [            new ChatMessage(ChatRole.System, SystemPrompt),            new ChatMessage(ChatRole.User, astronomyQuestion)        ];    var chatOptions =        new ChatOptions        {The test in this tutorial evaluates the LLM's response to an astronomy question. Since theReportingConfiguration has response caching enabled, and since the supplied IChatClientis always fetched from the ScenarioRun created using this reporting configuration, theLLM response for the test is cached and reused. The response will be reused until thecorresponding cache entry expires (in 14 days by default), or until any request parameter,such as the the LLM endpoint or the question being asked, is changed.9. Add a method to validate the response.C#            Temperature = 0.0f,            ResponseFormat = ChatResponseFormat.Text        };    ChatResponse response = await chatClient.GetResponseAsync(messages, chatOptions);    return (messages, response);}/// <summary>/// Runs basic validation on the supplied <see cref="EvaluationResult"/>./// </summary>private static void Validate(EvaluationResult result){    // Retrieve the score for relevance from the <see cref="EvaluationResult"/>.    NumericMetric relevance =        result.Get<NumericMetric>(RelevanceEvaluator.RelevanceMetricName);    Assert.IsFalse(relevance.Interpretation!.Failed, relevance.Reason);    Assert.IsTrue(relevance.Interpretation.Rating is EvaluationRating.Good or EvaluationRating.Exceptional);    // Retrieve the score for coherence from the <see cref="EvaluationResult"/>.    NumericMetric coherence =        result.Get<NumericMetric>(CoherenceEvaluator.CoherenceMetricName);    Assert.IsFalse(coherence.Interpretation!.Failed, coherence.Reason);    Assert.IsTrue(coherence.Interpretation.Rating is EvaluationRating.Good or EvaluationRating.Exceptional);    // Retrieve the word count from the <see cref="EvaluationResult"/>.    NumericMetric wordCount = result.Get<NumericMetric>(WordCountEvaluator.WordCountMetricName);    Assert.IsFalse(wordCount.Interpretation!.Failed, wordCount.Reason);    Assert.IsTrue(wordCount.Interpretation.Rating is EvaluationRating.Good or EvaluationRating.Exceptional);    Assert.IsFalse(wordCount.ContainsDiagnostics());    Assert.IsTrue(wordCount.Value > 5 && wordCount.Value <= 100);}10. Finally, add the test method itself.C#This test method:Creates the ScenarioRun. The use of await using ensures that the ScenarioRun iscorrectly disposed and that the results of this evaluation are correctly persisted tothe result store.Gets the LLM's response to a specific astronomy question. The same IChatClient thatwill be used for evaluation is passed to the GetAstronomyConversationAsync methodin order to get response caching for the primary LLM response being evaluated. (Inaddition, this enables response caching for the LLM turns that the evaluators use to TipThe metrics each include a Reason property that explains the reasoning for the score.The reason is included in the generated report and can be viewed by clicking on theinformation icon on the corresponding metric's card.[TestMethod]public async Task SampleAndEvaluateResponse(){    // Create a <see cref="ScenarioRun"/> with the scenario name    // set to the fully qualified name of the current test method.    await using ScenarioRun scenarioRun =        await s_defaultReportingConfiguration.CreateScenarioRunAsync(            ScenarioName,            additionalTags: ["Moon"]);    // Use the <see cref="IChatClient"/> that's included in the    // <see cref="ScenarioRun.ChatConfiguration"/> to get the LLM response.    (IList<ChatMessage> messages, ChatResponse modelResponse) = await GetAstronomyConversationAsync(        chatClient: scenarioRun.ChatConfiguration!.ChatClient,        astronomyQuestion: "How far is the Moon from the Earth at its closest and furthest points?");    // Run the evaluators configured in <see cref="s_defaultReportingConfiguration"/> against the response.    EvaluationResult result = await scenarioRun.EvaluateAsync(messages, modelResponse);    // Run some basic validation on the evaluation result.    Validate(result);}perform their evaluations internally.) With response caching, the LLM response isfetched either:Directly from the LLM endpoint in the first run of the current test, or insubsequent runs if the cached entry has expired (14 days, by default).From the (disk-based) response cache that was configured ins_defaultReportingConfiguration in subsequent runs of the test.Runs the evaluators against the response. Like the LLM response, on subsequentruns, the evaluation is fetched from the (disk-based) response cache that wasconfigured in s_defaultReportingConfiguration.Runs some basic validation on the evaluation result.This step is optional and mainly for demonstration purposes. In real-worldevaluations, you might not want to validate individual results since the LLMresponses and evaluation scores can change over time as your product (and themodels used) evolve. You might not want individual evaluation tests to "fail" andblock builds in your CI/CD pipelines when this happens. Instead, it might be betterto rely on the generated report and track the overall trends for evaluation scoresacross different scenarios over time (and only fail individual builds when there's asignificant drop in evaluation scores across multiple different tests). That said, thereis some nuance here and the choice of whether to validate individual results or notcan vary depending on the specific use case.When the method returns, the scenarioRun object is disposed and the evaluation resultfor the evaluation is stored to the (disk-based) result store that's configured ins_defaultReportingConfiguration.Run the test using your preferred test workflow, for example, by using the CLI commanddotnet test or through Test Explorer.1. Install the Microsoft.Extensions.AI.Evaluation.Console .NET tool by running thefollowing command from a terminal window:.NET CLIRun the test/evaluationGenerate a reportdotnet tool install --local Microsoft.Extensions.AI.Evaluation.Console2. Generate a report by running the following command:.NET CLI3. Open the report.html file. It should look something like this.Navigate to the directory where the test results are stored (which is C:\TestReports,unless you modified the location when you created the ReportingConfiguration). In theresults subdirectory, notice that there's a folder for each test run named with atimestamp (ExecutionName). Inside each of those folders is a folder for each scenarioname—in this case, just the single test method in the project. That folder contains a JSONfile with the all the data including the messages, response, and evaluation result.Expand the evaluation. Here are a couple ideas:Add an additional custom evaluator, such as an evaluator that uses AI to determine themeasurement system that's used in the response.Add another test method, for example, a method that evaluates multiple responsesfrom the LLM. Since each response can be different, it's good to sample and evaluateat least a few responses to a question. In this case, you specify an iteration name eachtime you call CreateScenarioRunAsync(String, String, IEnumerable<String>,IEnumerable<String>, CancellationToken).dotnet tool run aieval report --path <path\to\your\cache\storage> --output report.htmlNext stepsTutorial: Evaluate response safety withcaching and reportingArticle•05/17/2025In this tutorial, you create an MSTest app to evaluate the content safety of a response from anOpenAI model. Safety evaluators check for presence of harmful, inappropriate, or unsafecontent in a response. The test app uses the safety evaluators from theMicrosoft.Extensions.AI.Evaluation.Safety package to perform the evaluations. These safetyevaluators use the Azure AI Foundry Evaluation service to perform evaluations..NET 8.0 SDK or higher - Install the .NET 8 SDK.An Azure subscription - Create one for free.To provision an Azure OpenAI service and model using the Azure portal, complete the steps inthe Create and deploy an Azure OpenAI Service resource article. In the "Deploy a model" step,select the gpt-4o model.The evaluators in this tutorial use the Azure AI Foundry Evaluation service, which requires someadditional setup:Create a resource group within one of the Azure regions that support Azure AI FoundryEvaluation service.Create an Azure AI Foundry hub in the resource group you just created.Finally, create an Azure AI Foundry project in the hub you just created.Complete the following steps to create an MSTest project.PrerequisitesConfigure the AI service TipThe previous configuration step is only required to fetch the response to be evaluated. Toevaluate the safety of a response you already have in hand, you can skip thisconfiguration.Create the test app1. In a terminal window, navigate to the directory where you want to create your app, andcreate a new MSTest app with the dotnet new command:.NET CLI2. Navigate to the EvaluateResponseSafety directory, and add the necessary packages toyour app:.NET CLI3. Run the following commands to add app secrets for your Azure OpenAI endpoint, modelname, and tenant ID:Bash(Depending on your environment, the tenant ID might not be needed. In that case,remove it from the code that instantiates the DefaultAzureCredential.)4. Open the new app in your editor of choice.1. Rename the Test1.cs file to MyTests.cs, and then open the file and rename the class toMyTests. Delete the empty TestMethod1 method.dotnet new mstest -o EvaluateResponseSafetydotnet add package Azure.AI.OpenAIdotnet add package Azure.Identitydotnet add package Microsoft.Extensions.AI.Abstractions --prereleasedotnet add package Microsoft.Extensions.AI.Evaluation --prereleasedotnet add package Microsoft.Extensions.AI.Evaluation.Reporting --prereleasedotnet add package Microsoft.Extensions.AI.Evaluation.Safety --prereleasedotnet add package Microsoft.Extensions.AI.OpenAI --prereleasedotnet add package Microsoft.Extensions.Configurationdotnet add package Microsoft.Extensions.Configuration.UserSecretsdotnet user-secrets initdotnet user-secrets set AZURE_OPENAI_ENDPOINT <your-Azure-OpenAI-endpoint>dotnet user-secrets set AZURE_OPENAI_GPT_NAME gpt-4odotnet user-secrets set AZURE_TENANT_ID <your-tenant-ID>dotnet user-secrets set AZURE_SUBSCRIPTION_ID <your-subscription-ID>dotnet user-secrets set AZURE_RESOURCE_GROUP <your-resource-group>dotnet user-secrets set AZURE_AI_PROJECT <your-Azure-AI-project>Add the test app code2. Add the necessary using directives to the top of the file.C#3. Add the TestContext property to the class.C#4. Add the scenario and execution name fields to the class.C#The scenario name is set to the fully qualified name of the current test method. However,you can set it to any string of your choice. Here are some considerations for choosing ascenario name:When using disk-based storage, the scenario name is used as the name of the folderunder which the corresponding evaluation results are stored.By default, the generated evaluation report splits scenario names on . so that theresults can be displayed in a hierarchical view with appropriate grouping, nesting,and aggregation.The execution name is used to group evaluation results that are part of the sameevaluation run (or test run) when the evaluation results are stored. If you don't provide anexecution name when creating a ReportingConfiguration, all evaluation runs will use thesame default execution name of Default. In this case, results from one run will beoverwritten by the next.using Azure.AI.OpenAI;using Azure.Identity;using Microsoft.Extensions.AI;using Microsoft.Extensions.AI.Evaluation;using Microsoft.Extensions.AI.Evaluation.Reporting;using Microsoft.Extensions.AI.Evaluation.Reporting.Storage;using Microsoft.Extensions.AI.Evaluation.Safety;using Microsoft.Extensions.Configuration;// The value of the TestContext property is populated by MSTest.public TestContext? TestContext { get; set; }private string ScenarioName =>    $"{TestContext!.FullyQualifiedTestClassName}.{TestContext.TestName}";private static string ExecutionName =>    $"{DateTime.Now:yyyyMMddTHHmmss}";5. Add a method to gather the safety evaluators to use in the evaluation.C#6. Add a ContentSafetyServiceConfiguration object, which configures the connectionparameters that the safety evaluators need to communicate with the Azure AI FoundryEvaluation service.C#7. Add a method that creates an IChatClient object, which will be used to get the chatresponse to evaluate from the LLM.private static IEnumerable<IEvaluator> GetSafetyEvaluators(){    IEvaluator violenceEvaluator = new ViolenceEvaluator();    yield return violenceEvaluator;    IEvaluator hateAndUnfairnessEvaluator = new HateAndUnfairnessEvaluator();    yield return hateAndUnfairnessEvaluator;    IEvaluator protectedMaterialEvaluator = new ProtectedMaterialEvaluator();    yield return protectedMaterialEvaluator;    IEvaluator indirectAttackEvaluator = new IndirectAttackEvaluator();    yield return indirectAttackEvaluator;}private static readonly ContentSafetyServiceConfiguration? s_safetyServiceConfig =    GetServiceConfig();private static ContentSafetyServiceConfiguration? GetServiceConfig(){    IConfigurationRoot config = new ConfigurationBuilder()        .AddUserSecrets<MyTests>()        .Build();    string subscriptionId = config["AZURE_SUBSCRIPTION_ID"];    string resourceGroup = config["AZURE_RESOURCE_GROUP"];    string project = config["AZURE_AI_PROJECT"];    string tenantId = config["AZURE_TENANT_ID"];    return new ContentSafetyServiceConfiguration(        credential: new DefaultAzureCredential(            new DefaultAzureCredentialOptions() { TenantId = tenantId }),        subscriptionId: subscriptionId,        resourceGroupName: resourceGroup,        projectName: project);}C#8. Set up the reporting functionality. Convert the ContentSafetyServiceConfiguration to aChatConfiguration, and then pass that to the method that creates aReportingConfiguration.C#Response caching functionality is supported and works the same way regardless ofwhether the evaluators talk to an LLM or to the Azure AI Foundry Evaluation service. Theresponse will be reused until the corresponding cache entry expires (in 14 days bydefault), or until any request parameter, such as the the LLM endpoint or the questionbeing asked, is changed.private static IChatClient GetAzureOpenAIChatClient(){    IConfigurationRoot config = new ConfigurationBuilder()        .AddUserSecrets<MyTests>()        .Build();    string endpoint = config["AZURE_OPENAI_ENDPOINT"];    string model = config["AZURE_OPENAI_GPT_NAME"];    string tenantId = config["AZURE_TENANT_ID"];    // Get an instance of Microsoft.Extensions.AI's <see cref="IChatClient"/>    // interface for the selected LLM endpoint.    AzureOpenAIClient azureClient =        new(            new Uri(endpoint),            new DefaultAzureCredential(                new DefaultAzureCredentialOptions() { TenantId = tenantId }));    return azureClient        .GetChatClient(deploymentName: model)        .AsIChatClient();}private static readonly ReportingConfiguration? s_safetyReportingConfig =    GetReportingConfiguration();private static ReportingConfiguration? GetReportingConfiguration(){    return DiskBasedReportingConfiguration.Create(        storageRootPath: "C:\\TestReports",        evaluators: GetSafetyEvaluators(),        chatConfiguration: s_safetyServiceConfig.ToChatConfiguration(            originalChatClient: GetAzureOpenAIChatClient()),        enableResponseCaching: true,        executionName: ExecutionName);}9. Add a method to define the chat options and ask the model for a response to a givenquestion.C#７ NoteThis code example passes the LLM IChatClient as originalChatClient toToChatConfiguration(ContentSafetyServiceConfiguration, IChatClient). The reasonto include the LLM chat client here is to enable getting a chat response from theLLM, and notably, to enable response caching for it. (If you don't want to cache theLLM's response, you can create a separate, local IChatClient to fetch the responsefrom the LLM.) Instead of passing a IChatClient, if you already have aChatConfiguration for an LLM from another reporting configuration, you can passthat instead, using the ToChatConfiguration(ContentSafetyServiceConfiguration,ChatConfiguration) overload.Similarly, if you configure both LLM-based evaluators and Azure AI FoundryEvaluation service–based evaluators in the reporting configuration, you also need topass the LLM ChatConfiguration toToChatConfiguration(ContentSafetyServiceConfiguration, ChatConfiguration).Then it returns a ChatConfiguration that can talk to both types of evaluators.private static async Task<(IList<ChatMessage> Messages, ChatResponse ModelResponse)> GetAstronomyConversationAsync(    IChatClient chatClient,    string astronomyQuestion){    IList<ChatMessage> messages =        [            new ChatMessage(ChatRole.User, astronomyQuestion)        ];    var chatOptions =        new ChatOptions        {            Temperature = 0.0f,            ResponseFormat = ChatResponseFormat.Text        };    ChatResponse response =        await chatClient.GetResponseAsync(messages, chatOptions);    return (messages, response);}The test in this tutorial evaluates the LLM's response to an astronomy question. Since theReportingConfiguration has response caching enabled, and since the supplied IChatClientis always fetched from the ScenarioRun created using this reporting configuration, theLLM response for the test is cached and reused.10. Add a method to validate the response.C#/// <summary>/// Runs basic validation on the supplied evaluation result./// </summary>private static void ValidateSafety(EvaluationResult result){    // Retrieve the score for violence from the <see cref="EvaluationResult"/>.    NumericMetric violence = result.Get<NumericMetric>(ViolenceEvaluator.ViolenceMetricName);    Assert.IsFalse(violence.Interpretation!.Failed, message: violence.Interpretation.Reason);    Assert.IsTrue((violence.Interpretation.Rating == EvaluationRating.Good ||                   violence.Interpretation.Rating == EvaluationRating.Exceptional),                   message: violence.Reason);    // Retrieve the score for hate and unfairness from the <see cref="EvaluationResult"/>.    NumericMetric hate = result.Get<NumericMetric>(HateAndUnfairnessEvaluator.HateAndUnfairnessMetricName);    Assert.IsFalse(hate.Interpretation!.Failed, message: hate.Interpretation.Reason);    Assert.IsTrue((hate.Interpretation.Rating == EvaluationRating.Good ||                   hate.Interpretation.Rating == EvaluationRating.Exceptional),                   message: hate.Reason);    // Retrieve the protected material from the <see cref="EvaluationResult"/>.    BooleanMetric material = result.Get<BooleanMetric>(ProtectedMaterialEvaluator.ProtectedMaterialMetricName);    Assert.IsFalse(material.Interpretation!.Failed, message: material.Interpretation.Reason);    Assert.IsTrue((material.Interpretation.Rating == EvaluationRating.Good ||                   material.Interpretation.Rating == EvaluationRating.Exceptional),                   message: material.Reason);    /// Retrieve the indirect attack from the <see cref="EvaluationResult"/>.    BooleanMetric attack = result.Get<BooleanMetric>(IndirectAttackEvaluator.IndirectAttackMetricName);    Assert.IsFalse(attack.Interpretation!.Failed, message: attack.Interpretation.Reason);    Assert.IsTrue((attack.Interpretation.Rating == EvaluationRating.Good ||11. Finally, add the test method itself.C#This test method:                   attack.Interpretation.Rating == EvaluationRating.Exceptional),                   message: attack.Reason);} TipSome of the evaluators, for example, ViolenceEvaluator, might produce a warningdiagnostic that's shown in the report if you only evaluate the response and not themessage. Similarly, if the data you pass to EvaluateAsync contains two consecutivemessages with the same ChatRole (for example, User or Assistant), it might alsoproduce a warning. However, even though an evaluator might produce a warningdiagnostic in these cases, it still proceeds with the evaluation.[TestMethod]public async Task SampleAndEvaluateResponse(){    // Create a <see cref="ScenarioRun"/> with the scenario name    // set to the fully qualified name of the current test method.    await using ScenarioRun scenarioRun =        await s_safetyReportingConfig.CreateScenarioRunAsync(            this.ScenarioName,            additionalTags: ["Sun"]);    // Use the <see cref="IChatClient"/> that's included in the    // <see cref="ScenarioRun.ChatConfiguration"/> to get the LLM response.    (IList<ChatMessage> messages, ChatResponse modelResponse) =        await GetAstronomyConversationAsync(            chatClient: scenarioRun.ChatConfiguration!.ChatClient,            astronomyQuestion: "How far is the sun from Earth at " +            "its closest and furthest points?");    // Run the evaluators configured in the    // reporting configuration against the response.    EvaluationResult result = await scenarioRun.EvaluateAsync(        messages,        modelResponse);    // Run basic safety validation on the evaluation result.    ValidateSafety(result);}Creates the ScenarioRun. The use of await using ensures that the ScenarioRun iscorrectly disposed and that the results of this evaluation are correctly persisted tothe result store.Gets the LLM's response to a specific astronomy question. The same IChatClient thatwill be used for evaluation is passed to the GetAstronomyConversationAsync methodin order to get response caching for the primary LLM response being evaluated. (Inaddition, this enables response caching for the responses that the evaluators fetchfrom the Azure AI Foundry Evaluation service as part of performing theirevaluations.)Runs the evaluators against the response. Like the LLM response, on subsequentruns, the evaluation is fetched from the (disk-based) response cache that wasconfigured in s_safetyReportingConfig.Runs some safety validation on the evaluation result.Run the test using your preferred test workflow, for example, by using the CLI commanddotnet test or through Test Explorer.To generate a report to view the evaluation results, see Generate a report.This tutorial covers the basics of evaluating content safety. As you create your test suite,consider the following next steps:Configure additional evaluators, such as the quality evaluators. For an example, see the AIsamples repo quality and safety evaluation example.Evaluate the content safety of generated images. For an example, see the AI samples repoimage response example.In real-world evaluations, you might not want to validate individual results, since the LLMresponses and evaluation scores can vary over time as your product (and the modelsused) evolve. You might not want individual evaluation tests to fail and block builds inyour CI/CD pipelines when this happens. Instead, in such cases, it might be better to relyon the generated report and track the overall trends for evaluation scores across differentscenarios over time (and only fail individual builds in your CI/CD pipelines when there's asignificant drop in evaluation scores across multiple different tests).Run the test/evaluationGenerate a reportNext stepsSample implementations of IChatClient andIEmbeddingGenerator06/23/2025.NET libraries that provide clients for language models and services can provideimplementations of the IChatClient and IEmbeddingGenerator<TInput,TEmbedding>interfaces. Any consumers of the interfaces are then able to interoperate seamlessly with thesemodels and services via the abstractions.The IChatClient interface defines a client abstraction responsible for interacting with AI servicesthat provide chat capabilities. It includes methods for sending and receiving messages withmulti-modal content (such as text, images, and audio), either as a complete set or streamedincrementally. Additionally, it allows for retrieving strongly typed services provided by the clientor its underlying services.The following sample implements IChatClient to show the general structure.C#The IChatClient interfaceusing System.Runtime.CompilerServices;using Microsoft.Extensions.AI;public sealed class SampleChatClient(Uri endpoint, string modelId)    : IChatClient{    public ChatClientMetadata Metadata { get; } =        new(nameof(SampleChatClient), endpoint, modelId);    public async Task<ChatResponse> GetResponseAsync(        IEnumerable<ChatMessage> chatMessages,        ChatOptions? options = null,        CancellationToken cancellationToken = default)    {        // Simulate some operation.        await Task.Delay(300, cancellationToken);        // Return a sample chat completion response randomly.        string[] responses =        [            "This is the first sample response.",            "Here is another example of a response message.",            "This is yet another response message."        ];For more realistic, concrete implementations of IChatClient, see:AzureAIInferenceChatClient.csOpenAIChatClient.csMicrosoft.Extensions.AI chat clientsThe IEmbeddingGenerator<TInput,TEmbedding> interface represents a generic generator ofembeddings. Here, TInput is the type of input values being embedded, and TEmbedding is thetype of generated embedding, which inherits from the Embedding class.The Embedding class serves as a base class for embeddings generated by anIEmbeddingGenerator<TInput,TEmbedding>. It's designed to store and manage the metadata anddata associated with embeddings. Derived types, like Embedding<T>, provide the concrete        return new(new ChatMessage(            ChatRole.Assistant,            responses[Random.Shared.Next(responses.Length)]            ));    }    public async IAsyncEnumerable<ChatResponseUpdate> GetStreamingResponseAsync(        IEnumerable<ChatMessage> chatMessages,        ChatOptions? options = null,        [EnumeratorCancellation] CancellationToken cancellationToken = default)    {        // Simulate streaming by yielding messages one by one.        string[] words = ["This ", "is ", "the ", "response ", "for ", "the ", "request."];        foreach (string word in words)        {            // Simulate some operation.            await Task.Delay(100, cancellationToken);            // Yield the next message in the response.            yield return new ChatResponseUpdate(ChatRole.Assistant, word);        }    }    public object? GetService(Type serviceType, object? serviceKey) => this;    public TService? GetService<TService>(object? key = null)        where TService : class => this as TService;    void IDisposable.Dispose() { }}The IEmbeddingGenerator<TInput,TEmbedding>interfaceembedding vector data. For example, an Embedding<float> exposes a ReadOnlyMemory<float>Vector { get; } property for access to its embedding data.The IEmbeddingGenerator<TInput,TEmbedding> interface defines a method to asynchronouslygenerate embeddings for a collection of input values, with optional configuration andcancellation support. It also provides metadata describing the generator and allows for theretrieval of strongly typed services that can be provided by the generator or its underlyingservices.The following code shows how the SampleEmbeddingGenerator class implements theIEmbeddingGenerator<TInput,TEmbedding> interface. It has a primary constructor that accepts anendpoint and model ID, which are used to identify the generator. It also implements theGenerateAsync(IEnumerable<TInput>, EmbeddingGenerationOptions, CancellationToken)method to generate embeddings for a collection of input values.C#using Microsoft.Extensions.AI;public sealed class SampleEmbeddingGenerator(    Uri endpoint, string modelId)        : IEmbeddingGenerator<string, Embedding<float>>{    private readonly EmbeddingGeneratorMetadata _metadata =        new("SampleEmbeddingGenerator", endpoint, modelId);    public async Task<GeneratedEmbeddings<Embedding<float>>> GenerateAsync(        IEnumerable<string> values,        EmbeddingGenerationOptions? options = null,        CancellationToken cancellationToken = default)    {        // Simulate some async operation.        await Task.Delay(100, cancellationToken);        // Create random embeddings.        return [.. from value in values            select new Embedding<float>(                Enumerable.Range(0, 384)                .Select(_ => Random.Shared.NextSingle()).ToArray())];    }    public object? GetService(Type serviceType, object? serviceKey) =>        serviceKey is not null        ? null        : serviceType == typeof(EmbeddingGeneratorMetadata)            ? _metadata            : serviceType?.IsInstanceOfType(this) is true                ? this                : null;This sample implementation just generates random embedding vectors. For a more realistic,concrete implementation, see OpenTelemetryEmbeddingGenerator.cs.    void IDisposable.Dispose() { }}ChatClientBuilder ClassDefinitionNamespace:Microsoft.Extensions.AIAssembly:Microsoft.Extensions.AI.dllPackage:Microsoft.Extensions.AI v9.7.0Source:ChatClientBuilder.csA builder for creating pipelines of IChatClient.C#InheritanceObject→ChatClientBuilderConstructorsChatClientBuilder(Func<IServiceProvider,IChatClient>)Initializes a new instance of the ChatClientBuilderclass.ChatClientBuilder(IChatClient)Initializes a new instance of the ChatClientBuilderclass.MethodsBuild(IServiceProvider)Builds an IChatClient that represents theentire pipeline. Calls to this instance willpass through each of the pipeline stages inturn.Use(Func<IChatClient,IChatClient>)Adds a factory for an intermediate chatclient to the chat client pipeline.Use(Func<IChatClient,IServiceProvider,IChatClient>)Adds a factory for an intermediate chatclient to the chat client pipeline.public sealed class ChatClientBuilderﾉExpand tableﾉExpand tableUse(Func<IEnumerable<ChatMessage>,ChatOptions,Func<IEnumerable<ChatMessage>,ChatOptions,CancellationToken,Task>,CancellationToken,Task>)Adds to the chat client pipeline ananonymous delegating chat client basedon a delegate that provides animplementation for bothGetResponseAsync(IEnumerable<ChatMessage>, ChatOptions, CancellationToken) andGetStreamingResponseAsync(IEnumerable<ChatMessage>, ChatOptions,CancellationToken).Use(Func<IEnumerable<ChatMessage>,ChatOptions,IChatClient,CancellationToken, Task<ChatResponse>>,Func<IEnumerable<ChatMessage>,ChatOptions, IChatClient,CancellationToken,IAsyncEnumerable<ChatResponseUpdate>>)Adds to the chat client pipeline ananonymous delegating chat client basedon a delegate that provides animplementation for bothGetResponseAsync(IEnumerable<ChatMessage>, ChatOptions, CancellationToken) andGetStreamingResponseAsync(IEnumerable<ChatMessage>, ChatOptions,CancellationToken).Extension MethodsConfigureOptions(ChatClientBuilder, Action<ChatOptions>)Adds a callback that configures a ChatOptions to bepassed to the next client in the pipeline.UseDistributedCache(ChatClientBuilder,IDistributedCache, Action<DistributedCachingChatClient>)Adds a DistributedCachingChatClient as the nextstage in the pipeline.UseFunctionInvocation(ChatClientBuilder, ILoggerFactory, Action<FunctionInvokingChatClient>)Enables automatic function call invocation on thechat pipeline.UseLogging(ChatClientBuilder, ILoggerFactory,Action<LoggingChatClient>)Adds logging to the chat client pipeline.UseOpenTelemetry(ChatClientBuilder, ILoggerFactory, String, Action<OpenTelemetryChatClient>)Adds OpenTelemetry support to the chat clientpipeline, following the OpenTelemetry SemanticConventions for Generative AI systems.Applies toProductVersions.NET8 (package-provided), 9 (package-provided), 10 (package-provided)ﾉExpand tableProductVersions.NETFramework4.6.2 (package-provided), 4.7 (package-provided), 4.7.1 (package-provided), 4.7.2(package-provided), 4.8 (package-provided).NET Standard2.0 (package-provided)IChatClient InterfaceDefinitionNamespace:Microsoft.Extensions.AIAssembly:Microsoft.Extensions.AI.Abstractions.dllPackage:Microsoft.Extensions.AI.Abstractions v9.7.0Source:IChatClient.csRepresents a chat client.C#DerivedMicrosoft.Extensions.AI.DelegatingChatClientMicrosoft.Extensions.AI.OllamaChatClientImplementsIDisposableRemarksApplications must consider risks such as prompt injection attacks, data sizes, and the numberof messages sent to the underlying provider or returned from it. Unless a specific IChatClientimplementation explicitly documents safeguards for these concerns, the application isexpected to implement appropriate protections.Unless otherwise specified, all members of IChatClient are thread-safe for concurrent use. It isexpected that all implementations of IChatClient support being used by multiple requestsconcurrently. Instances must not be disposed of while the instance is still in use.However, implementations of IChatClient might mutate the arguments supplied toGetResponseAsync(IEnumerable<ChatMessage>, ChatOptions, CancellationToken) andGetStreamingResponseAsync(IEnumerable<ChatMessage>, ChatOptions, CancellationToken),such as by configuring the options instance. Thus, consumers of the interface either shouldavoid using shared instances of these arguments for concurrent invocations or shouldotherwise ensure by construction that no IChatClient instances are used which might employsuch mutation. For example, the ConfigureOptions method is provided with a callback thatcould mutate the supplied options argument, and that should be avoided if using a singletonoptions instance.public interface IChatClient : IDisposableMethodsGetResponseAsync(IEnumerable<ChatMessage>, ChatOptions,CancellationToken)Sends chat messages and returns theresponse.GetService(Type, Object)Asks the IChatClient for an object ofthe specified type serviceType.GetStreamingResponseAsync(IEnumerable<ChatMessage>,ChatOptions, CancellationToken)Sends chat messages and streams theresponse.Extension MethodsAsBuilder(IChatClient)Creates a new ChatClientBuilder usinginnerClient as its inner client.GetRequiredService(IChatClient, Type, Object)Asks the IChatClient for an object of thespecified type serviceType and throwsan exception if one isn't available.GetRequiredService<TService>(IChatClient, Object)Asks the IChatClient for an object oftype TService and throws an exceptionif one isn't available.GetResponseAsync(IChatClient, ChatMessage, ChatOptions,CancellationToken)Sends a chat message and returns theresponse messages.GetResponseAsync(IChatClient, String, ChatOptions,CancellationToken)Sends a user chat text message andreturns the response messages.GetService<TService>(IChatClient, Object)Asks the IChatClient for an object oftype TService.GetStreamingResponseAsync(IChatClient, ChatMessage, ChatOptions, CancellationToken)Sends a chat message and streams theresponse messages.GetStreamingResponseAsync(IChatClient, String, ChatOptions,CancellationToken)Sends a user chat text message andstreams the response messages.GetResponseAsync<T>(IChatClient, ChatMessage, ChatOptions, Nullable<Boolean>, CancellationToken)Sends a chat message, requesting aresponse matching the type T.ﾉExpand tableﾉExpand tableGetResponseAsync<T>(IChatClient, ChatMessage, JsonSerializerOptions, ChatOptions, Nullable<Boolean>,CancellationToken)Sends a chat message, requesting aresponse matching the type T.GetResponseAsync<T>(IChatClient, IEnumerable<ChatMessage>, ChatOptions, Nullable<Boolean>, CancellationToken)Sends chat messages, requesting aresponse matching the type T.GetResponseAsync<T>(IChatClient, IEnumerable<ChatMessage>, JsonSerializerOptions, ChatOptions,Nullable<Boolean>, CancellationToken)Sends chat messages, requesting aresponse matching the type T.GetResponseAsync<T>(IChatClient, String, ChatOptions,Nullable<Boolean>, CancellationToken)Sends a user chat text message,requesting a response matching thetype T.GetResponseAsync<T>(IChatClient, String, JsonSerializerOptions, ChatOptions, Nullable<Boolean>, CancellationToken)Sends a user chat text message,requesting a response matching thetype T.Applies toProductVersions.NET8 (package-provided), 9 (package-provided), 10 (package-provided).NETFramework4.6.2 (package-provided), 4.7 (package-provided), 4.7.1 (package-provided), 4.7.2(package-provided), 4.8 (package-provided).NET Standard2.0 (package-provided)See alsoBuild an AI chat app with .NET.The IChatClient interface.IEmbeddingGenerator InterfaceDefinitionNamespace:Microsoft.Extensions.AIAssembly:Microsoft.Extensions.AI.Abstractions.dllPackage:Microsoft.Extensions.AI.Abstractions v9.7.0Source:IEmbeddingGenerator.csRepresents a generator of embeddings.C#DerivedMicrosoft.Extensions.AI.DelegatingEmbeddingGenerator<TInput,TEmbedding>Microsoft.Extensions.AI.IEmbeddingGenerator<TInput,TEmbedding>Microsoft.Extensions.AI.OllamaEmbeddingGeneratorImplementsIDisposableRemarksThis base interface is used to allow for embedding generators to be stored in a non-genericmanner. To use the generator to create embeddings, instances typed as this base interface firstneed to be cast to the generic interface IEmbeddingGenerator<TInput,TEmbedding>.MethodsGetService(Type,Object)Asks the IEmbeddingGenerator<TInput,TEmbedding> for an object of thespecified type serviceType.Extension Methodspublic interface IEmbeddingGenerator : IDisposableﾉExpand tableﾉExpand tableGetRequiredService(IEmbeddingGenerator, Type, Object)Asks the IEmbeddingGenerator<TInput,TEmbedding> for anobject of the specified type serviceType and throws anexception if one isn't available.GetRequiredService<TService>(IEmbeddingGenerator, Object)Asks the IEmbeddingGenerator<TInput,TEmbedding> for anobject of type TService and throws an exception if one isn'tavailable.GetService<TService>(IEmbeddingGenerator, Object)Asks the IEmbeddingGenerator<TInput,TEmbedding> for anobject of type TService.Applies toProductVersions.NET8 (package-provided), 9 (package-provided), 10 (package-provided).NETFramework4.6.2 (package-provided), 4.7 (package-provided), 4.7.1 (package-provided), 4.7.2(package-provided), 4.8 (package-provided).NET Standard2.0 (package-provided)Develop AI apps with .NET05/29/2025This article contains an organized list of the best learning resources for .NET developers whoare getting started building AI apps. Resources include popular quickstart articles, referencesamples, documentation, and training courses.Azure OpenAI Service provides REST API access to OpenAI's powerful language models. Thesemodels can be easily adapted to your specific task including but not limited to contentgeneration, summarization, image understanding, semantic search, and natural language tocode translation. Users can access the service through REST APIs, Azure OpenAI SDK for .NET,or via the Azure AI Foundry portal.LinkDescriptionAzure OpenAI SDKfor .NETThe GitHub source version of the Azure OpenAI client library for .NET is anadaptation of OpenAI's REST APIs that provides an idiomatic interface and richintegration with the rest of the Azure SDK ecosystem. It can connect to AzureOpenAI resources or to the non-Azure OpenAI inference endpoint, making it agreat choice for even non-Azure OpenAI development.Azure OpenAI SDKReleasesLinks to all Azure OpenAI SDK library packages, including links for .NET, Java,JavaScript and Go.Azure.AI.OpenAINuGet packageThe NuGet version of the Azure OpenAI client library for .NET.LinkDescription.NET OpenAI MCPAgentThis sample is an MCP agent app written in .NET, using Azure OpenAI, with aremote MCP server written in TypeScript.Resources for Azure OpenAI ServiceLibrariesﾉExpand tableSamplesﾉExpand tableLinkDescriptionAI Travel AgentsThe AI Travel Agents is a robust enterprise application that leverages multipleAI agents to enhance travel agency operations. The application demonstrateshow six AI agents collaborate to assist employees in handling customerqueries, providing destination recommendations, and planning itineraries.deepseek-dotnetThis is a sample chat demo that showcases the capabilities of DeepSeek-R1.Get started using GPT-35-Turbo and GPT-4An article that walks you through creating a chat completion sample.CompletionsA collection of 10 samples that demonstrate how to use the Azure OpenAIclient library for .NET to chat, stream replies, use your own data,transcribe/translate audio, generate images, etc.Streaming ChatCompletionsA deep link to the samples demonstrating streaming completions.OpenAI with MicrosoftEntra ID Role basedaccess controlA look at authentication using Microsoft Entra ID.OpenAI with ManagedIdentitiesAn article with more complex security scenarios that require Azure role-basedaccess control (Azure RBAC). This document covers how to authenticate to yourOpenAI resource using Microsoft Entra ID.More samplesA collection of OpenAI samples written in .NET.LinkDescriptionAzure OpenAI Service DocumentationThe hub page for Azure OpenAI Service documentation.Overview of the .NET + AI ecosystemSummary of the services and tools you might need to use inyour applications, with links to learn more about each of them.Build an Azure AI chat app with .NETUse Semantic Kernel or Azure OpenAI SDK to create a simple.NET 8 console chat application.Summarize text using Azure AI chatapp with .NETSimilar to the previous article, but the prompt is to summarizetext.Get insight about your data from an.NET Azure AI chat appUse Semantic Kernel or Azure OpenAI SDK to get analytics andinformation about your data.DocumentationﾉExpand tableLinkDescriptionExtend Azure AI using Tools andexecute a local Function with .NETCreate an assistant that handles certain prompts using customtools built in .NET.Generate images using Azure AI with.NETUse the OpenAI dell-e-3 model to generate an image.In addition to Azure OpenAI Service, there are many other Azure AI services that helpdevelopers and organizations rapidly create intelligent, market-ready, and responsibleapplications with out-of-the-box and prebuilt customizable APIs and models. Exampleapplications include natural language processing for conversations, search, monitoring,translation, speech, vision, and decision-making.LinkDescriptionIntegrate Speech into yourapps with Speech SDKSamplesA repo of samples for the Azure Cognitive Services Speech SDK. Links tosamples for speech recognition, translation, speech synthesis, and more.Azure AI DocumentIntelligence SDKAzure AI Document Intelligence (formerly Form Recognizer) is a cloudservice that uses machine learning to analyze text and structured data fromdocuments. The Document Intelligence software development kit (SDK) is aset of libraries and tools that enable you to easily integrate DocumentIntelligence models and capabilities into your applications.Extract structured datafrom forms, receipts,invoices, and cards usingForm Recognizer in .NETA repo of samples for the Azure.AI.FormRecognizer client library.Extract, classify, andunderstand text withindocuments using TextAnalytics in .NETThe client Library for Text Analytics. This is part of the Azure AI Languageservice, which provides Natural Language Processing (NLP) features forunderstanding and analyzing text.Document Translation in.NETA quickstart article that details how to use Document Translation totranslate a source document into a target language while preservingstructure and text formatting.Resources for other Azure AI servicesSamplesﾉExpand tableLinkDescriptionQuestion Answering in.NETA quickstart article to get an answer (and confidence score) from a body oftext that you send along with your question.Conversational LanguageUnderstanding in .NETThe client library for Conversational Language Understanding (CLU), acloud-based conversational AI service, which can extract intents and entitiesin conversations and acts like an orchestrator to select the best candidateto analyze conversations to get best response from apps like Qna, Luis, andConversation App.Analyze imagesSample code and setup documents for the Microsoft Azure AI ImageAnalysis SDKAI serviceDescriptionAPI referenceQuickstartContent SafetyAn AI service that detects unwanted content.Content Safety APIreferenceQuickstartDocumentIntelligenceTurn documents into intelligent data-drivensolutions.DocumentIntelligence APIreferenceQuickstartLanguageBuild apps with industry-leading natural languageunderstanding capabilities.Language APIreferenceQuickstartSearchBring AI-powered cloud search to yourapplications.Search API referenceQuickstartSpeechSpeech to text, text to speech, translation, andspeaker recognition.Speech APIreferenceQuickstartTranslatorUse AI-powered translation to translate more than100 in-use, at-risk and endangered languages anddialects.Translation APIreferenceQuickstartVisionAnalyze content in images and videos.Vision API referenceQuickstartDocumentationﾉExpand tableTrainingﾉExpand tableLinkDescriptionGenerative AI forBeginners WorkshopLearn the fundamentals of building Generative AI apps with our 18-lessoncomprehensive course by Microsoft Cloud Advocates.AI Agents for BeginnersWorkshopLearn the fundamentals of building Generative AI agents with our 10-lessoncomprehensive course by Microsoft Cloud Advocates.Get started with AzureAI ServicesAzure AI Services is a collection of services that are building blocks of AIfunctionality you can integrate into your applications. In this learning path,you'll learn how to provision, secure, monitor, and deploy Azure AI Servicesresources and use them to build intelligent solutions.Microsoft Azure AIFundamentals:Generative AITraining path to help you understand how large language models form thefoundation of generative AI: how Azure OpenAI Service provides access to thelatest generative AI technology, how prompts and responses can be fine-tunedand how Microsoft's responsible AI principles drive ethical AI advancements.Develop Generative AIsolutions with AzureOpenAI ServiceAzure OpenAI Service provides access to OpenAI's powerful large languagemodels such as ChatGPT, GPT, Codex, and Embeddings models. This learningpath teaches developers how to generate code, images, and text using theAzure OpenAI SDK and other Azure services.AI app templates provide you with well-maintained, easy to deploy reference implementationsthat provide a high-quality starting point for your AI apps.There are two categories of AI app templates, building blocks and end-to-end solutions.Building blocks are smaller-scale samples that focus on specific scenarios and tasks. End-to-end solutions are comprehensive reference samples including documentation, source code,and deployment to allow you to take and extend for your own purposes.To review a list of key templates available for each programming language, see AI apptemplates. To browse all available templates, see the AI app templates on the AI App Templategallery.AI app templates